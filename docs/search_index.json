[
["index.html", "Statistical Methods in Water Resources with R Preface", " Statistical Methods in Water Resources with R MiRoVaGo 2020-05-05 Preface This book is a re-coding of Statistical Methods in Water Resources by Helsel &amp; Hirsch. As the use of R in the hydrology community steadily grows, translating Helsel &amp; Hirsch material into R was an opportunity we could not miss. The content herein is a direct adaptation of the source material into R and its tools. "],
["ch1.html", "Chapter 1 Summarizing data 1.1 Characteristics of Water Resources Data 1.2 Measures of Location 1.3 Measures of Spread 1.4 Measures of Skewness 1.5 Other Resistant Measures 1.6 Outliers 1.7 Transformations Exercises", " Chapter 1 Summarizing data When determining how to appropriately analyze any collection of data, the first consideration must be the characteristics of the data themselves. Little is gained by employing analysis procedures which assume that the data possess characteristics which in fact they do not. The result of such false assumptions may be that the interpretations provided by the analysis are incorrect, or unnecessarily inconclusive. Therefore we begin this book with a discussion of the common characteristics of water resources data. These characteristics will determine the selection of appropriate data analysis procedures. One of the most frequent tasks when analyzing data is to describe and summarize those data in forms which convey their important characteristics. “What is the sulfate concentration one might expect in rainfall at this location”? “How variable is hydraulic conductivity”? “What is the 100 year flood” (the 99th percentile of annual flood maxima)? Estimation of these and similar summary statistics are basic to understanding data. Characteristics often described include: a measure of the center of the data, a measure of spread or variability, a measure of the symmetry of the data distribution, and perhaps estimates of extremes such as some large or small percentile. This chapter discusses methods for summarizing or describing data. This first chapter also quickly demonstrates one of the major themes of the book – the use of robust and resistant techniques. The reasons why one might prefer to use a resistant measure, such as the median, over a more classical measure such as the mean, are explained. The data about which a statement or summary is to be made are called the population, or sometimes the target population. These might be concentrations in all waters of an aquifer or stream reach, or all streamflows over some time at a particular site. Rarely are all such data available to the scientist. It may be physically impossible to collect all data of interest (all the water in a stream over the study period), or it may just be financially impossible to collect them. Instead, a subset of the data called the sample is selected and measured in such a way that conclusions about the sample may be extended to the entire population. Statistics computed from the sample are only inferences or estimates about characteristics of the population, such as location, spread, and skewness. Measures of location are usually the sample mean and sample median. Measures of spread include the sample standard deviation and sample interquartile range. Use of the term “sample” before each statistic explicitly demonstrates that these only estimate the population value, the population mean or median, etc. As sample estimates are far more common than measures based on the entire population, the term “mean” should be interpreted as the “sample mean”, and similarly for other statistics used in this book. When population values are discussed they will be explicitly stated as such. 1.1 Characteristics of Water Resources Data Data analyzed by the water resources scientist often have the following characteristics: A lower bound of zero. No negative values are possible. Presence of ‘outliers’, observations considerably higher or lower than most of the data, which infrequently but regularly occur. outliers on the high side are more common in water resources. Positive skewness, due to items 1 and 2. An example of a skewed distribution, the lognormal distribution, is presented in figure 1.1. Values of an observation on the horizontal axis are plotted against the frequency with which that value occurs. These density functions are like histograms of large data sets whose bars become infinitely narrow. Skewness can be expected when outlying values occur in only one direction. Non-normal distribution of data, due to items 1 - 3 above. Figure 1.2 shows an important symmetric distribution, the normal. While many statistical tests assume data follow a normal distribution as in figure 1.2, water resources data often look more like figure 1.1. In addition, symmetry does not guarantee normality. Symmetric data with more observations at both extremes (heavy tails) than occurs for a normal distribution are also non-normal. Data reported only as below or above some threshold (censored data). Examples include concentrations below one or more detection limits, annual flood stages known only to be lower than a level which would have caused a public record of the flood, and hydraulic heads known only to be above the land surface (artesian wells on old maps). Seasonal patterns. Values tend to be higher or lower in certain seasons of the year. Autocorrelation. Consecutive observations tend to be strongly correlated with each other. For the most common kind of autocorrelation in water resources (positive autocorrelation), high values tend to follow high values and low values tend to follow low values. Dependence on other uncontrolled variables. Values strongly covary with water discharge, hydraulic conductivity, sediment grain size, or some other variable. Methods for analysis of water resources data, whether the simple summarization methods such as those in this chapter, or the more complex procedures of later chapters, should recognize these common characteristics. 1.2 Measures of Location The mean and median are the two most commonly-used measures of location, though they are not the only measures available. What are the properties of these two measures, and when should one be employed over the other? 1.2.1 Classical Measure – the Mean The mean (\\(\\mathbf{\\overline{X}}\\)) is computed as the sum of all data values Xi, divided by the sample size n: \\[\\begin{equation} \\overline{X} = \\sum_{i=1}^{n} \\frac{X_{i}}{n} \\tag{1.1} \\end{equation}\\] mean_x &lt;- mean(x) For data which are in one of k groups, equation (1.1) can be rewritten to show that the overall mean depends on the mean for each group, weighted by the number of observations ni in each group: \\[\\begin{equation} \\overline{X} = \\sum_{i=1}^{n} \\overline{X}_{i} \\frac{n_{i}}{n} \\tag{1.2} \\end{equation}\\] where \\(\\mathbf{\\overline{X}_{i}}\\) is the mean for group i. The influence of any one observation Xj on the mean can be seen by placing all but that one observation in one “group”, or \\[\\begin{equation} \\begin{split} \\overline{X} &amp; =\\overline{X}_{(j)} \\frac{n-1}{n} + X_{j} \\bullet \\frac{1}{n} \\\\ &amp; =\\overline{X}_{(j)} + \\left( X_{j} - \\overline{X}_{(j)} \\right) \\bullet \\frac{1}{n} \\end{split} \\tag{1.3} \\end{equation}\\] where \\(\\mathbf{\\overline{X}_{(j)}}\\) is the mean of all observations excluding Xj. Each observation’s influence on the overall mean \\(\\mathbf{\\overline{X}}\\) is \\(\\left( \\mathbf{X_{j} -\\overline{X}_{(j)}} \\right)\\), the distance between the observation and the mean excluding that observation. Thus all observations do not have the same influence on the mean. An ‘outlier’ observation, either high or low, has a much greater influence on the overall mean \\(\\mathbf{\\overline{X}}\\) than does a more ‘typical’ observation, one closer to its \\(\\mathbf{\\overline{X}_{(j)}}\\). Figure 1.1: Density Function for a Lognormal Distribution Figure 1.2: Density Function for a Normal Distribution Another way of illustrating this influence is to realize that the mean is the balance point of the data, when each point is stacked on a number line (figure 1.3). Data points further from the center exert a stronger downward force than those closer to the center. If one point near the center were removed, the balance point would only need a small adjustment to keep the data set in balance. But if one outlying value were removed, the balance point would shift dramatically (figure 1.4). This sensitivity to the magnitudes of a small number of points in the data set defines why the mean is not a “resistant” measure of location. It is not resistant to changes in the presence of, or to changes in the magnitudes of, a few outlying observations. When this strong influence of a few observations is desirable, the mean is an appropriate measure of center. This usually occurs when computing units of mass, such as the average concentration of sediment from several samples in a cross-section. Suppose that sediment concentrations closer to the river banks were much higher than those in the center. Waters represented by a bottle of high concentration would exert more influence (due to greater mass of sediment per volume) on the final concentration than waters of low or average concentration. This is entirely appropriate, as the same would occur if the stream itself were somehow mechanically mixed throughout its cross section. Figure 1.3: The mean (triangle) as balance point of a data set Figure 1.4: Shift of the mean downward after removal of outlier 1.2.2 Resistant Measure – the Median The median, or 50th percentile P0.50, is the central value of the distribution when the data are ranked in order of magnitude. For an odd number of observations, the median is the data point which has an equal number of observations both above and below it. For an even number of observations, it is the average of the two central observations. To compute the median, first rank the observations from smallest to largest, so that x1 is the smallest observation, up to xn, the largest observation. Then \\[\\begin{equation} \\begin{aligned} median \\left(P_{0.50} \\right) &amp; = X_{(n+1)/2} &amp;&amp; \\text{when n is odd, and}\\\\ median \\left(P_{0.50} \\right) &amp; = \\frac{1}{2} \\left( X_{(n/2)} + X_{(n/2) + 1} \\right) &amp;&amp; \\text{when n is even} \\end{aligned} \\tag{1.4} \\end{equation}\\] median_x &lt;- median(x) The median is only minimally affected by the magnitude of a single observation, being determined solely by the relative order of observations. This resistance to the effect of a change in value or presence of outlying observations is often a desirable property. To demonstrate the resistance of the median, suppose the last value of the following data set (a) of 7 observations were multiplied by 10 to obtain data set (b): a &lt;- c(2,4,8,9,11,11,12) mean(a) ## [1] 8.142857 median(a) ## [1] 9 b &lt;- c(2,4,8,9,11,11,120) mean(b) ## [1] 23.57143 median(b) ## [1] 9 The mean increases from 8.1 to 23.6. The median, the \\(\\frac{(7+1)}{2}\\)th or 4th lowest data point, is unaffected by the change. When a summary value is desired that is not strongly influenced by a few extreme observations, the median is preferable to the mean. One such example is the chemical concentration one might expect to find over many streams in a given region. Using the median, one stream with unusually high concentration has no greater effect on the estimate than one with low concentration. The mean concentration may be pulled towards the outlier, and be higher than concentrations found in most of the streams. Not so for the median. 1.2.3 Other Measures of Location Three other measures of location are less frequently used: the mode, the geometric mean, and the trimmed mean. The mode is the most frequently observed value. It is the value having the highest bar in a histogram. It is far more applicable for grouped data, data which are recorded only as falling into a finite number of categories, than for continuous data. It is very easy to obtain, but a poor measure of location for continuous data, as its value often depends on the arbitrary grouping of those data. The geometric mean (GM) is often reported for positively skewed data sets. It is the mean of the logarithms, transformed back to their original units. \\[\\begin{equation} \\begin{aligned} GM &amp; = exp\\left( \\overline{Y} \\right), &amp;&amp; \\text{where $Y_{i} = \\ln{(X_{i})}$}\\\\ \\end{aligned} \\tag{1.5} \\end{equation}\\] geometric_mean_x &lt;- Gmean(x) (in this book the natural, base e logarithm will be abbreviated ln, and its inverse ex abbreviated exp(x)). For positively skewed data the geometric mean is usually quite close to the median. In fact, when the logarithms of the data are symmetric, the geometric mean is an unbiased estimate of the median. This is because the median and mean logarithms are equal, as in figure 1.2. When transformed back to original units, the geometric mean continues to be an estimate for the median, but is not an estimate for the mean (figure 1.1). Compromises between the median and mean are available by trimming off several of the lowest and highest observations, and calculating the mean of what is left. Such estimates of location are not influenced by the most extreme (and perhaps anomalous) ends of the sample, as is the mean. Yet they allow the magnitudes of most of the values to affect the estimate, unlike the median. These estimators are called “trimmed means”, and any desirable percentage of the data may be trimmed away. The most common trimming is to remove 25 percent of the data on each end – the resulting mean of the central 50 percent of data is commonly called the “trimmed mean”, but is more precisely the 25 percent trimmed mean. A “0% trimmed mean” is the sample mean itself, while trimming all but 1 or 2 central values produces the median. Percentages of trimming should be explicitly stated when used. The trimmed mean is a resistant estimator of location, as it is not strongly influenced by outliers, and works well for a wide variety of distributional shapes (normal, lognormal, etc.). It may be considered a weighted mean, where data beyond the cutoff ‘window’ are given a weight of 0, and those within the window a weight of 1.0 (see figure 1.5). Figure 1.5: Window diagram for the trimmed mean 1.3 Measures of Spread It is just as important to know how variable the data are as it is to know their general center or location. Variability is quantified by measures of spread. 1.3.1 Classical Measures The sample variance, and its square root the sample standard deviation, are the classical measures of spread. Like the mean, they are strongly influenced by outlying values. \\[\\begin{equation} \\begin{aligned} s^{2} &amp; = \\sum_{i=1}^{n} \\frac{(X_{i} - \\overline{X})^{2}}{(n-1)} &amp;&amp; \\text{sample variance}\\\\ \\end{aligned} \\tag{1.6} \\end{equation}\\] variance_x &lt;- var(x) \\[\\begin{equation} \\begin{aligned} s &amp; = \\sqrt{s^{2}} &amp;&amp; \\text{sample standard deviation} \\end{aligned} \\tag{1.7} \\end{equation}\\] standard_deviation_x &lt;- sd(x) They are computed using the squares of deviations of data from the mean, so that outliers influence their magnitudes even more so than for the mean. When outliers are present these measures are unstable and inflated. They may give the impression of much greater spread than is indicated by the majority of the data set. 1.3.2 Resistant Measures The interquartile range (IQR) is the most commonly-used resistant measure of spread. It measures the range of the central 50 percent of the data, and is not influenced at all by the 25 percent on either end. It is therefore the width of the non-zero weight window for the trimmed mean of figure 1.5. The IQR is defined as the 75th percentile minus the 25th percentile. The 75th, 50th (median) and 25th percentiles split the data into four equal-sized quarters. The 75th percentile (P.75), also called the upper quartile, is a value which exceeds no more than 75 percent of the data and is exceeded by no more than 25 percent of the data. The 25th percentile (P.25) or lower quartile is a value which exceeds no more than 25 percent of the data and is exceeded by no more than 75 percent. Consider a data set ordered from smallest to largest: Xi, i = 1,…n. Percentiles (Pj) are computed using equation (1.8) \\[\\begin{equation} \\begin{aligned} &amp; P_{j} = X_{(n + 1) \\bullet j} \\\\ \\text{where } &amp; \\text{n is the sample size of $X_{i}$, and} \\\\ &amp; \\text{j is the fraction of data less than or equal to the percentile value (for the 25th, 50th}\\\\ &amp; \\text{and 75th percentiles, j = .25, .50, and .75).} \\end {aligned} \\tag{1.8} \\end{equation}\\] interquartile_range_x &lt;- IQR(x, type = 2) # There are 9 different types you can specify. Type 2 seems to be the method used herein. Non-integer values of (n+1)•j imply linear interpolation between adjacent values of X. For the example 1 data set given earlier, n = 7, and therefore the 25th percentile is X(7+1)•.25 or X2 = 4, the second lowest observation. The 75th percentile is X6, the 6th lowest observation, or 11. The IQR is therefore 11 − 4 = 7. a ## [1] 2 4 8 9 11 11 12 IQR(a, type = 2) ## [1] 7 One resistant estimator of spread other than the IQR is the Median Absolute Deviation, or MAD. The MAD is computed by first listing the absolute value of all differences |d| between each observation and the median. The median of these absolute values is then the MAD. \\[\\begin{equation} \\begin{aligned} &amp; MAD (X_{i}) = median |d_{i}|, &amp;&amp; \\text{where $d_{i} = X_{i} - median(X_{i})$} \\end {aligned} \\tag{1.9} \\end{equation}\\] median_absolute_deviation_x &lt;- mad(x, constant = 1) Comparison of each estimate of spread for the Example 1 data set is as follows. When the last value is changed from 12 to 120, the standard deviation increases from 3.8 to 42.7. The IQR and the MAD remain exactly the same. c(IQR(a, type = 2), var(a), mad(a, constant = 1)) ## [1] 7.00000 14.47619 2.00000 c(IQR(b, type = 2), var(b), mad(b, constant = 1)) ## [1] 7.000 1819.619 2.000 1.4 Measures of Skewness Hydrologic data are typically skewed, meaning that data sets are not symmetric around the mean or median, with extreme values extending out longer in one direction. The density function for a lognormal distribution shown previously as figure 1.1 illustrates this skewness. When extreme values extend the right tail of the distribution, as they do with figure 1.1, the data are said to be skewed to the right, or positively skewed. Left skewness, when the tail extends to the left, is called negative skew. When data are skewed the mean is not expected to equal the median, but is pulled toward the tail of the distribution. Thus for positive skewness the mean exceeds more than 50 percent of the data, as in figure 1.1. The standard deviation is also inflated by data in the tail. Therefore, tables of summary statistics which include only the mean and standard deviation or variance are of questionable value for water resources data, as those data often have positive skewness. The mean and standard deviation reported may not describe the majority of the data very well. Both will be inflated by outlying observations. Summary tables which include the median and other percentiles have far greater applicability to skewed data. Skewed data also call into question the applicability of hypothesis tests which are based on assumptions that the data have a normal distribution. These tests, called parametric tests, may be of questionable value when applied to water resources data, as the data are often neither normal nor even symmetric. Later chapters will discuss this in much detail, and suggest several solutions. 1.4.1 Classical Measure of Skewness The coefficient of skewness (g) is the skewness measure used most often. It is the adjusted third moment divided by the cube of the standard deviation: \\[\\begin{equation} g = \\frac{n}{(n-1)(n-2)} \\sum_{i = 1}^{n} \\frac{(x_{i} - \\overline{X})^{3}}{s^{3}} \\tag{1.10} \\end{equation}\\] library(e1071) skewness_x &lt;- skewness(x, type = 2) # There are 3 different types you can specify. Type 2 seems to be the method used herein. A right-skewed distribution has positive g; a left-skewed distribution has negative g. Again, the influence of a few outliers is important – an otherwise symmetric distribution having one outlier will produce a large (and possibly misleading) measure of skewness. For the example 1 data, the g skewness coefficient increases from −0.8 to 2.6 when the last data point is changed from 12 to 120. e1071::skewness(a, type = 2) ## [1] -0.8398416 e1071::skewness(b, type = 2) ## [1] 2.610094 1.4.2 Resistant Measure of Skewness A more resistant measure of skewness is the quartile skew coefficient qs (Kenney and Keeping (1954)): \\[\\begin{equation} qs = \\frac{(P_{.75}-P_{.50})-(P_{.50}-P_{.25})}{P_{.75}-P_{.25}} \\tag{1.11} \\end{equation}\\] quartiles_x &lt;- quantile(x, prob = c(0.25, 0.50, 0.75), type = 2) # There are 9 different types you can specify. Type 2 seems to be the method used herein. quartile_skew_x &lt;- ((quartiles_x[3] - quartiles_x[2]) - (quartiles_x[2] - quartiles_x[1])) / (quartiles_x[3] - quartiles_x[1]) the difference in distances of the upper and lower quartiles from the median, divided by the IQR. A right-skewed distribution again has positive qs; a left-skewed distribution has negative qs. Similar to the trimmed mean and IQR, qs uses the central 50 percent of the data. For the example 1 data, qs = (11−9) − (9−4) / (11−4) = −0.43 both before and after alteration of the last data point. Note that this resistance may be a liability if sensitivity to a few observations is important. quartiles_a &lt;- quantile(a, prob = c(0.25, 0.50, 0.75), type = 2, names = FALSE) quartile_skew_a &lt;- ((quartiles_a[3] - quartiles_a[2]) - (quartiles_a[2] - quartiles_a[1])) / (quartiles_a[3] - quartiles_a[1]) quartile_skew_a ## [1] -0.4285714 quartiles_b &lt;- quantile(b, prob = c(0.25, 0.50, 0.75), type = 2, names = FALSE) quartile_skew_b &lt;- ((quartiles_b[3] - quartiles_b[2]) - (quartiles_b[2] - quartiles_b[1])) / (quartiles_b[3] - quartiles_b[1]) quartile_skew_b ## [1] -0.4285714 1.5 Other Resistant Measures Other percentiles may be used to produce a series of resistant measures of location, spread and skewness. For example, the 10 percent trimmed mean can be coupled with the range between the 10th and 90th percentiles as a measure of spread, and a corresponding measure of skewness: \\[\\begin{equation} qs_{.10} = \\frac{(P_{.90}-P_{.50})-(P_{.50}-P_{.10})}{P_{.90}-P_{.10}} \\tag{1.12} \\end{equation}\\] quartiles_x &lt;- quantile(x, prob = c(0.10, 0.50, 0.90), type = 2) # There are 9 different types you can specify. Type 2 seems to be the method used herein. quartile_skew_10_x &lt;- ((quartiles_x[3] - quartiles_x[2]) - (quartiles_x[2] - quartiles_x[1])) / (quartiles_x[3] - quartiles_x[1]) to produce a consistent series of resistant statistics. Geologists have used the 16th and 84th percentiles for many years to compute a similar series of robust measures of the distributions of sediment particles (Inman (1952)). However, measures based on quartiles have become generally standard, and other measures should be clearly defined prior to their use. The median, IQR, and quartile skew can be easily summarized graphically using a boxplot (see Chapter 2) and are familiar to most data analysts. 1.6 Outliers Outliers, observations whose values are quite different than others in the data set, often cause concern or alarm. They should not. They are often dealt with by throwing them away prior to describing data, or prior to some of the hypothesis test procedures of later chapters. Again, they should not. Outliers may be the most important points in the data set, and should be investigated further. It is said that data on the Antarctic ozone “hole”, an area of unusually low ozone concentrations, had been collected for approximately 10 years prior to its actual discovery. However, the automatic data checking routines during data processing included instructions on deleting “outliers”. The definition of outliers was based on ozone concentrations found at mid-latitudes. Thus all of this unusual data was never seen or studied for some time. If outliers are deleted, the risk is taken of seeing only what is expected to be seen. Outliers can have one of three causes: a measurement or recording error. an observation from a population not similar to that of most of the data, such as a flood caused by a dam break rather than by precipitation. a rare event from a single population that is quite skewed. The graphical methods of the Chapter 2 are very helpful in identifying outliers. Whenever outliers occur, first verify that no copying, decimal point, or other obvious error has been made. If not, it may not be possible to determine if the point is a valid one. The effort put into verification, such as re-running the sample in the laboratory, will depend on the benefit gained versus the cost of verification. Past events may not be able to be duplicated. If no error can be detected and corrected, outliers should not be discarded based solely on the fact that they appear unusual. Outliers are often discarded in order to make the data nicely fit a preconceived theoretical distribution such as the normal. There is no reason to suppose that they should! The entire data set may arise from a skewed distribution, and taking logarithms or some other transformation may produce quite symmetrical data. Even if no transformation achieves symmetry, outliers need not be discarded. Rather than eliminating actual (and possibly very important) data in order to use analysis procedures requiring symmetry or normality, procedures which are resistant to outliers should instead be employed. If computing a mean appears of little value because of an outlier, the median has been shown to be a more appropriate measure of location for skewed data. If performing a t-test (described later) appears invalidated because of the non-normality of the data set, use a rank-sum test instead. In short, let the data guide which analysis procedures are employed, rather than altering the data in order to use some procedure having requirements too restrictive for the situation at hand. 1.7 Transformations Transformations are used for three purposes: to make data more symmetric, to make data more linear, and to make data more constant in variance. Some water resources scientists fear that by transforming data, results are derived which fit preconceived ideas. Therefore, transformations are methods to ‘see what you want to see’ about the data. But in reality, serious problems can occur when procedures assuming symmetry, linearity, or homoscedasticity (constant variance) are used on data which do not possess these required characteristics. Transformations can produce these characteristics, and thus the use of transformed variables meets an objective. Employment of a transformation is not merely an arbitrary choice. One unit of measurement is no more valid a priori than any other. For example, the negative logarithm of hydrogen ion concentration, pH, is as valid a measurement system as hydrogen ion concentration itself. Transformations like the square root of depth to water at a well, or cube root of precipitation volume, should bear no more stigma than does pH. These measurement scales may be more appropriate for data analysis than are the original units. Hoaglin (1988) has written an excellent article on hidden transformations, consistently taken for granted, which are in common use by everyone. Octaves in music are a logarithmic transform of frequency. Each time a piano is played a logarithmic transform is employed! Similarly, the Richter scale for earthquakes, miles per gallon for gasoline consumption, f-stops for camera exposures, etc. all employ transformations. In the science of data analysis, the decision of which measurement scale to use should be determined by the data, not by preconceived criteria. The objectives for use of transformations are those of symmetry, linearity and homoscedasticity. In addition, the use of many resistant techniques such as percentiles and nonparametric test procedures (to be discussed later) are invariant to measurement scale. The results of a rank-sum test, the nonparametric equivalent of a t-test, will be exactly the same whether the original units or logarithms of those units are employed. 1.7.1 The Ladder of Powers In order to make an asymmetric distribution become more symmetric, the data can be transformed or re-expressed into new units. These new units alter the distances between observations on a line plot. The effect is to either expand or contract the distances to extreme observations on one side of the median, making it look more like the other side. The most commonly-used transformation in water resources is the logarithm. Logs of water discharge, hydraulic conductivity, or concentration are often taken before statistical analyses are performed. Transformations usually involve power functions of the form \\(y = x^{\\mathbf{θ}}\\), where x is the untransformed data, y the transformed data, and θ the power exponent. In figure 1.6 the values of θ are listed in the “ladder of powers” (Velleman and Hoaglin (1981)), a useful structure for determining a proper value of θ. As can be seen from the ladder of powers, any transformations with θ less than 1 may be used to make right-skewed data more symmetric. Constructing a boxplot or Q-Q plot (see Chapter 2) of the transformed data will indicate whether the transformation was appropriate. Should a logarithmic transformation overcompensate for right skewness and produce a slightly leftskewed distribution, a ‘milder’ transformation with θ closer to 1, such as a square-root or cuberoot transformation, should be employed instead. Transformations with θ &gt; 1 will aid in making left-skewed data more symmetric. Figure 1.6: “LADDER OF POWERS” (modified from Velleman and Hoaglin (1981)) However, the tendency to search for the ‘best’ transformation should be avoided. For example, when dealing with several similar data sets, it is probably better to find one transformation which works reasonably well for all, rather than using slightly different ones for each. It must be remembered that each data set is a sample from a larger population, and another sample from the same population will likely indicate a slightly different ‘best’ transformation. Determination of ‘best’ in great precision is an approach that is rarely worth the effort. Exercises 1.1 Yields in wells penetrating rock units without fractures were measured by Wright (1985), and are given below. Calculate the mean trimmed mean geometric mean median compare these estimates of location. Why do they differ? Unit well yields (in gal/min/ft) in Virginia (Wright (1985)) 0.001 0.030 0.10 0.003 0.040 0.454 0.007 0.041 0.49 0.020 0.077 1.02 library(data.table) library(DescTools) x &lt;- fread(&#39;data/well_yields_virginia.csv&#39;) a &lt;- mean(x$unit_well_yield) b &lt;- mean(x$unit_well_yield, trim = 0.25) c &lt;- Gmean(x$unit_well_yield) d &lt;- median(x$unit_well_yield) results &lt;- data.frame(c(a, b, c, d), row.names = c(&quot;Mean&quot;, &quot;Trimmed Mean&quot;, &quot;Geometric Mean&quot;, &quot;Median&quot;)) setnames(results, &quot;Value&quot;) knitr::kable(results) Value Mean 0.1902500 Trimmed Mean 0.0513333 Geometric Mean 0.0426380 Median 0.0405000 They differ because the data are skewed. The estimates which are more robust are similar, while the mean is larger. 1.2 For the well yield data of exercise 1.1, calculate the standard deviation interquartile range MAD skew and quartile skew. Discuss the differences between a through c. library(data.table) library(DescTools) library(e1071) x &lt;- fread(&#39;data/well_yields_virginia.csv&#39;) a &lt;- sd(x$unit_well_yield) b &lt;- IQR(x$unit_well_yield, type = 2) c &lt;- mad(x$unit_well_yield, constant = 1) d_1 &lt;- skewness(x$unit_well_yield, type = 2) quartiles_x &lt;- quantile(x$unit_well_yield, prob = c(0.25, 0.50, 0.75), type = 2, names = FALSE) d_2 &lt;- ((quartiles_x[3] - quartiles_x[2]) - (quartiles_x[2] - quartiles_x[1])) / (quartiles_x[3] - quartiles_x[1]) results &lt;- data.frame(c(a, b, c, d_1, d_2), row.names = c(&quot;Standard Deviation&quot;, &quot;IQR&quot;, &quot;MAD&quot;, &quot;Skew&quot;, &quot;Quartile Skew&quot;)) setnames(results, &quot;Value&quot;) knitr::kable(results) Value Standard Deviation 0.3123000 IQR 0.2635000 MAD 0.0370000 Skew 2.0740805 Quartile Skew 0.7950664 Because the data are asymmetric, the median difference is small, but the IQR and standard deviation are not. 1.3 Ammonia plus organic nitrogen (in mg/L) was measured in samples of precipitation by Oltmann and Shulters (1989). Some of their data are presented below. Compute summary statistics for these data. Which observation might be considered an outlier? How should this value affect the choice of summary statistics used to compute the mass of nitrogen falling per square mile. to compute a “typical” concentration and variability for these data? 0.3 0.9 0.36 0.92 0.5 1.0 0.7 9.7 0.7 1.3 library(data.table) library(DescTools) library(e1071) x &lt;- c(0.3, 0.9, 0.36, 0.92, 0.5, 1.0, 0.7, 9.7, 0.7, 1.3) a &lt;- mean(x) b &lt;- median(x) c &lt;- Gmean(x) d &lt;- skewness(x, type = 2) e &lt;- sd(x) f &lt;- IQR(x, type = 2) g &lt;- mad(x, constant = 1) quartiles_x &lt;- quantile(x, prob = c(0.25, 0.50, 0.75), type = 2, names = FALSE) h &lt;- ((quartiles_x[3] - quartiles_x[2]) - (quartiles_x[2] - quartiles_x[1])) / (quartiles_x[3] - quartiles_x[1]) results &lt;- data.frame(c(a, b, c, d, e, f, g, h), row.names = c(&quot;Mean&quot;, &quot;Median&quot;, &quot;Geometric Mean&quot;, &quot;Skew&quot;, &quot;Standard Deviation&quot;, &quot;IQR&quot;, &quot;MAD&quot;, &quot;Quartile Skew&quot;)) setnames(results, &quot;Value&quot;) knitr::kable(results) Value Mean 1.6380000 Median 0.8000000 Geometric Mean 0.8792928 Skew 3.0947642 Standard Deviation 2.8490848 IQR 0.5000000 MAD 0.2500000 Quartile Skew -0.2000000 The largest observation is an outlier. Though the skew appears to be strongly positive, and the standard deviation large, this is due only to the effect of that one point. The majority of the data are not skewed, as shown by the more resistant quartile skew coefficient. assuming the outlying observation is accurate, representing some high-nitrogen location which is important to have sampled, the mean must be used to compute the mass of nitrogen falling per square mile. It would probably be computed by weighting concentrations by the surface area represented by each environment. The median would under-represent this mass loading. the median would be a better “typical” concentration, and the IQR a better “typical” variability, than the mean and standard deviation. This is due to the strong effect of the one unusual point on these traditional measures. References "],
["ch2.html", "Chapter 2 Graphical Data Analysis 2.1 Graphical Analysis of Single Data Sets 2.2 Graphical Comparisons of Two or More Data Sets 2.3 Scatterplots and Enhancements 2.4 Graphs for Multivariate Data Exercises", " Chapter 2 Graphical Data Analysis Perhaps it seems odd that a chapter on graphics appears at the front of a text on statistical methods. We believe this is very appropriate, as graphs provide crucial information to the data analyst which is difficult to obtain in any other way. For example, figure 2.1 shows eight scatterplots, all of which have exactly the same correlation coefficient. Computing statistical measures without looking at a plot is an invitation to misunderstanding data, as figure 2.1 illustrates. Graphs provide visual summaries of data which more quickly and completely describe essential information than do tables of numbers. Graphs are essential for two purposes: to provide insight for the analyst into the data under scrutiny, and to illustrate important concepts when presenting the results to others. The first of these tasks has been called exploratory data analysis (EDA), and is the subject of this chapter. EDA procedures often are (or should be) the ‘first look’ at data. Patterns and theories of how the system behaves are developed by observing the data through graphs. These are inductive procedures – the data are summarized rather than tested. Their results provide guidance for the selection of appropriate deductive hypothesis testing procedures. Once an analysis is complete, the findings must be reported to others. Whether a written report or oral presentation, the analyst must convince the audience that the conclusions reached are supported by the data. No better way exists to do this than through graphics. Many of the same graphical methods which have concisely summarized the information for the analyst will also provide insight into the data for the reader or audience. The chapter begins with a discussion of graphical methods for analysis of a single data set. Two methods are particularly useful: boxplots and probability plots. Their construction is presented in detail. Next, methods for comparison of two or more groups of data are discussed. Then bivariate plots (scatterplots) are presented, with an especially useful enhancement called a smooth. The chapter ends with a discussion of plots appropriate for multivariate data. Figure 2.1: Eight scatterplots all with correlation coefficient r = 0.70 (Chambers et al. (1983)) Throughout sections 2.1 and 2.2 two data sets will be used to compare and contrast the effectiveness of each graphical method. These are annual streamflow (in cubic feet per second, or cfs) for the Licking River at Catawba, Kentucky, from 1929 through 1983, and unit well yields (in gallons per minute per foot of water-bearing material) for valleys without fracturing in Virginia (Wright (1985)). 2.1 Graphical Analysis of Single Data Sets 2.1.1 Histograms Histograms are familiar graphics, and their construction is detailed in numerous introductory texts on statistics. Bars are drawn whose height is the number ni, or fraction ni/n, of data falling into one of several categories or intervals (figures 2.2 &amp; 2.3). Iman and Conover (1983) suggest that, for a sample size of n, the number of intervals k should be the smallest integer such that 2k ≥ n. Histograms have one primary deficiency – their visual impression depends on the number of categories selected for the plot. For example, compare figure 2.2 with 2.3. Both are histograms of the same data: annual streamflows for the Licking River. Comparisons of shape and similarity among these two figures and the many other possible histograms of the same data depend on the choice of bar widths and centers. False impressions that these are different distributions might be given by characteristics such as the gap around 6,250 cfs. It is seen in 2.3 but not in 2.2. Histograms are quite useful for depicting large differences in shape or symmetry, such as whether a data set appears symmetric or skewed. They cannot be used for more precise judgements such as depicting individual values. Thus from figure 2.2 the lowest flow is seen to be larger than 750 cfs, but might be as large as 2,250 cfs. More detail is given in 2.3, but this lowest observed discharge is still only known to be somewhere between 500 to 1,000 cfs. For data measured on a continuous scale (such as streamflow or concentration), histograms are not the best method for graphical analysis. The process of forcing continuous data into discrete categories may obscure important characteristics of the distribution. However, histograms are excellent when displaying data which have natural categories or groupings. Examples of such data would include the number of individual organisms found at a stream site grouped by species type, or the number of water-supply wells exceeding some critical yield grouped by geologic unit. Figure 2.2: Histogram of annual streamflow for the Licking River Figure 2.3: Second histogram of same data, but with different interval divisions 2.1.2 Stem and Leaf Diagrams Figure 2.4 shows a stem and leaf (S-L) diagram for the Licking River streamflow data with the same divisions as in figure 2.3. Stem and leaf diagrams are like histograms turned on their side, with data magnitudes to two significant digits presented rather than only bar heights. Individual values are easily found. The S-L profile is identical to the histogram and can similarly be used to judge shape and symmetry, but the numerical information adds greater detail. One S-L could function as both a table and a histogram for small data sets. An S-L is constructed by dividing the range of the data into roughly 10 intervals, and placing the first digit corresponding to these intervals to the left of the vertical line. This is the ‘stem’, ranging from 1 to 8 (0 to 8000+ cfs) in figure 2.4*. Each observation is then represented by one digit to the right of the line (the ‘leaves’), so that the number of leaves equals the number of observations falling into that interval. To provide more detail, figure 2.4* has two lines for each stem digit, split to allow 5 leaf digits per line (0-4 and 5-9). Here the first stem is the stem for leaves less than 5, and the second stem for leaves greater than or equal to 5. For example, in figure 2.4* three observations occur between 1500 and 2000 cfs, with values of 1800, 1900, and 1900 cfs. The lowest flow is now seen to be between 1000 and 1500 cfs. The gap between 7,000 to 7,500 cfs is still evident, and now the numerical values of the three highest flows are presented. Comparisons between distributions still remain difficult using S-L plots, however, due to the required arbitrary choice of group boundaries. ## ## The decimal point is 3 digit(s) to the right of the | ## ## 1 | 34 ## 1 | 899 ## 2 | 2 ## 2 | 566789 ## 3 | 013 ## 3 | 5667789 ## 4 | 000111222 ## 4 | 5558 ## 5 | 00034 ## 5 | 66789 ## 6 | 12 ## 6 | 5 ## 7 | ## 7 | 5 ## 8 | 0 ## [1] &quot;Fig 2.4* Stem and Leaf Plot of Annual Streamflow&quot; 2.1.3 Quantile Plots Quantile plots visually portray the quantiles, or percentiles (which equal the quantiles times 100) of the distribution of sample data. Quantiles of importance such as the median are easily discerned (quantile, or cumulative frequency = 0.5). With experience, the spread and skewness of the data, as well as any bimodal character, can be examined. Quantile plots have three advantages: Arbitrary categories are not required, as with histograms or S-L’s. All of the data are displayed, unlike a boxplot. Every point has a distinct position, without overlap. Figure 2.4 is a quantile plot of the streamflow data from figures 2.2 and 2.3. Attributes of the data such as the gap between 6500 and 7500 cfs (indicated by the nearly horizontal line segment) are evident. The percent of data in the sample less than a given cfs value can be read from the graph with much greater accuracy than from a histogram. Figure 2.4: Quantile plot of the Licking River annual streamflow data 2.1.3.1 Construction of a quantile plot To construct a quantile plot, the data are ranked from smallest to largest. The smallest data value is assigned a rank i = 1, while the largest receives a rank i = n, where n is the sample size of the data set. The data values themselves are plotted along one axis, usually the horizontal axis. On the other axis is the “plotting position”, which is a function of the rank i and sample size n. As discussed in the next section, the Cunnane plotting position pi = (i−0.4)/(n+0.2) is used in this book. Below are listed the first and last 5 of the 50 data pairs used in construction of figure 2.4. When tied data values are present, each is assigned a separate plotting position (the plotting positions are not averaged). In this way tied values are portrayed as a vertical “cliff” on the plot. ## idx V1 p_i ## 1: 1 1253.083 0.01195219 ## 2: 2 1357.667 0.03187251 ## 3: 3 1833.891 0.05179283 ## 4: 4 1861.817 0.07171315 ## 5: 5 1891.042 0.09163347 ## 6: 46 6078.083 0.90836653 ## 7: 47 6235.858 0.92828685 ## 8: 48 6535.075 0.94820717 ## 9: 49 7513.200 0.96812749 ## 10: 50 7984.617 0.98804781 Quantile plots are sample approximations of the cumulative distribution function (cdf) of a continuous random variable. The cdf for a normal distribution is shown in figure 2.7. A second approximation is the sample (or empirical) cdf, which differs from quantile plots in its vertical scale. The vertical axis of a sample cdf is the probability i/n of being less than or equal to that observation. The largest observation has i/n = 1, and so has a zero probability of being exceeded. For samples (subsets) taken from a population, a nonzero probability of exceeding the largest value observed thus far should be recognized. This is done by using the plotting position, a value less than i/n, on the vertical axis of the quantile plot. As sample sizes increase, the quantile plot will more closely mimic the underlying population cdf. 2.1.3.1.1 Plotting positions Variations of quantile plots are used frequently for three purposes: to compare two or more data distributions (a Q-Q plot), to compare data to a normal distribution (a probability plot), and to calculate frequencies of exceedance (a flow-duration curve). Unfortunately, different plotting positions have traditionally been used for each of the above three purposes. It would be desirable instead to use one formula that is suitable for all three. Numerous plotting position formulas have been suggested, most having the general formula \\[p = (i − a) / (n + 1 − 2a)\\] where a varies from 0 to 0.5. Five of the most commonly-used formulas are: Reference a Formula Weibull (1939) 0 \\(i / (n + 1)\\) Blom (1958) 0.375 \\((i - 0.375) / (n + 0.25)\\) Cunnane (1978) 0.4 \\((i - 0.4) / (n + 0.2)\\) Gringorten (1963) 0.44 \\((i - 0.44) / (n + 0.12)\\) Hazen (1914) 0.5 \\((i - 0.5) / n\\) The Weibull formula has long been used by hydrologists in the United States for plotting flowduration and flood-frequency curves (Langbein (1960)). It is used in Bulletin 17B, the standard reference for determining flood frequencies in the United States (Interagency Advisory Committee on Water Data (1982)). The Blom formula is best for comparing data quantiles to those of a normal distribution in probability plots, though all of the above formulas except the Weibull are acceptable for that purpose (Looney and Gulledge Jr (1985a)). The Hazen formula is used by Chambers et al. (1983) for comparing two or more data sets using Q-Q plots. Separate formulae could be used for the situations in which each is optimal. In this book we instead use one formula, the Cunnane formula given above, for all three purposes. We do this in an attempt to simplify. The Cunnane formula was chosen because it is acceptable for normal probability plots, being very close to Blom. it is used by Canadian and some European hydrologists for plotting flowduration and flood-frequency curves. Cunnane (1978) presents the arguments for use of this formula over the Weibull when calculating exceedance probabilities. For convenience when dealing with small sample sizes, table B1 of the Appendix presents Cunnane plotting positions for sample sizes n = 5 to 20. 2.1.4 Boxplots A very useful and concise graphical display for summarizing the distribution of a data set is the boxplot (figure 2.5). Boxplots provide visual summaries of the center of the data (the median–the center line of the box) the variation or spread (interquartile range–the box height) the skewness (quartile skew–the relative size of box halves) presence or absence of unusual values (“outside” and “far outside” values). Boxplots are even more useful in comparing these attributes among several data sets. Compare figures 2.4 and 2.5, both of the Licking River data. Boxplots do not present all of the data, as do stem-and-leaf or quantile plots. Yet presenting all data may be more detail than is necessary, or even desirable. Boxplots do provide concise visual summaries of essential data characteristics. For example, the symmetry of the Licking River data is shown in figure 2.5 by the similar sizes of top and bottom box halves, and by the similar lengths of whiskers. In contrast, in figure 2.6 the taller top box halves and whiskers indicate a right-skewed distribution, the most commonly occurring shape for water resources data. Boxplots are often put side-by-side to visually compare and contrast groups of data. Three commonly used versions of the boxplot are described as follows (figure 2.6). Any of the three may appropriately be called a boxplot. Figure 2.5: Boxplot of the Licking River annual streamflow data 2.1.4.1 Simple boxplot The simple boxplot was originally called a “box-and-whisker” plot by Tukey (1977). It consists of a center line (the median) splitting a rectangle defined by the upper and lower hinges (very similar to quartiles – see appendix). Whiskers are lines drawn from the ends of the box to the maximum and minimum of the data, as depicted in graph a of figure 2.6. 2.1.4.2 Standard boxplot Tukey’s “schematic plot” has become the most commonly used version of a boxplot (middle graph in figure 2.6), and will be the type of boxplot used throughout this book. With this standard boxplot, outlying values are distinguished from the rest of the plot. The box is as defined above. However, the whiskers are shortened to extend only to the last observation within one step beyond either end of the box (“adjacent values”). A step equals 1.5 times the height of the box (1.5 times the interquartile range). Observations between one and two steps from the box in either direction, if present, are plotted individually with an asterisk (“outside values”). Outside values occur fewer than once in 100 times for data from a normal distribution. Observations farther than two steps beyond the box, if present, are distinguished by plotting them with a small circle (“far-out values”). These occur fewer than once in 300,000 times for a normal distribution. The occurrence of outside or far-out values more frequently than expected gives a quick visual indication that data may not originate from a normal distribution. 2.1.4.3 Truncated boxplot In a third version of the boxplot (left graph of figure 2.6), the whiskers are drawn only to the 90th and 10th percentiles of the data set. The largest 10 percent and smallest 10 percent of the data are not shown. This version could easily be confused with the simple boxplot, as no data appear beyond the whiskers, and should be clearly defined as having eliminated the most extreme 20 percent of data. It should be used only when the extreme 20 percent of data are not of interest. In a variation on the truncated boxplot, Cleveland and McGill (1985) plotted all observations beyond the 10th and 90th percentile-whiskers individually, calling this a “box graph”. The weakness of this style of graph is that 10 percent of the data will always be plotted individually at each end, and so the plot is far less effective than a standard boxplot for defining and emphasizing unusual values. Further detail on construction of boxplots may be found in the appendix, and in Chambers et al. (1983) and McGill, Tukey, and Larsen (1978). Figure 2.6: Three versions of the boxplot (unit well yield data). 2.1.5 Probability Plots Probability plots are used to determine how well data fit a theoretical distribution, such as the normal, lognormal, or gamma distributions. This could be attempted by visually comparing histograms of sample data to density curves of the theoretical distributions such as figures 1.1 and 1.2. However, research into human perception has shown that departures from straight lines are discerned more easily than departures from curvilinear patterns. By expressing the theoretical distribution as a straight line, departures from the distribution are more easily perceived. This is what occurs with a probability plot. To construct a probability plot, quantiles of sample data are plotted against quantiles of the standardized theoretical distribution. In figure 2.7, quantiles from the quantile plot of the Licking River streamflow data (lower scale) are overlain with the S-shaped quantiles of the standard normal distribution (upper scale). For a given cumulative frequency (plotting position, p), quantiles from each curve are paired and plotted as one point on the probability plot, figure 2.8. Note that quantiles of the data are simply the observation values themselves, the pth quantiles where \\(p = (i − 0.4)/(n + 0.2)\\). Quantiles of the standard normal distribution are available in table form in most textbooks on statistics. Thus, for each observation, a pair of quantiles is plotted in figure 2.8 as one point. For example, the median \\((p = 0.5)\\) equals 0 for the standard normal, and 4068 cfs for the Licking River data. The point (0,4068) is one point included in figure 2.8. Data closely approximating the shape of the theoretical distribution, in this case a normal distribution, will plot near to a straight line. To illustrate the construction of a probability plot in detail, data on unit well yields (yi) from Wright (1985) will be plotted versus their normal quantiles (also called normal scores). The data are ranked from the smallest (\\(i = 1\\)) to largest (i=n), and their corresponding plotting positions \\(p_{i} = (i − 0.4)/(n + 0.2)\\) calculated. Normal quantiles (Zp) for a given plotting position pi may be obtained in one of three ways: from a table of the standard normal distribution found in most statistics textbooks from table B2 in the Appendix, which presents standard normal quantiles for the Cunnane plotting positions of table B1 from a computerized approximation to the inverse standard normal distribution available in many statistical packages, or as listed by Zelen and Severo (1964). Entering the table with \\(p_{i} = .05\\), for example, will provide a \\(Zp = − 1.65\\). Note that since the median of the standard normal distribution is 0, Zp will be symmetrical about the median, and only half of the Zp values must be looked up: Figure 2.7: Overlay of Licking River and standard normal distribution quantile plots Figure 2.8: Probability plot of the Licking River data ## unit_well_yield p_i zp ## 1: 0.001 0.04918033 -1.6528536 ## 2: 0.003 0.13114754 -1.1209830 ## 3: 0.007 0.21311475 -0.7956603 ## 4: 0.020 0.29508197 -0.5385985 ## 5: 0.030 0.37704918 -0.3132400 ## 6: 0.040 0.45901639 -0.1029120 ## 7: 0.041 0.54098361 0.1029120 ## 8: 0.077 0.62295082 0.3132400 ## 9: 0.100 0.70491803 0.5385985 ## 10: 0.454 0.78688525 0.7956603 ## 11: 0.490 0.86885246 1.1209830 ## 12: 1.020 0.95081967 1.6528536 For comparison purposes, it is helpful to plot a reference straight line on the plot. The solid line on figure 2.8 is the normal distribution which has the same mean and standard deviation as do the sample data. This reference line is constructed by plotting \\(\\overline{y}\\) as the y intercept of the line (\\(Zp = 0\\)), so that the line is centered at the point (\\(0, \\overline{y}\\)), the mean of both sets of quantiles. The standard deviation \\(s\\) is the slope of the line on a normal probability plot, as the quantiles of a standard normal distribution are in units of standard deviation. Thus the line connects the points (\\(0, \\overline{y}\\)) and (\\(1 , \\overline{y} + s\\)). 2.1.5.1 Probability paper Specialized ‘probability paper’ is often used for probability plots. This paper simply retransforms the linear scale for quantiles of the standard distribution back into a nonlinear scale of plotting positions (figure 2.9). There is no difference between the two versions except for the horizontal scale. With probability paper the horizontal axis can be directly interpreted as the percent probability of occurrence, the plotting position times 100. The linear quantile scale of figure 2.8 is sometimes included on probability paper as ‘probits,’ where a probit = normal quantile + 5.0. Probability paper is available for distributions other than the normal, but all are constructed the same way, using standardized quantiles of the theoretical distribution. In figure 2.9 the lower horizontal scale results from sorting the data in increasing order, and assigning rank 1 to the smallest value. This is commonly done in water-quality and low-flow studies. Had the data been sorted in decreasing order, assigning rank 1 to the largest value as is done in flood-flow studies, the upper scale would result – the percent exceedance. Either horizontal scale may be obtained by subtracting the other from 100 percent. Figure 2.9: Probability plot of Licking River data on probability paper 2.1.5.2 Deviations from a linear pattern If probability plots do not exhibit a linear pattern, their nonlinearity will indicate why the data do not fit the theoretical distribution. This is additional information that hypothesis tests for normality (described later) do not provide. Three typical conditions resulting in deviations from linearity are: asymmetry or skewness, outliers, and heavy tails of the distribution. These are discussed below. Figure 2.10 is a probability plot of the base 10 logarithms of the Licking River data. The data are negatively (left) skewed. This is seen in figure 2.10 as a greater slope on the left-hand side of the plot, producing a slightly convex shape. Figure 2.11 shows a right-skewed distribution, the unit well yield data. The lower bound of zero, and the large slope on the right-hand side of the plot produces an overall concave shape. Thus probability plots can be used to indicate what type of transformation is needed to produce a more symmetric distribution. The degree of curvature gives some indication of the severity of skewness, and therefore the degree of transformation required. Outliers appear on probability plots as departures from the pattern of the rest of the data. Figure 2.12 is a probability plot of the Licking River data, but the two largest observations have been altered (multiplied by 3). Compare figures 2.12 and 2.8. Note that the majority of points in figure 2.12 still retain a linear pattern, with the two outliers offset from that pattern. Note that the straight line, a normal distribution with mean and standard deviation equal to those of the altered data, does not fit the data well. This is because the mean and standard deviation are inflated by the two outliers. The third departure from linearity occurs when more data are present in both tails (areas furthest from the median) than would be expected for a normal distribution. Figure 2.13 is a probability plot of adjusted nitrate concentrations in precipitation from Wellston, Michigan (Schertz and Hirsch (1985)). These data are actually residuals (departures) from a regression of log of nitrate concentration versus log of precipitation volume. A residual of 0 indicates that the concentration is exactly what would be expected for that volume, a positive residual more than what is expected, and negative less than expected. The data in figure 2.13 display a predominantly linear pattern, yet one not fit well by the theoretical normal shown as the solid line. Again this lack of fit indicates outliers are present. The outliers are data to the left which plot below the linear pattern, and those above the pattern to the right of the figure. Outliers occur on both ends in greater numbers than expected from a normal distribution. A boxplot for the data is shown in figure 2.14 for comparison. Note that both the box and whiskers are symmetric, and therefore no power transformation such as those in the “ladder of powers” would produce a more nearly normal distribution. Data may depart from a normal distribution not only in skewness, but by the number of extreme values. Excessive numbers of extreme values may cause significance levels of tests requiring the normality assumption to be in error. Therefore procedures which assume normality for their validity when applied to data of this type may produce quite inaccurate results. Figure 2.10: Probability plot of a left-skewed distribution (logs of Licking River data) Figure 2.11: Probability plot of a right-skewed distribution (unit well yields) ## Warning: Removed 6 rows containing missing values (geom_path). Figure 2.12: Probability plot of data with high outliers Figure 2.13: Probability plot of a heavy-tailed data set Figure 2.14: Boxplot of a heavy-tailed data set 2.1.5.3 Probability plots for comparing among distributions In addition to comparisons to a normal distribution, quantiles may be computed and probability plots constructed for any two-parameter distribution. The distribution which causes data to be most like a straight line on its probability plot is the one which most closely resembles the distributional shape of the data. Data may be compared to a two-parameter lognormal distribution by simply plotting the logarithms of the data as the data quantiles, as was done in figure 2.10. Vogel (1986) demonstrated the construction of probability plots for the Gumbel (extreme-value) distribution, which is sometimes employed for flood-flow studies. Vogel and Kroll (1989) cover the use of probability plots for the two-parameter Weibull distribution, used in fitting low-flow data. Again, the best fit is obtained with the distribution which most closely produces a linear plot. In both references, the use of a test of significance called the probability plot correlation coefficient augmented the visual determination of linearity on the plot. This test will be covered in detail in Chapter 4. Use of three-parameter distributions can also be indicated by probability plots. For example, if significant right-skewness remains after logarithms are taken, the resulting concave shape on a lognormal probability plot indicates that a log-Pearson III distribution would better fit the data. Vogel and Kroll (1989) demonstrate the construction of a probability plot for the log-Pearson III distribution using a Wilson-Hilferty transformation. 2.2 Graphical Comparisons of Two or More Data Sets Each of the graphical methods discussed thus far can be, and have been, used for comparing more than one group of data. However, each is not equally effective. As the following sections show, histograms are not capable of providing visual comparisons between data sets at the same level of detail as boxplots or probability plots. Boxplots excel in clarity and easy discrimination of important distributional characteristics, even for comparisons between many groups of data. A newer type of plot, the quantile-quantile (Q-Q) plot, provides additional information about the relationship between two data sets. Each graphic will be developed for the same data set, a comparison of unit well yields in Virginia (Wright (1985)). These are small data sets: 13 wells are from valleys underlain by fractured rocks, and 12 wells from valleys underlain by unfractured rocks. 2.2.1 Histograms Figure 2.15 presents histograms for the two sets of well yield data. The right-skewness of each data set is easily seen, but it is difficult to discern whether any differences exist between them. Histograms do not provide a good visual picture of the centers of the distributions, and only a slightly better comparison of spreads. Positioning histograms side-by-side instead of one above the other provide even less ability to compare data, as the data axes would not be aligned. Unfortunately, this is commonly done. Also common are overlapping histograms, such as in figure 2.16. Overlapping histograms provide poor visual discrimination between multiple data sets. Figure 2.15: Histogram of the unit well yield data Figure 2.16: Overlapping histograms of the unit well yield data 2.2.2 Dot and Line Plots of Means, Standard Deviations Figure 2.17 is a “dot and line” plot often used to represent the mean and standard deviation (or standard error) of data sets. Each dot is the mean of the data set. The bars extend to plus and minus either one standard deviation (shown), or plus and minus one or more standard errors \\((s.e. = s / \\sqrt{n})\\), beyond the mean. This plot displays differences in mean yields, but little else. No information on the symmetry of the data or presence of outliers is available. Because of this, there is not much information given on the spread of the data, as the standard deviation may describe the spread of most of the data, or may be strongly influenced by skewness and a few outliers. Figure 2.17: Dot and line plot for the unit well yield data To emphasize the deficiencies of dot and line plots such as these, figure 2.18 presents three data sets with very different characteristics. The first is a uniform distribution of values between 0 and 20. It is symmetric. The second is a right-skewed data set with outliers. The third is a bimodal distribution, also symmetric. All three have a mean of 10 and standard deviation of 6.63. Therefore each of the three would be represented by the same dot and line plot, shown at the right of the figure. Dot and line plots are useful only when the data are actually symmetric. If skewness or outliers are present, as with data set 2, neither the plots (or a table of means and standard deviations) indicate their presence. Even for symmetric distributions, differences such as those between data sets 1 and 3 will not be evident. Far better graphical methods are available. Figure 2.18: Number lines of 3 dissimilar groups of data, all having an identical dot and line plot (shown at right) 2.2.3 Boxplots Figure 2.19 presents boxplots of the well yield data. The median well yield is seen to be higher for the areas with fractures. The IQR of wells with fractures is slightly larger than that for wells without, and the highest value for each group is similar. Both data sets are seen to be rightskewed. Thus a large amount of information is contained in this very concise illustration. The mean yield, particularly for wells without fractures, is undoubtedly inflated due to skewness, and differences between the two groups of data will in general be larger than indicated by the differences in their mean values. Figure 2.19: Boxplots of the unit well yield data In figure 2.20, boxplots of the three data sets given in figure 2.18 are presented. The skewness of data set 2 is clear, as is the symmetry of 1 and 3. The difference in shape between 1 and 3 is evident. The minute whiskers of data set 3 illustrate that over 25 percent of the data are located essentially at the upper and lower quartiles – a bimodal distribution. Figure 2.20: Boxplots of the 3 dissimilar groups of data shown in figure 2.18 The characteristics which make boxplots useful for inspecting a single data set make them even more useful for comparing multiple data sets. They are valuable guides in determining whether central values, spread, and symmetry differ among groups of data. They will be used in later chapters to guide whether tests based on assumptions of normality may be employed. The essential characteristics of numerous groups of data may be displayed in a small space. For example, the 20 boxplots of figure 2.21 were used by Holtschlag (1987) to illustrate the source of ammonia nitrogen on a section of the Detroit River. The Windmill Point Transect is upstream of the U.S. city of Detroit, while the Fermi Transect is below the city. Note the marked changes in concentration (the median lines of the boxplots) and variability (the widths of the boxes) on the Michigan side of the river downstream of Detroit. A lot of information on streamwater quality is succinctly summarized in this relatively small figure. Figure 2.21: Boxplots of total ammonia nitrogen concentrations (mg/L as N) at two transects on the Detroit River (from Holtschlag (1987)) 2.2.4 Probability Plots Probability plots are also useful graphics for comparing groups of data. Characteristics evident in boxplots are also seen using probability plots, though in a different format. Comparisons of each quantile, not just the boxplot quartiles, can be made. The straightness of each data set also allows quick comparisons to conformity with the theoretical distribution. Figure 2.22 is a probability plot of the two well yield data sets. The right-skewness of each data set is shown by their concave shapes. Wells without fractures have greater skewness as shown by their greater concavity on the plot. Quantiles of the wells with fractures are higher than those without, indicating generally higher yields. Figure 2.22 shows that the lowest yields in each group are similar, as both data sets approach zero yield. Also seen are the similarity in the highest yield for each group, due to the outlier for the without fractures group. Comparisons between median values are simple to do – just travel up the normal quantile = 0 line. Comparisons of spreads are more difficult – the slopes of each data set display their spread. Figure 2.22: Probability plot of the unit well yield data In general, boxplots summarize the differences between data groups in a manner more quickly discerned by the viewer. When comparisons to a particular theoretical distribution such as the normal are important, or comparisons between quantiles other than the quartiles are necessary, probability plots are useful graphics. Either have many advantages over histograms or dot and line plots. 2.2.5 Q-Q Plots Direct comparisons can be made between two data sets by graphing the quantiles (percentiles) of one versus the quantiles (percentiles) of the second. This is called a quantile-quantile or Q-Q plot (Chambers et al. (1983)). If the two data sets came from the same distribution, the quantile pairs would plot along a straight line with \\(Yp = Xp\\), where p is the plotting position and Yp is the pth quantile of Y. In this case it would be said that the median, the quartiles, the 10th and 90th percentiles, etc., of the two data sets were equal. If one data set had the same shape as the second, differing only by an additive amount (each quantile was 5 units higher than for the other data set, for example), the quantile pairs would fall along a line parallel to but offset from the \\(Yp = Xp\\) line, also with slope = 1. If the data sets differed by a multiplicative constant (\\(Yp = 5 \\bullet Xp\\), for example), the quantile pairs would lie along a straight line with slope equal to the multiplicative constant. More complex relationships will result in pairs of quantiles which do not lie along a straight line. The question of whether or not data sets differ by additive or multiplicative relationships will become important when hypothesis testing is conducted. Figure 2.23 is a Q-Q plot of the well yield data. Several aspects of the relationship between the two data sets are immediately seen. First, the lowest 9 quantile pairs appear to fall along a straight line with a slope greater than 1, not parallel to the \\(Yp = Xp\\) line shown as a reference. This indicates a multiplicative relation between the data, with \\(Y \\cong 4.4 \\bullet X\\), where 4.4 is the slope of those data on the plot. Therefore, the yields with fractures are generally 4.4 times those without fractures for the lowest 75 percent of the data. The 3 highest quantile pairs return near to the \\(Y = X\\) line, indicating that the higher yields in the two data sets approach being equal. The hydrologist might be able to explain this phenomenon, such as higher yielding wells are deeper and less dependent on fracturing, or that some of the wells were misclassified, etc. Therefore the Q-Q plot becomes a valuable tool in understanding the relationships between data sets prior to performing any hypothesis tests. Figure 2.23: Q-Q plot of the unit well yield data 2.2.5.1 Construction of Q-Q plots Q-Q plots are similar to probability plots. Now instead of plotting data quantiles from one group against quantiles of a theoretical distribution such as the normal, they are plotted against quantiles of a second data group. When sample sizes of the two groups are identical, the x’s and y’s can be ranked separately, and the Q-Q plot is simply a scatterplot of the ordered data pairs \\((x_{1}, y_{1}) \\cdots (x_{n}, y_{n})\\). When sample sizes are not equal, consider n to be the sample size of the smaller data set and m to be the sample size of the larger data set. The data values from the smaller data set are its pth quantiles, where \\(p = (i − 0.4) / (n + 0.2)\\). The n corresponding quantiles for the larger data set are interpolated values which divide the larger data set into n equally-spaced parts. The following example illustrates the procedure. For the well yield data, the 12 values without fractures designated \\(x_{i}, i = 1, \\cdots n\\) are themselves the sample quantiles for the smaller data set. Repeating the without fractures data given earlier in the chapter: The .05 quantile (5th percentile) value of 0.001, for example, is to be paired on the Q-Q plot with the .05 quantile of the yields with fractures. To compute the corresponding y quantiles for the second data set, \\(p = (j − 0.4) / (m + 0.2)\\), and therefore j must be: \\[\\begin{equation} \\begin{aligned} \\frac{(j - 0.4)}{(m + 0.2)} &amp; = \\frac{(i - 0.4)}{(n + 0.2)} &amp;&amp; \\text{, or}\\\\ j &amp; = \\frac{(m + 0.2) \\bullet (i - 0.4)}{(n + 0.2)} + 0.4 \\end{aligned} \\tag{2.1} \\end{equation}\\] If j is an integer, the data value yj itself is plotted versus xi. Usually, however, j will lie between two integers, and the y quantile must be linearly interpolated between the y data corresponding to the ranks on either side of j: \\[\\begin{equation} \\begin{aligned} y_{j} &amp; = y_{j^{\\prime}} + \\left( j - j^{\\prime} \\right) \\bullet \\left( y_{\\left( j^{\\prime} + 1 \\right)} - y_{j^{\\prime}} \\right)\\\\ &amp;&amp; \\text{where $j^{\\prime}$ = integer(j)} \\end{aligned} \\tag{2.2} \\end{equation}\\] For example, the well yield data with fractures are the following: \\[0.020\\;\\;0.031\\;\\;0.086\\;\\;0.130\\;\\;0.160\\;\\;0.160\\;\\;0.180\\;\\;0.300\\;\\;0.400\\;\\;0.440\\;\\;0.510\\;\\;0.720\\;\\;0.950\\] Therefore \\(n = 12\\)       \\(m = 13\\)       and from eq. (2.1), \\(j = 1.08i − 0.03\\). The first of the 12 quantiles to be computed for the data with fractures is then: \\[\\begin{equation} \\begin{aligned} &amp; i = 1 &amp;&amp; j = 1.05 &amp;&amp;&amp; j^{\\prime} = 1 &amp;&amp;&amp;&amp; y_{j} = y_{1} + 0.05 \\bullet \\left( y_{2} - y_{1} \\right) \\\\ &amp; &amp;&amp; &amp;&amp;&amp; &amp;&amp;&amp;&amp; y_{j} = 0.020 + 0.05 \\bullet (0.031 + 0.020) \\\\ &amp; &amp;&amp; &amp;&amp;&amp; &amp;&amp;&amp;&amp; y_{j} = 0.021 \\end{aligned} \\end{equation}\\] All 12 quantiles are similarly interpolated: These interpolated values are added to the table of quantiles given previously: These (xi, yj) pairs are the circles which were plotted in figure 2.23. 2.3 Scatterplots and Enhancements The two-dimensional scatterplot is one of the most familiar graphical methods for data analysis. It illustrates the relationship between two variables. Of usual interest is whether that relationship appears to be linear or curved, whether different groups of data lie in separate regions of the scatterplot, and whether the variability or spread is constant over the range of data. In each case, an enhancement called a “smooth” enables the viewer to resolve these issues with greater clarity than would be possible using the scatterplot alone. The following sections discuss these three uses of the scatterplot, and the enhancements available for each use. 2.3.1 Evaluating Linearity Figure 2.24 is a scatterplot of the mass load of transported sand versus stream discharge for the Colorado River at Lees Ferry, Colorado, during 1949-1964. Are these data sufficiently linear to fit a linear regression to them, or should some other term or transformation be included in order to account for curvature? In Chapters 9 and 11, other ways to answer this question will be presented, but many judgements on linearity are made solely on the basis of plots. To aid in this judgement, a “smooth” will be superimposed on the data. The human eye is an excellent judge of the range of data on a scatterplot, but has a difficult time accurately judging the center – the pattern of how y varies with x. This results in two difficulties with judging linearity on a scatterplot as evident in figure 2.24. Outliers such as the two lowest sand concentrations may fool the observer into believing a linear model may not fit. Alternatively, true changes in slope are often difficult to discern from only a scatter of data. To aid in seeing central patterns without being strongly influenced by outliers, a resistant center line can be fit to the data whose direction and slope varies locally in response to the data themselves. Many methods are available for constructing this type of center line – probably the most familiar is the (non-resistant) moving average. All such methods may be called a “middle smooth”, as they smooth out variations in the data into a coherent pattern through the middle. We discuss computation of smooths in Chapter 10. For now, we will merely illustrate their use as aids to graphical data analysis. The smoothing procedure we prefer is called LOWESS, or LOcally WEighted Scatterplot Smoothing (Cleveland and McGill (1984); Cleveland and McGill (1985)). Figure 2.24: Suspended sand transport at Lees Ferry, Arizona, 1949-1952 Figure 2.25 presents the Lees Ferry sediment data of figure 2.24, with a superimposed middle smooth. Note the nonlinearity now evident by the curving smooth on the left-hand side of the plot. The rate of sand transport slows above 6600 (e8.8) cfs. This curvature is easier to see with the superimposed smooth. It is important to remember that no model, such as a linear or quadratic function, is assumed prior to computing a smooth. The smoothed pattern is totally derived by the pattern of the data, and may take on any shape. As such, smooths are an exploratory tool for discerning the form of relationship between y and x. Seeing the pattern of figure 2.25, a quadratic term might be added, a piecewise linear fit used, or a transformation stronger than logs used prior to performing a linear regression of concentration versus discharge (see Chapter 9). Figure 2.25: Data of figure 2.24 with superimposed lowess smooth Middle smooths should be regularly used when analyzing data on scatterplots, and when presenting those data to others. As no model form is assumed by them, they let the data describe the pattern of dependence of y on x. Smooths are especially useful when large amounts of data are to be plotted, and several groups of data are placed on the same plot. For example, Welch, Lico, and Hughes (1988) depicted the dependence of log of arsenic concentration on pH for thousands of groundwater samples throughout the western United States (figure 2.26). By using middle smooths, data from one physiographic province was seen to differ from the other three provinces in its relationship between pH and arsenic. Figure 2.26: Dependance of log(As) on pH for 4 areas in the western U.S. (Welch, Lico, and Hughes (1988)) 2.3.2 Evaluating Differences in Location on a Scatterplot Figure 2.27 is a scatterplot of conductance versus pH for samples collected at low-flow in small streams within the coal mining region of Ohio (data from Helsel (1983)). Each stream was classified by the type of land it was draining – unmined land, lands mined and later reclaimed, and lands mined and then abandoned without reclamation. These three types of upstream lands are plotted with different symbols in figure 2.27. Figure 2.27: Scatterplot of water-quality draining three types of upstream land use To see the three locations more clearly, a smooth can be constructed for each group which encloses either 50 or 75 percent of the data. This type of smooth is called a polar smooth (Cleveland and McGill (1984)), and its computation is detailed in Chapter 10. Briefly, the data are transformed into polar coordinates, a middle or similar smooth computed, and the smooth is re-transformed back into the original units. In figure 2.28. a polar smooth enclosing 75 percent of the data in each of the types of upstream land is plotted. These smooths are again not limited to a prior shape or form, such as that of an ellipse. Their shapes are determined from the data. Polar smooths can be a great aid in exploratory data analysis. For example, the irregular pattern for the polar smooth of data from abandoned lands in figure 2.28 suggests that two separate subgroups are present, one with higher pH than the other. Using different symbols for data from each of the two geologic units underlying these streams shows indeed that the basins underlain by a limestone unit have generally higher pH than those underlain by a sandstone. Therefore the type of geologic unit should be included in any analysis or model of the behavior of chemical constituents for these data. Polar smooths are especially helpful when there is a large amount of data to be plotted on a scatterplot. In such situations, the use of different symbols for distinguishing between groups will be ineffective, as the plot will be too crowded to see patterns in the locations of symbols. Indeed, in some locations it will not be possible to distinguish which symbol is plotted. Plots presenting small data points and the polar smooths as in figure 2.28, or even just the polar smooths themselves, will provide far greater visual differentiation between groups. Figure 2.28: Polar smooths for the three groups of data in figure 2.27 2.3.3 Evaluating Differences in Spread In addition to understanding where the middle of data lie on a scatterplot, it is often of interest to know something about the spread of the data as well. Homoscedasticity (constant variance) is a crucial assumption of ordinary least-squares regression, as we will see later. Changes in variance also invalidate parametric hypothesis test procedures such as analysis of variance. From a more exploratory point of view, changes in variance may be as important or more important than changes in central value. Differences between estimation methods for flood quantiles, or between methods of laboratory analysis of some chemical constituent, are often differences in repeatability of the results and not of method bias. Graphs again can aid in judging differences in data variability, and are often used for this purpose. A major problem with judgements of changing spread on a scatterplot is again that the eye is sensitive to seeing the range of data. The presence of a few unusual values may therefore incorrectly trigger a perception of changing spread. This is especially a problem when the density of data changes across a scatterplot, a common occurrence. Assuming the distribution of data to be identical across a scatterplot, and that no changes in variablility or spread actually occur, areas where data are more dense are more likely to contain outlying values on the plot, and the range of values is likely to be larger. This leads to a perception that the spread has changed. One graphical means of determining changes in spread has been given by Chambers et al. (1983). First, a middle smooth is computed, as in figure 2.25. The absolute values of differences |di| between each data point and the smooth at its value of x is a measure of spread. \\[\\begin{equation} \\begin{aligned} &amp; |d_{i}| = |y_{i} - l_{i}| &amp;&amp; \\text{where $l_{i}$ is the value for the lowess smooth at $x_{i}$} \\end{aligned} \\tag{2.3} \\end{equation}\\] By graphing these absolute differences |di| versus xi, changes in spread will show as changes in absolute differences. A middle smooth of these differences should also be added to make the pattern more clear. This is done in figure 2.29, a plot of the absolute differences between sand concentration and its lowess smooth for the Lees Ferry data of figure 2.25. Note that there is a slight decrease in |di|, indicating a small decrease of variability or spread in concentration with increasing discharge at that site. Figure 2.29: Absolute residuals show wheter the spread changes with changing x – sediment concentrations at Lees Ferry, Arizona 2.4 Graphs for Multivariate Data Boxplots effectively illustrate the characteristics of data for a single variable, and accentuate outliers for further inspection. Scatterplots effectively illustrate the relationships between two variables, and accentuate points which appear unusual in their x-y relationship. Yet there are numerous situations where relationships between more than two variables should be considered simultaneously. Similarities and differences between groups of observations based on 3 or more variables are frequently of interest. Also of interest is the detection of outliers for data with multiple variables. Graphical methods again can provide insight into these relationships. They supplement and enhance the understanding provided by formal hypothesis test procedures. Two multivariate graphical methods already are widely used in water-quality studies – Stiff and Piper diagrams. These and other graphical methods are outlined in the following sections. For more detailed discussions on multivariate graphical methods, see Chambers et al. (1983), or the textbook by Everitt (1978). 2.4.1 Profile Plots Profile plots are a class of graphical methods which assign each variable to a separate and parallel axis. One observation is represented by a series of points, one per axis, which are connected by a straight line forming the profile. Each axis is scaled independently, based on the range of values in the entire data set. Comparisons between observations are made by comparing profiles. As an example, assume that sediment loads are to be regionalized. That is, mean annual loads are to be predicted at ungaged sites based on basin characteristics (physical and climatic conditions) at those sites. Of interest may be the interrelationships between sites based on their basin characteristics, as well as which characteristics are associated with high or low annual values. Profile plots such as the one of site basin characteristics in figure 2.30 would effectively illustrate those relationships. Figure 2.30: Profile plot of selected basin characteristics, Cow Creek near Lyons, Kansas (data from Jordan (1979)) 2.4.1.1 Stiff diagrams Stiff diagrams (Hem (1985)) are the most familiar application of profile plots in water resources. In a Stiff diagram, the milliequivalents of major water-quality constituents are plotted for a single sample, with the cation profile plotted to the left of the center line, and anion profile to the right (figure 2.31). Comparisons between several samples based on multiple water-quality constituents is then easily done by comparing shapes of the Stiff diagrams. Figure 2.32 shows one such comparison for 14 groundwater samples from the Fox Hills Sandstone in Wyoming (Henderson (1985)). Figure 2.31: Stiff diagram for a groundwater sample from the Columbia River Basalt aquifer, Oregon (data from Miller and Gonthier (1984)) Figure 2.32: Stiff diagrams to display areal differences in water quality (from Henderson (1985)) 2.4.2 Star Plots A second method of displaying multiple axes is to have them radiate from a central point, rather than aligned parallel as in a profile plot. Again, one observation would be represented by a point on each axis, and these points are connected by line segments. The resulting figures resemble a star pattern, and are often called star plots. Angles between rays of the star are \\(360^{\\circ}/k\\), where \\(k\\) is the number of axes to be plotted. To provide the greatest visual discrimination between observations, rays measuring related characteristics should be grouped together. Unusual observations will stand out as a star looking quite different than the other data, perhaps having an unusually long or short ray. In figure 2.33, the basalt water-quality data graphed using a Stiff diagram in figure 2.31 is displayed as a star plot. Note that the cations are grouped together on the top half of the star, with anions along the bottom. Figure 2.33: Star diagram of the basalt water-quality data 2.4.2.1 Kite diagrams A simplified 4-axis star diagram, the “kite diagram”, has been used for displaying water-quality compositions, especially to portray compositions of samples located on a map (Colby et al. (1956)). Cations are plotted on the two vertical axes, and anions on the two horizontal axes. The primary advantage of this plot is its simplicity. Its major disadvantage is also its simplicity, in that the use of only four axes may hide important characteristics of the data. One might need to know whether calcium or magnesium were present in large amounts, for example, but that could not be determined from the kite diagram. There is no reason why a larger number of axes could not be employed to give more detail, making the plot a true star diagram. Compare for example the basalt data plotted as a star diagram in figure 2.33 and as a kite diagram in figure 2.34. Figure 2.34: Kite diagram of the basalt water-quality data One innovative use of the kite diagram was made by Davis and Rogers (1984). They plotted the quartiles of all observations taken from each of several formations, and at different depth ranges, in order to compare water quality between formations and depths (figure 2.35). The kite plots in this case are somewhat like multivariate boxplots. There is no reason why the other multivariate plots described here could not also present percentile values for a group of observations rather than descriptions of individual values, and be used to compare among groups of data. Figure 2.35: Kite diagram of quartiles of composition from an alluvial formation in Montana (from Davis and Rogers (1984)) 2.4.3 Trilinear Diagrams Trilinear diagrams have been used within the geosciences since the early 1900’s. When three variables for a single observation sum to 100 percent, they can be represented as one point on a triangular (trilinear) diagram. Figure 2.36 is one example – three major cation axes upon which is plotted the cation composition for the basalt data of figure 2.31. Each of the three cation values, in milliequivalents, is divided by the sum of the three values, to produce a new scale in percent of total cations: \\[\\begin{equation} c_{i} = m_{i} / \\left( m_{1} + m_{2} + m_{3} \\right) \\\\ \\text{where the $c_{i}$ is in percent of total cations, and $m_{i}$ are the milliequivalents of cation i.} \\tag{2.4} \\end{equation}\\] For the basalt data, \\(Ca = 0.80\\;meq\\), \\(Mg = 0.26\\;meq\\), and \\(Na + K = 0.89\\;meq\\). Thus \\(\\% Ca = 41\\), \\(\\% Mg = 13\\), and \\(\\%[Na + K] = 46\\). As points on these axes sum to 100 percent, only two of the variables are independent. By knowing two values \\(c_{1}\\) and \\(c_{2}\\), the third is also known: \\(c_{3} = \\left( 100 − c_{1} − c_{2} \\right)\\). Figure 2.36: Trilinear diagram for the basalt cation composition (units are percent milliequivalents). 2.4.3.1 Piper diagrams Piper (1944) applied these trilinear diagrams to both cation and anion compositions of water qualtiy data. He also combined both trilinear diagrams into a single summary diagram with the shape of a diamond (figure 2.37). This diamond has four sides, two for cations and two for anions. However, it also has only two independent axes, one for a cation (say \\(Ca + Mg\\)), and one for an anion (say \\(Cl + SO_{4}\\)). If the (\\(Ca + Mg\\)) percentage is known, so is the (\\(Na + K\\)) percentage, as one is 100% minus the other, and similarly for the anions. The collection of these three diagrams in the format shown in figure 2.37 is called a Piper diagram. Piper diagrams have the advantage over Stiff and star diagrams that each observation is shown as only one point. Therefore, similarities and differences in composition between numerous observations is more easily seen with Piper diagrams. Stiff and star diagrams have two advantages over Piper diagrams: they may be separated in space and placed on a map or other graph, and more than four independent attributes (two cation and two anion) can be displayed at one time. Thus the choice of which to use will depend on the purpose to which they are put. Envelopes have been traditionally drawn by eye around a collection of points on a Piper diagram to describe waters of “similar” composition. Trends (along a flow path, for example) have traditionally been indicated by using different symbols on the diagram for different data groups, such as for upgradient and downgradient observations, and drawing an arrow from one group to the other. Recently, both of these practices have been quantified into significance tests for differences and trends associated with Piper diagrams (Helsel (1992)). Objective methods for drawing envelopes (a smoothed curve) and trend lines on a Piper diagram were also developed. The envelope drawn on figure 2.37 is one example. Smoothing procedures are discussed in more detail in Chapter 10. Figure 2.37: Piper diagram of groundwaters from the Columbia River Basalt aquifer in Oregon (data from Miller and Gonthier (1984)) 2.4.4 Plots of Principal Components One method for viewing observations on multiple axes is to reduce the number of axes to two, and then plot the data as a scatterplot. An important dimension reduction technique is principal components analysis, or PCA (Johnson and Wichern (1982)). Principal components are linear combinations of the p original variables which form a new set of variables or axes. These new axes are uncorrelated with one another, and have the property that the first principal component is the axis that explains more of the variance of the data than any other axis. The second principal component explains more of the remaining variance than any other axis which is uncorrelated with (orthogonal to) the first. The resulting p axes are thus new “variables”, the first few of which often explain the major patterns of the data in multivariate space. The remaining principal components may be treated as residuals, measuring the “lack of fit” of observations along the first few axes. Each observation can be located on the new set of principal component (pc) axes. For example, suppose principal components were computed for four original variables, the cations Ca, Mg, Na and K. The new axes would be linear combinations of these variables, such as: \\[\\begin{equation} \\begin{aligned} &amp; pc1 = 0.75 Ca + 0.80 Mg + 0.10 Na + 0.06 K &amp;&amp; \\text{a &quot;calcareous&quot; axis?}\\\\ &amp; pc2 = 0.17 Ca + 0.06 Mg + 0.60 Na + 0.80 K &amp;&amp; \\text{a &quot;Na + K&quot; axis?}\\\\ &amp; pc3 = 0.40 Ca - 0.25 Mg - 0.10 Na + 0.10 K &amp;&amp; \\text{a &quot;Ca vs. Mg&quot; axis?}\\\\ &amp; pc4 = 0.05 Ca - 0.10 Mg + 0.10 Na + 0.20 K &amp;&amp; \\text{residual noise} \\end{aligned} \\end{equation}\\] An observation which had milliequivalents of \\(Ca = 1.6\\), \\(Mg = 1.0\\), \\(Na = 1.3\\) and \\(K = 0.1\\) would have a value on pc1 equal to (\\(0.6 \\bullet 1.6 + 0.8 \\bullet 1.0 + 0.1 \\bullet 1.3 + 0.06 \\bullet 0.1) = 1.9\\), and similarly for the other new “variables”. At this point no reduction in dimensions has taken place, as each observation still has values along the \\(p = 4\\) new pc axes, as they did for the 4 original axes. Now, however, plots can be made of the locations of observations oriented along the new principal components axes. Most notably, a scatterplot for the first two components (pc1 vs. pc2) will show how the observations group together along the new axes which now contain the most important information about the variation in the data. Thus groupings in multivariate space have been simplified into groupings along the two most important axes, allowing those groupings to be seen by the data analyst. Waters with generally different chemical compositions should plot at different locations on the pc scatterplot. Data known to come from two different groups may be compared using boxplots, probability plots, or Q-Q plots, but now using the first several pc axes as the measurement “variables”. Additionally, plots can be made of the last few pc axes, to check for outliers. These outliers in multivariate space will now be visible by using the “lack of fit” principal components to focus attention at the appropriate viewing angle. Outliers having unusually large or small values on these plots should be checked for measurement errors, unusual circumstances, and the other investigations outliers warrant. Examples of the use of plots of components include Xhoffer et al. (1991), Meglen and Sistko (1985), and Lins (1985). 2.4.5 Other Multivariate Plots 2.4.5.1 3-Dimensional rotation If three variables are all that are under consideration, several microcomputer packages now will plot data in pseudo-3 dimensions, and allow observations to be rotated in space along all three axes. In this way the inter-relationships between the three variables can be visually observed, data visually clustered into groups of similar observations, and outliers discerned. In figure 2.38 two of the many possible orientations for viewing a data set were output from MacSpin (Donoho et al. (1985)), a program for the Apple Macintosh. The data are water qualtity variables measured at low flow in basins with and without coal mining and reclamation (Helsel (1983)). Figure 2.38: Two 3-dimensional plots of a water-quality data set Note the u-shaped pattern in the data seen in the right-hand plot. There is some suggestion of two separate groups of data, the causes of which can be checked by the analyst. This pattern is not evident in the left-hand orientation. By rotating data around their three axes, patterns may be seen which would not be evident without a 3-dimensional perspective, and greater insight into the data is obtained. 2.4.5.2 Scatterplot matrix Another method for inspecting data measured by p variables is to produce a scatterplot for each of the \\(p \\bullet (p − 1) / 2\\) possible pairs of variables. These are then printed all on one screen or page. Obviously, little detail can be discerned on any single plot within the matrix, but variables which are related can be grouped, linear versus nonlinear relationships discerned, etc. Chambers et al. (1983) describe the production and utility of scatterplot matrices in detail. Figure 2.39 is a scatterplot matrix for 5 water-quality variables at low-flow from the coal mining data of Helsel (1983). On the lowest row are histograms for each individual variable. Note the right skewness for all variables except pH. All rows above the last contain scatterplots between each pair of varables. For example, the single plot in the first row is the scatterplot of conductance (cond) versus pH. Note the two separate subgroups of data, representing low and high pH waters. Evident from other plots are the linear association between conductance and sulfate (SO4), the presence of high total iron concentrations (TFe) for waters of low alkalinity (ALK) and pH, and high TFe for waters of high sulfate and conductance. Figure 2.39: Scatterplot matrix showing the relationshps between 5 water-quality variables 2.4.5.3 Methods to avoid Two commonly-used methods should usually be avoided, as they provide little ability to compare differences between groups of data. These are stacked bar charts and pie charts. Both allow only coarse discrimination to be made between segments of the plot. Figure 2.40, for example, is a stacked bar chart of the basalt water-quality data previously shown as a Stiff (figure 2.31) and star (figure 2.33) plot. Note that only large differences between categories within a bar are capable of being discerned. For example, it is much easier to see that chloride (Cl) is larger than sulfate (SO4) on the Stiff diagram than on the stacked bar chart. In addition, stacked bar charts provide much less visual distinction when comparing differences among many sites, as in figure 2.32. Stiff or star diagrams allow differences to be seen as differences in shape, while stacked bar charts require judgements of length without a common datum, a very difficult type of judgement. Multiple pie charts require similarly imprecise and difficult judgements. Further information on these and other types of presentation graphics is given in the last chapter. Figure 2.40: Stacked bar chart of the basalt data Exercises 2.1 Annual peak discharges for the Saddle River in New Jersey are given in Appendix C1. For the peaks occuring from 1968-1989, draw a histogram a boxplot a quantile plot (using \\((i − 0.4)/(n + 0.2)\\)) What transformation, if any, would make these data more symmetric? library(data.table) library(ggplot2) source(&quot;./data/themes.R&quot;) source(&quot;./data/palettes.R&quot;) x &lt;- fread(&#39;data/appendix_c1.csv&#39;) x &lt;- x[Year &gt;= 1968 &amp; Year &lt;= 1989] setorder(x, Flow_cfs) x[, idx := as.numeric(row.names(.SD))][, p_i := (idx - 0.4)/(.N + 0.2)] graph_a &lt;- ggplot(x, aes(x = Flow_cfs)) + geom_histogram(binwidth = 400, color = &quot;white&quot;, fill = palettes_bright$colset_cheer_brights[1]) + theme_generic + labs(x = &quot;Annual Streamflow in cfs&quot;, y = &quot;NUMBER OF OCCURRENCES&quot;) + scale_x_continuous(breaks = seq(1600, 4400, 400)) graph_b &lt;- ggplot(x, aes(y = Flow_cfs)) + geom_boxplot(alpha = 0.4) + labs(x = NULL, y = &quot;Annual Streamflow in cfs&quot;) + scale_x_continuous(limits = c(-2, 2)) + scale_y_continuous(breaks = seq(2000, 4000, 1000)) + scale_fill_manual(values = palettes_bright$colset_cheer_brights) + scale_color_manual(values = palettes_bright$colset_cheer_brights) + theme_generic + theme(legend.position = &quot;none&quot;, panel.grid = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank()) graph_c &lt;- ggplot(x, aes(x = Flow_cfs, y = p_i)) + geom_point() + geom_line() + scale_color_manual(values = palettes_bright$colset_cheer_brights) + theme_generic + labs(x = &quot;Annual Streamflow, in cfs&quot;, y = &quot;CUMULATIVE FREQUENCY&quot;) + scale_x_continuous(breaks = seq(1000, 5000, 1000), limits = c(1000, 5000)) + scale_y_continuous(breaks = seq(0, 1, 0.1)) graph_a graph_b graph_c Either a cube root or logarithmic transform would increase symmetry. 2.2 Arsenic concentrations (in ppb) were reported for ground waters of southeastern New Hampshire (Boudette et al. (1985)). For these data, compute a boxplot a probability plot Based on the probability plot, describe the shape of the data distribution. What transformation, if any, would make these data more symmetric? 1.3 1.5 1.8 2.6 2.8 3.5 4.0 4.8 8 9.5 12 14 19 23 41 80 100 110 120 190 240 250 300 340 580 library(data.table) library(ggplot2) source(&quot;./data/themes.R&quot;) source(&quot;./data/palettes.R&quot;) x &lt;- data.table(c(1.3, 1.5, 1.8, 2.6, 2.8, 3.5, 4.0, 4.8, 8, 9.5, 12, 14, 19, 23, 41, 80, 100, 110, 120, 190, 240, 250, 300, 340, 580)) setnames(x, &quot;Concentration&quot;) setorder(x, Concentration) x[, idx := as.numeric(row.names(.SD))][, p_i := (idx - 0.4)/(.N + 0.2)][, zp:= qnorm(p_i, 0, 1)] graph_a &lt;- ggplot(x, aes(y = Concentration)) + geom_boxplot(alpha = 0.4) + labs(x = NULL, y = &quot;Arsenic Concentration in ppb&quot;) + scale_x_continuous(limits = c(-2, 2)) + scale_y_continuous(breaks = seq(0, 600, 200)) + scale_fill_manual(values = palettes_bright$colset_cheer_brights) + scale_color_manual(values = palettes_bright$colset_cheer_brights) + theme_generic + theme(legend.position = &quot;none&quot;, panel.grid = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank()) graph_b &lt;- ggplot(x, aes(y = Concentration, x = zp)) + geom_point() + geom_line() + scale_color_manual(values = palettes_bright$colset_cheer_brights) + theme_generic + labs(y = &quot;Arsenic Concentration in ppb&quot;, x = &quot;NORMAL QUANTILES&quot;) + scale_x_continuous(breaks = seq(-1.5, 1.5, 1.5)) + scale_y_continuous(breaks = seq(0, 600, 300)) graph_a graph_b The data are strongly right-skewed. A log transformation makes these data more symmetric. 2.3 Feth et al. (1964) measured chemical compositions of waters in springs draining differing rock types. Compare chloride concentrations from two of these rock types using a Q-Q plot. Also plot two other types of graphs. Describe the similarities and differences in chloride. What characteristics are evident in each graph? Chloride concentration, in mg/L 6.0 0.5 0.4 0.7 0.8 6.0 Granodiorite 5.0 0.6 1.2 0.3 0.2 0.5 0.5 10 0.2 0.2 1.7 3.0 1.0 0.2 1.2 1.0 0.3 0.1 Qtz Monzonite 0.1 0.4 3.2 0.3 0.4 1.8 0.9 0.1 0.2 0.3 0.5 library(data.table) library(ggplot2) library(gridExtra) source(&quot;./data/themes.R&quot;) source(&quot;./data/palettes.R&quot;) x &lt;- c(1.0, 0.2, 1.2, 1.0, 0.3, 0.1, 0.1, 0.4, 3.2, 0.3, 0.4, 1.8, 0.9, 0.1, 0.2, 0.3, 0.5) x &lt;- sort(x) y &lt;- c(6.0, 0.5, 0.4, 0.7, 0.8, 6.0, 5.0, 0.6, 1.2, 0.3, 0.2, 0.5, 0.5, 10, 0.2, 0.2, 1.7, 3.0) y &lt;- sort(y) y_interp &lt;- x for (i in 1:length(x)){ j &lt;- ((length(y) * (i - 0.4)) / (length(x) + 0.2)) + 0.4 j_prime &lt;- as.numeric(as.integer(j)) y_interp[i] &lt;- y[j_prime] + ((j - j_prime) * (y[j_prime + 1] - y[j_prime])) } chloride_concentration &lt;- data.table(x,y_interp) setnames(chloride_concentration, c(&quot;Qtz_Monzonite&quot;, &quot;Granodiorite&quot;)) graph_a &lt;- ggplot(chloride_concentration, aes(x = Qtz_Monzonite, y = Granodiorite)) + geom_point() + geom_abline(intercept = 0, slope = 1) + annotate(&quot;text&quot;, x = 0.5, y = 9, label = &quot;• DATA QUANTILES \\n X = Y&quot;) + scale_color_manual(values = palettes_bright$colset_cheer_brights) + theme_generic + labs(y = &quot;GRANODIORITE&quot;, x = &quot;QTZ MONZONITE&quot;, fill = &quot;Dose (mg)&quot;) + scale_x_continuous(breaks = seq(0, 3.5, 0.5), limits = c(0, 3.5)) + scale_y_continuous(breaks = seq(0, 10, 2), limits = c(0, 10)) graph_b &lt;- ggplot(chloride_concentration) + geom_boxplot(data = as.data.table(y), aes(x = -1, y = y), alpha = 0.4) + geom_boxplot(aes(x = 1, y = Qtz_Monzonite), alpha = 0.4) + labs(x = NULL, y = &quot;Chloride Concentration&quot;) + scale_x_continuous(limits = c(-2, 2), breaks = c(-1, 1), labels = c(&quot;GRANODIORITE&quot;, &quot;QTZ MONZONITE&quot;)) + scale_y_continuous(breaks = seq(0, 10, 2.5), limits = c(0,10)) + scale_fill_manual(values = palettes_bright$colset_cheer_brights) + scale_color_manual(values = palettes_bright$colset_cheer_brights) + theme_generic + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) graph_c &lt;- ggplot(as.data.table(y), aes(x = y)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, fill = palettes_bright$colset_cheer_brights[1]) + theme_generic + labs(x = &quot;Granodiorite Chloride Concentration&quot;, y = &quot;NUMBER OF OCCURRENCES&quot;) + scale_y_continuous(breaks = seq(0, 10, 2), limits = c(0,11)) + scale_x_continuous(breaks = seq(0, 10, 1), limits = c(-1,11)) graph_d &lt;- ggplot(chloride_concentration, aes(x = Qtz_Monzonite)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, fill = palettes_bright$colset_cheer_brights[2]) + theme_generic + labs(x = &quot;Qtz Monzonite Chloride Concentration&quot;, y = NULL) + scale_y_continuous(breaks = seq(0, 10, 2), limits = c(0,11)) + scale_x_continuous(breaks = seq(0, 10, 1), limits = c(-1,11)) graph_a graph_b grid.arrange(graph_c, graph_d, nrow = 1) The granodiorite shows higher chloride concentrations than does the quartz monzonite. This can be seen with any of the three graphs, but most easily with the boxplot. From the Q-Q plot, the relationship does not look linear. References "],
["ch3.html", "Chapter 3 Describing Uncertainty 3.1 Definition of Interval Estimates 3.2 Interpretation of Interval Estimates 3.3 Confidence Intervals for the Median 3.4 Confidence Intervals For The Mean 3.5 Nonparametric Prediction Intervals 3.6 Parametric Prediction Intervals 3.7 Confidence Intervals For Percentiles (Tolerance Intervals) 3.8 Other Uses For Confidence Intervals Exercises", " Chapter 3 Describing Uncertainty The mean nitrate concentration in a shallow aquifer under agricultural land was calculated as \\(5.1 \\; mg/L\\). How reliable is this estimate? Is \\(5.1 \\; mg/L\\) in violation of a health advisory limit of \\(5 \\; mg/L\\)? Should it be treated differently than another aquifer having a mean concentration of \\(4.8 \\; mg/L\\)? Thirty wells over a 5-county area were found to have a mean specific capacity of 1 gallon per minute per foot, and a standard deviation of 7 gallons per minute per foot. A new well was drilled and developed with an acid treatment. The well produced a specific capacity of 15 gallons per minute per foot. To determine whether this increase might be due to the acid treatment, how likely is a specific capacity of 15 to result from the regional distribution of the other 30 wells? An estimate of the 100-year flood, the 99th percentile of annual flood peaks, was determined to be 10,000 cubic feet per second (cfs). Assuming that the choice of a particular distribution to model these floods (Log Pearson Type III) is correct, what is the reliability of this estimate? In chapter 1 several summary statistics were presented which described key attributes of a data set. They were sample estimates (such as \\(\\overline{x}\\) and \\(s^{2}\\)) of true and unknown population parameters (such as µ and σ2). In this chapter, descriptions of the uncertainty or reliability of sample estimates is presented. As an alternative to reporting a single estimate, the utility of reporting a range of values called an “interval estimate” is demonstrated. Both parametric and nonparametric interval estimates are presented. These intervals can also be used to test whether the population parameter is significantly different from some pre-specified value. 3.1 Definition of Interval Estimates The sample median and sample mean estimate the corresponding center points of a population. Such estimates are called point estimates. By themselves, point estimates do not portray the reliability, or lack of reliability (variability), of these estimates. For example, suppose that two data sets X and Y exist, both with a sample mean of 5 and containing the same number of data. The Y data all cluster tightly around 5, while the X data are much more variable. The point estimate of 5 for X is much less reliable than that for Y because of the greater variability in the X data. In other words, more caution is needed when stating that 5 estimates the true population mean of X than when stating this for Y. Reporting only the sample (point) estimate of 5 fails to give any hint of this difference. As an alternative to point estimates, interval estimates are intervals which have a stated probability of containing the true population value. The intervals are wider for data sets having greater variability. Thus in the above example an interval between 4.7 and 5.3 may have a 95% probability of containing the (unknown) true population mean of Y. It would take a much wider interval, say between 2.0 and 8.0, to have the same probability of containing the true mean of X. The difference in the reliability of the two estimates is therefore clearly stated using interval estimates. Interval estimates can provide two pieces of information which point estimates cannot: A statement of the probability or likelihood that the interval contains the true population value (its reliability). A statement of the likelihood that a single data point with specified magnitude comes from the population under study. Interval estimates for the first purpose are called confidence intervals; intervals for the second purpose are called prediction intervals. Though related, the two types of interval estimates are not identical, and cannot be interchanged. In sections 3.3 and 3.4, confidence intervals will be developed for both the median and mean. Prediction intervals, both parametric and nonparametric, will be used in sections 3.5 and 3.6 to judge whether one new observation is consistent with existing data. Intervals for percentiles other than the median will be discussed in section 3.7. 3.2 Interpretation of Interval Estimates Suppose that the true population mean µ of concentration in an aquifer was 10. Also suppose that the true population variance σ2 equals 1. As these values in practice are never known, samples are taken to estimate them by the sample mean and sample variance s2. Sufficient funding is available to take 12 water samples (roughly one per month) during a year, and the days on which sampling occurs are randomly chosen. From these 12 samples and s2 are computed. Although in reality only one set of 12 samples would be taken each year, using a computer 12 days can be selected multiple times to illustrate the concept of an interval estimate. For each of 10 independent sets of 12 samples, a confidence interval on the mean is computed using equations given later in section 3.4.1. The results are shown in table 3.1 and figure 3.1. Table 3.1: Ten 90% confidence intervals around a true mean of 10. Data follow a normal distribution. The interval with the asterisk does not inclue the true value. N Mean St. Dev. 90% Confidence Interval 1 12 10.06 1.11 (9.49 to 10.64) 2 12 10.60 0.81 *(10.18 to 11.02) 3 12 9.95 1.26 (9.29 to 10.60) 4 12 10.18 1.26 (9.52 to 10.83) 5 12 10.17 1.33 (9.48 to 10.85) 6 12 10.22 1.19 (9.60 to 10.84) 7 12 9.71 1.51 (8.92 to 10.49) 8 12 9.90 1.01 (9.38 to 10.43) 9 12 9.95 0.10 (9.43 to 10.46) 10 12 9.88 1.37 (9.17 to 10.59) Figure 3.1: Ten 90% confidence intervals for normally-distributed data with true mean = 10. These ten intervals are “90% confidence intervals” on the true population mean. That is, the true mean will be contained in these intervals an average of 90 percent of the time. So for the 10 intervals in the table, nine are expected to include the true value while one is not. This is in fact what happened. Of course when a one-time sampling occurs, the computed interval will either include or not include the true, unknown population mean. The probability that the interval does include the true value is called the confidence level of the interval. The probability that this interval will not cover the true value, called the alpha level (\\(\\alpha\\)), is computed as \\[\\begin{equation} \\alpha = 1 - confidence level. \\tag{3.1} \\end{equation}\\] The width of a confidence interval is a function of the shape of the data distribution (its variability and skewness), the sample size, and of the confidence level desired. As the confidence level increases the interval width also increases, because a larger interval is more likely to contain the true value than is a shorter interval. Thus a 95% confidence interval will be wider than a 90% interval for the same data. Symmetric confidence intervals on the mean are commonly computed assuming the data follow a normal distribution (see section 3.4.1). If not, the distribution of the mean itself will be approximately normal as long as sample sizes are large (say 50 observations or greater). Confidence intervals assuming normality will then include the true mean (\\(1 − \\alpha\\))% of the time. In the above example, the data were generated from a normal distribution, so the small sample size of 12 is not a problem. However when data are skewed and sample sizes below 50 or more, symmetric confidence intervals will not contain the mean (\\(1 − \\alpha\\))% of the time. In the example below, symmetric confidence intervals are incorrectly computed for skewed data (figure 3.2). The results (figure 3.3 and table 3.2) show that the confidence intervals miss the true value of 1 more frequently than they should. The greater the skewness, the larger the sample size must be before symmetric confidence intervals can be relied on. As an alternative, asymmetric confidence intervals can be computed for the common situation of skewed data. They are also presented in the following sections. Table 3.2: Ten 90% confidence intervals around a true mean of 1. Data do not follow a normal distribution. Intervals with an asterisk do not inclue the true value. N Mean St. Dev. 90% Confidence Interval 1 12 0.784 0.320 *(0.618 to 0.950) 2 12 0.811 0.299 *(0.656 to 0.966) 3 12 1.178 0.700 (0.815 to 1.541) 4 12 1.030 0.459 (0.792 to 1.267) 5 12 1.079 0.573 (0.782 to 1.376) 6 12 0.833 0.363 (0.644 to 1.021) 7 12 0.789 0.240 *(0.664 to 0.913) 8 12 1.159 0.815 (0.736 to 1.581) 9 12 0.822 0.365 *(0.633 to 0.992) 10 12 0.837 0.478 (0.589 to 1.085) Figure 3.2: Histogram of skewed example data. µ = 1.0 σ = 0.75. Figure 3.3: Ten 90% confidence intervals for skewed data with true mean = 1.0 3.3 Confidence Intervals for the Median A confidence interval for the true population median may be computed either without assuming the data follow any specific distribution (section 3.3.1), or assuming they follow a distribution such as the lognormal (section 3.3.2). 3.3.1 Nonparametric Interval Estimate For The Median A nonparametric interval estimate for the true population median is computed using the binomial distribution. First, the desired significance level \\(\\alpha\\) is stated, the acceptable risk of not including the true median. One-half (\\(\\alpha / 2\\)) of this risk is assigned to each end of the interval (figure 3.4). A table of the binomial distribution provides lower and upper critical values x’ and x at one-half the desired alpha level (\\(\\alpha / 2\\)). These critical values are transformed into the ranks Rl and Ru corresponding to data points Cl and Cu at the ends of the confidence interval. Figure 3.4: Probability of containing the true median P.50 in a 2-sided interval estimate. P.50 will be below the lower interval bound (Cl) α/2% of the time, and above the upper interval bound (Cu) α/2% of the time. For small sample sizes, the binomial table is entered at the \\(p = 0.5\\) (median) column in order to compute a confidence interval on the median. This column is reproduced in Appendix Table B5 – it is identical to the quantiles for the sign test (see chapter 6). A critical value x’ is obtained from Table B5 corresponding to \\(\\alpha / 2\\), or as close to \\(\\alpha / 2\\) as possible. This critical value is then used to compute the ranks Ru and Rl corresponding to the data values at the upper and lower confidence limits for the median. These limits are the Rlth ranked data points going in from each end of the sorted list of n observations. The resulting confidence interval will reflect the shape (skewed or symmetric) of the original data. \\[\\begin{equation} R_{l} = x^{\\prime} \\\\ \\tag{3.2} \\end{equation}\\] \\[\\begin{equation} \\begin{aligned} R_{u} &amp; = n - x^{\\prime} = x &amp;&amp; \\text{for $x^{\\prime}$ and from Appendix Table B5} \\end{aligned} \\tag{3.3} \\end{equation}\\] Nonparametric intervals cannot always exactly produce the desired confidence level when sample sizes are small. This is because they are discrete, jumping from one data value to the next at the ends of the intervals. However, confidence levels close to those desired are available for all but the smallest sample sizes. Example 2 The following 25 arsenic concentrations (in ppb) were reported for ground waters of southeastern New Hampshire (Boudette et al. (1985)). A histogram of the data is shown in figure 3.5. Compute the \\(\\alpha = 0.05\\) interval estimate of the median concentration. 1.3 1.5 1.8 2.6 2.8 3.5 4.0 4.8 8 9.5 12 14 19 23 41 80 100 110 120 190 240 250 300 340 580 Figure 3.5: Histogram of Example 2 arsenic concentrations (in ppb) The sample median concentration \\(\\hat{C}_{0.5} = 19\\), the 13th observation ranked from smallest to largest. To determine a 95% confidence interval for the true median concentration \\(C_{0.5}\\), the tabled critical value with an entry nearest to \\(\\alpha / 2 = 0.025\\) is \\(x^{\\prime} = 7\\) from Table B5. The entry value of 0.022 is quite near 0.025, and is the equivalent to the shaded area at one side of figure 3.4. From equations (3.2) and (3.3) the rank Rl of the observation corresponding to the lower confidence limit is 8, and Ru corresponding to the upper confidence limit is 25 − 7 = 18. For this confidence interval the alpha level \\(\\alpha = 2 \\bullet 0.022 = 0.044\\). This is equivalent to a 1−0.044 or 95.6% confidence limit for \\(C_{0.5}\\), and is the interval between the 8th and 18th ranked observations (the 8th point in from either end), or \\[C_{l} = 4.8 \\leq C_{0.5} \\leq 110 = C_{u} \\; \\; \\text{at} \\; \\alpha = 0.044\\] The asymmetry around \\(\\hat{C}_{0.5} = 19\\) reflects the skewness of the data. An alternative method for computing the same nonparametric interval is used when the sample size \\(n &gt; 20\\). This large-sample approximation utilizes a table of the standard normal distribution available in every basic statistics textbook to approximate the binomial distribution. By using this approximation, only small tables of the binomial distribution up to \\(n = 20\\) need be included in statistics texts. A critical value \\(z_{\\alpha / 2}\\) from the normal table determines the upper and lower ranks of observations corresponding to the ends of the confidence interval. Those ranks are \\[\\begin{equation} R_{l} = \\frac{n - z_{\\alpha / 2} \\sqrt{n}}{2} \\\\ \\tag{3.4} \\end{equation}\\] \\[\\begin{equation} R_{u} = \\frac{n + z_{\\alpha / 2} \\sqrt{n}}{2} + 1 \\tag{3.5} \\end{equation}\\] The computed ranks Ru and Rl are rounded to the nearest integer when necessary. Example 2,cont. For the \\(n = 25\\) arsenic concentrations, an approximate 95 percent confidence interval on the true median \\(C_{0.5}\\) is computed using \\(z_{\\alpha / 2} = 1.96\\) so that \\[\\begin{equation} \\begin{aligned} R_{l} &amp;= \\frac{25 - 1.96 \\bullet \\sqrt{25}}{2} &amp;&amp;= 7.6\\\\ R_{u} &amp;= \\frac{25 + 1.96 \\bullet \\sqrt{25}}{2} + 1 &amp;&amp;= 18.4 \\end{aligned} \\end{equation}\\] the “7.6th ranked observation” in from either end. Rounding to the nearest integer, the 8th and 18th ranked observations are used as the ends of the \\(\\alpha = 0.05\\) confidence limit on \\(C_{0.5}\\), agreeing with the exact 95.6% confidence limit computed previously. 3.3.2 Parametric Interval Estimate For The Median As mentioned in chapter 1, the geometric mean of x (\\(GM_{x}\\)) is an estimate of the median in original (x) units when the data logarithms \\(y = \\ln{(x)}\\) are symmetric. The mean of \\(y\\) and confidence interval on the mean of y become the geometric mean with its (asymmetric) confidence interval after being retransformed back to original units by exponentiation (equations (3.6) and (3.7)). These are parametric alternatives to the point and interval estimates of section 3.3.1. Here it is assumed that the data are distributed as a lognormal distribution. The geometric mean and interval would be more efficient (shorter interval) measures of the median and its confidence interval when the data are truly lognormal. The sample median and its interval are more appropriate and more efficient if the logarithms of data still exhibit skewness and/or outliers. \\[\\begin{equation} \\begin{aligned} &amp; GM_{x} = \\exp{\\left( \\overline{y} \\right)} &amp;&amp; \\text{where $y = \\ln{(x)}$ and $\\overline{y} =$ sample mean of $y$.} \\end{aligned} \\tag{3.6} \\end{equation}\\] \\[\\begin{equation} \\exp{\\left( \\overline{y} - t_{(\\alpha / 2, n - 1)} \\sqrt{s^{2}_{y} / n} \\right)} \\leq GM_{x} \\leq \\exp{\\left( \\overline{y} + t_{(\\alpha / 2, n - 1)} \\sqrt{s^{2}_{y} / n} \\right)} \\\\ \\text{where $s^{2}_{y} =$ sample variance of $y$ in natural log units.} \\tag{3.7} \\end{equation}\\] Example 2,cont. Natural logs of the arsenic data are as follows: 0.262 0.405 0.588 0.956 1.030 1.253 1.387 1.569 2.079 2.251 2.485 2.639 2.944 3.135 3.714 4.382 4.605 4.700 4.787 5.247 5.481 5.521 5.704 5.829 6.363 The mean of the logs \\(= 3.17\\), with standard deviation of 1.96. From figure 3.6 the logs of the data appear more symmetric than do the original units of concentration shown previously in figure 3.5. Figure 3.6: Histogram of natural logs of the arsenic concentrations of Example 2 From equations (3.6) and (3.7), the geometric mean and its 95% confidence interval are: \\[GM_{C} = \\exp{(3.17)} = 23.8\\] \\[\\begin{equation} \\begin{aligned} \\exp{\\left( 3.17 - 2.064 \\bullet \\sqrt{1.96^{2} / 25} \\right)} \\leq &amp;GM_{C} \\leq \\exp{\\left( 3.17 + 2.064 \\bullet \\sqrt{1.96^{2} / 25} \\right)} \\\\ \\exp{(2.36)} \\leq &amp;GM_{C} \\leq \\exp{(3.98)} \\\\ 10.6 \\leq &amp;GM_{C} \\leq 53.5 \\end{aligned} \\end{equation}\\] The scientist must decide whether it is appropriate to assume a lognormal distribution. If not, the nonparametric interval of section 3.3.1 would be preferred. 3.4 Confidence Intervals For The Mean Interval estimates may also be computed for the true population mean \\(\\mu\\). These are appropriate if the center of mass of the data is the statistic of interest (see Chapter 1). Intervals symmetric around the sample mean \\(\\overline{X}\\) are computed most often. For large sample sizes a symmetric interval adequately describes the variation of the mean, regardless of the shape of the data distribution. This is because the distribution of the sample mean will be closely approximated by a normal distribution as sample sizes get larger, even though the data may not be normally distributed1. For smaller sample sizes, however, the mean will not be normally distributed unless the data themselves are normally distributed. As data increase in skewness, more data are required before the distribution of the mean can be adequately approximated by a normal distribution. For highly skewed distributions or data containing outliers, it may take more than 100 observations before the mean will be sufficiently unaffected by the largest values to assume that its distribution will be symmetric. 3.4.1 Symmetric Confidence Interval For The Mean Symmetric confidence intervals for the mean are computed using a table of the student’s t distribution available in statistics textbooks and software. This table is entered to find critical values for t at one-half the desired alpha level. The width of the confidence interval is a function of these critical values, the standard deviation of the data, and the sample size. When data are skewed or contain outliers, the assumptions behind the t-interval do not hold. The resulting symmetric interval will be so wide that most observations will be included in it. It may also extend below zero on the lower end. Negative endpoints of a confidence interval for data which cannot be negative are clear signals that the assumption of a symmetric confidence interval is not warranted. For such data, assuming a lognormal distribution as described in section 3.4.2 would be more appropriate. The student’s t statistic \\(t_{(\\alpha / 2, n − 1)}\\) is used to compute the following symmetric confidence interval: \\[\\begin{equation} \\overline{x} - t_{(\\alpha / 2, n - 1)} \\bullet \\sqrt{s^{2} / n} \\leq \\mu \\leq \\overline{x} + t_{(\\alpha / 2, n - 1)} \\bullet \\sqrt{s^{2} / n} \\tag{3.8} \\end{equation}\\] Example 2,cont. The sample mean arsenic concentration \\(\\overline{C} = 98.4\\). This is the point estimate for the true unknown population mean \\(\\mu\\). An \\(\\alpha = 0.05\\) confidence interval on \\(\\mu\\) is \\[\\begin{equation} \\begin{aligned} 98.4 - t_{(0.025, 24)} \\bullet \\sqrt{144.7^{2} / 25} \\leq &amp;\\mu \\leq 98.4 + t_{(0.025, 24)} \\bullet \\sqrt{144.7^{2} / 25} \\\\ 98.4 - 2.064 \\bullet 28.9 \\leq &amp;\\mu \\leq 98.4 + 2.064 \\bullet 28.9 \\\\ 38.7 \\leq &amp;\\mu \\leq 158.1 \\end{aligned} \\end{equation}\\] Thus there is a 95% probability that \\(\\mu\\) is contained in the interval between 38.7 and 158.1 ppb, assuming that a symmetric confidence interval is appropriate. Note that this confidence interval is, like \\(\\overline{C}\\), sensitive to the highest data values. If the largest value of 580 were changed to 380, the median and its confidence interval would be unaffected. \\(\\overline{C}\\) would change to 90.4, with a 95% interval estimate for \\(\\mu\\) from 40.7 to 140.1. 3.4.2 Asymmetric Confidence Interval For The Mean (For Skewed Data) Means and confidence intervals may also be computed by assuming that the logarithms \\(y = \\ln{(x)}\\) of the data are symmetric. If the data appear more like a lognormal than a normal distribution, this assumption will give a more reliable (lower variance) estimate of the mean than will computation of the usual sample mean without log transformation. To estimate the population mean \\(\\mu_{x}\\) in original units, assume the data are lognormal. One-half the variance of the logarithms is added to \\(\\overline{y}\\) (the mean of the logs) prior to exponentiation (Aitchison and Brown (1981)). As the sample variance \\(s^{2}_{y}\\) is only an estimate of the true variance of the logarithms, the sample estimate of the mean is biased (Bradu and Mundlak (1970)). However, for small \\(s^{2}_{y}\\) and large sample sizes the bias is negligible. See Chapter 9 for more information on the bias of this estimator. \\[\\begin{equation} \\begin{aligned} &amp; \\hat{\\mu}_{x} = \\exp{\\left( \\overline{y} + 0.5 \\bullet s^{2}_{y} \\right)} &amp;&amp;\\text{where $y = \\ln{(x)}$,} \\\\ \\end{aligned} \\tag{3.9} \\end{equation}\\] \\[\\overline{y} = \\text{sample mean and $s^{2}_{y} =$ sample variance of $y$ in natural log units.}\\] The confidence interval around \\(\\hat{\\mu}_{x}\\) is not the interval estimate computed for the geometric mean in equation (3.7). It cannot be computed simply by exponentiating the interval around \\(\\overline{y}\\). An exact confidence interval in original units for the mean of lognormal data can be computed, though the equation is beyond the scope of this book. See Land (1971) and Land (1972) for details. Example 2,cont. To estimate the mean concentration assuming a lognormal distribution, \\[\\hat{\\mu}_{C} = \\exp{\\left( 3.17 + 0.5 \\bullet 1.96^{2} \\right)} = 162.8 .\\] This estimate does not even fall within the confidence interval computed earlier for the geometric mean (\\(10.6 \\leq GM_{C} \\leq 53.5\\)). Thus here is a case where it is obvious that the CI on the geometric mean is not an interval estimate of the mean. It is an interval estimate of the median, assuming the data follow a lognormal distribution. 3.5 Nonparametric Prediction Intervals The question is often asked whether one new observation is likely to have come from the same distribution as previously-collected data, or alternatively from a different distribution. This can be evaluated by determining whether the new observation is outside the prediction interval computed from existing data. Prediction intervals contain \\(100 \\bullet (1 − \\alpha)\\) percent of the data distribution, while \\(100 \\bullet \\alpha\\) percent are outside of the interval. If a new observation comes from the same distribution as previously-measured data, there is a \\(100 \\bullet \\alpha\\) percent chance that it will lie outside of the prediction interval. Therefore being outside the interval does not “prove” the new observation is different, just that it is likely to be so. How likely this is depends on the choice of \\(\\alpha\\) made by the scientist. Prediction intervals are computed for a different purpose than confidence intervals – they deal with individual data values as opposed to a summary statistic such as the mean. A prediction interval is wider than the corresponding confidence interval, because an individual observation is more variable than is a summary statistic computed from several observations. Unlike a confidence interval, a prediction interval takes into account the variability of single data points around the median or mean, in addition to the error in estimating the center of the distribution. When the mean \\(\\pm\\) 2 standard deviations are mistakenly used to estimate the width of a prediction interval, new data are asserted as being from a different population more frequently than they should. In this section nonparametric prediction intervals are presented – intervals not requiring the data to follow any particular distributional shape. Prediction intervals can also be developed assuming the data follow a particular distribution, such as the normal. These are discussed in section 3.6. Both two-sided and one-sided prediction intervals are described. It may also be of interest to know whether the median or mean of a new set of data differs from that for an existing group. To test for differences in medians, use the rank-sum test of Chapter 5. To test for differences in means, the two-sample t-test of Chapter 5 should be performed. 3.5.1 Two-Sided Nonparametric Prediction Interval The nonparametric prediction interval of confidence level α is simply the interval between the \\(\\alpha / 2\\) and \\(1 − (\\alpha / 2)\\) percentiles of the distribution (figure 3.7). This interval contains \\(100 \\bullet (1 − \\alpha)\\) percent of the data, while \\(100 \\bullet \\alpha\\) percent lies outside of the interval. Therefore if the new additional data point comes from the same distribution as the previously measured data, there is a \\(100 \\bullet \\alpha\\) percent chance that it will lie outside of the prediction interval and be incorrectly labeled as “changed”. The interval will reflect the shape of the data it is developed from, and no assumptions about the form of that shape need be made. \\[\\begin{equation} PI_{np} = X_{\\alpha / 2 \\bullet (n + 1)} \\;\\; \\text{to} \\;\\; X_{\\lbrack 1 - (\\alpha / 2) \\rbrack \\bullet (n+1)} \\tag{3.10} \\end{equation}\\] Figure 3.7: Two-sided prediction interval. A new observation will be below Xl α/2% and above \\(X_{u}\\) α/2% of the time, when the data distribution is unchanged. Example 2,cont. Compute a 90% (\\(\\alpha = 0.10\\)) prediction interval for the arsenic data without assuming the data follow any particular distribution. The 5th and 95th percentiles of the arsenic data are the observations with ranks of (\\(0.05 \\bullet 26\\)) and (\\(0.95 \\bullet 26\\)), or 1.3 and 24.7. By linearly interpolating between the 1st and 2nd, and 24th and 25th observations, the \\(\\alpha = 0.10\\) prediction interval is \\[\\begin{equation} \\begin{aligned} X_{1} + 0.3 \\bullet \\left( X_{2} - X_{1} \\right) &amp; \\text{ to } X_{24} + 0.7 \\bullet \\left( X_{25} - X_{24} \\right) \\\\ 1.3 + 0.3 \\bullet 0.2 &amp; \\text{ to } 340 + 0.7 \\bullet 240 \\\\ 1.4 &amp; \\text{ to } 508 \\text{ ppb} \\end{aligned} \\end{equation}\\] A new observation less than 1.4 or greater than 508 can be considered as coming from a different distribution at a 10% risk level (\\(\\alpha = 0.10\\)). 3.5.2 One-Sided Nonparametric Prediction Interval One-sided prediction intervals are appropriate if the interest is in whether a new observation is larger than existing data, or smaller than existing data, but not both. The decision to use a onesided interval must be based entirely on the question of interest. It should not be determined after looking at the data and deciding that the new observation is likely to be only larger, or only smaller, than existing information. One-sided intervals use \\(\\alpha\\) rather than \\(\\alpha / 2\\) as the error risk, placing all the risk on one side of the interval (figure 3.8). \\[\\begin{equation} \\begin{aligned} \\text{one-sided PI np: $\\;\\;\\;\\;\\;$} &amp; \\text{new $x$} &lt; X_{\\alpha \\bullet (n + 1)} \\text{ , or} \\\\ &amp; \\text{new $x$} &lt; X_{\\lbrack 1 - (\\alpha / 2) \\rbrack \\bullet (n+1)} \\\\ &amp; \\text{(but not either, or)} \\end{aligned} \\tag{3.11} \\end{equation}\\] Figure 3.8: Confidence level and alpha level for a 1-sided prediction interval Probability of obtaining a new observation greater than \\(X_{u}\\) when the distribution is unchanged is α. Example 2,cont. An arsenic concentration of 350 ppb is found in a New Hampshire well. Does this indicate a change to larger values as compared to the distribution of concentrations for the example 2 data? Use \\(\\alpha = 0.10\\). As only large concentrations are of interest, the new data point will be considered larger if it exceeds the \\(\\alpha = 0.10\\) one-sided prediction interval, or upper 90th percentile of the existing data. \\(X_{0.90 \\bullet 26} = X_{23.4}\\). By linear interpolation this corresponds to a concentration of \\[X_{23} + 0.4 \\bullet (X_{24} - X_{23}) = 300 + 0.4 \\bullet (40) = 316\\] In other words, a concentration of 316 or greater will occur approximately 10 percent of the time if the distribution of data has not increased. Therefore a concentration of 350 ppb is considered larger than the existing data at an α level of 0.10. 3.6 Parametric Prediction Intervals Parametric prediction intervals are also used to determine whether a new observation is likely to come from a different distribution than previously-collected data. However, an assumption is now made about the shape of that distribution. This assumption provides more information with which to construct the interval, as long as the assumption is valid. If the data do not approximately follow the assumed distribution, the prediction interval may be quite inaccurate. 3.6.1 Symmetric Prediction Interval The most common assumption is that the data follow a normal distribution. Prediction intervals are then constructed to be symmetric around the sample mean, and wider than confidence intervals on the mean. The equation for this interval differs from that for a confidence interval around the mean by adding a term \\(\\sqrt{s^{2}} = s\\), the standard deviation of individual observations around their mean: \\[\\begin{equation} PI = \\overline{x} - t_{(\\alpha / 2, n - 1)} \\bullet \\sqrt{s^{2} + \\left( s^{2} / n \\right)} \\;\\; \\text{to} \\;\\; \\overline{x} + t_{(\\alpha / 2, n - 1)} \\bullet \\sqrt{s^{2} + \\left( s^{2} / n \\right)} \\tag{3.12} \\end{equation}\\] One-sided intervals are computed as before, using \\(\\alpha\\) rather than \\(\\alpha / 2\\) and comparing new data to only one end of the prediction interval. Example 2,cont. Assuming symmetry, is a concentration of 350 ppb different (not just larger) than what would be expected from the previous distribution of arsenic concentrations? Use \\(\\alpha = 0.10\\). The parametric two-sided \\(\\alpha = 0.10\\) prediction interval is \\[\\begin{equation} \\begin{aligned} 98.4 - t_{(0.05, 24)} \\bullet \\sqrt{144.7^{2} + \\left( 144.7^{2} / 25 \\right)} &amp;\\text{$\\;$to$\\;$} 98.4 + t_{(0.05, 24)} \\bullet \\sqrt{144.7^{2} + \\left( 144.7^{2} / 25 \\right)} \\\\ 98.4 - 1.711 \\bullet 147.6 &amp;\\text{$\\;$to$\\;$} 98.4 + 1.711 \\bullet 147.6 \\\\ -154.1 &amp;\\text{$\\;$to$\\;$} 350.9 \\end{aligned} \\end{equation}\\] 350 ppb is at the upper limit of 350.9, so the concentration is not declared different at \\(\\alpha = 0.10\\). The negative concentration reported as the lower prediction bound is a clear indication that the underlying data are not symmetric, as concentrations are non-negative. To avoid an endpoint as unrealistic as this negative concentration, an asymmetric prediction interval should be used instead. 3.6.2 Asymmetric Prediction Intervals Asymmetric intervals can be computed either using the nonparametric intervals of section 3.5, or by assuming symmetry of the logarithms and computing a parametric interval on the logs of the data. Either asymmetric interval is more valid than a symmetric interval when the underlying data are not symmetric, as is the case for the arsenic data of example 2. As stated in Chapter 1, most water resources data and indeed most environmental data show positive skewness. Thus they should be modelled using asymmetric intervals. Symmetric prediction intervals should be used only when the data are known to come from a normal distribution. This is because prediction intervals deal with the behavior of individual observations. Therefore the Central Limit Theorem (see first footnote in this chapter) does not apply. Data must be assumed nonnormal unless shown otherwise. It is difficult to disprove normality using hypothesis tests (Chapter 4) due to the small sample sizes common to environmental data sets. It is also difficult to see non-normality with graphs unless the departures are strong (Chapter 10). It is unfortunate that though most water resources data sets are asymmetric and small, symmetric intervals are commonly used. An asymmetric (but parametric) prediction interval can be computed using logarithms. This interval is parametric because percentiles are computed assuming that the data follow a lognormal distribution. Thus from equation (3.12): \\[\\begin{equation} PI = \\exp{\\left( \\overline{y} - t_{(\\alpha / 2, n - 1)} \\bullet \\sqrt{s^{2}_{y} + \\left( s^{2}_{y} / n \\right)}\\right)} \\text{$\\;$to$\\;$} \\exp{\\left( \\overline{y} + t_{(\\alpha / 2, n - 1)} \\bullet \\sqrt{s^{2}_{y} + \\left( s^{2}_{y} / n \\right)}\\right)} \\\\ \\text{where $y = \\ln{(X)}$, $\\overline{y}$ is the mean and $s^{2}_{y}$ the variance of the logarithms.} \\tag{3.13} \\end{equation}\\] Example 2,cont. An asymmetric prediction interval is computed using the logs of the arsenic data. A 90% prediction interval becomes \\[\\begin{equation} \\begin{aligned} \\ln{(PI)} = 3.17 - t_{(0.05, 24)} \\bullet \\sqrt{1.96^{2} + 1.96^{2} / 25} &amp;\\text{$\\;\\;$to$\\;\\;$} 3.17 + t_{(0.05, 24)} \\bullet \\sqrt{1.96^{2} + 1.96^{2} / 25} \\\\ 3.17 - 1.71 \\bullet 2.11 &amp;\\text{$\\;\\;$to$\\;\\;$} 3.17 + 1.71 \\bullet 2.11 \\\\ 0.44 &amp;\\text{$\\;\\;$to$\\;\\;$} 6.78 \\\\ \\text{which when exponentiated into original units becomes} \\\\ 1.55 &amp;\\text{$\\;\\;$to$\\;\\;$} 880.1 \\end{aligned} \\end{equation}\\] As percentiles can be transformed directly from one measurement scale to another, the prediction interval in log units can be directly exponentiated to give the prediction interval in original units. This parametric prediction interval differs from the one based on sample percentiles in that a lognormal distribution is assumed. The parametric interval would be preferred if the assumption of a lognormal distribution is believed. The sample percentile interval would be preferred when a robust interval is desired, such as when a lognormal model is not believed, or when the scientist does not wish to assume any model for the data distribution. 3.7 Confidence Intervals For Percentiles (Tolerance Intervals) Quantiles or percentiles have had the traditional use in water resources of describing the frequency of flood events. Thus the 100-year flood is the 99th percentile (0.99 quantile) of the distribution of annual flood peaks. It is the magnitude of flood which is expected to be exceeded only once in 100 years. The 20-year flood is of a magnitude which is expected to be exceeded only once in 20 years (5 times in 100 years), or is the 95th percentile of annual peaks. Similarly, the 2-year flood is the median or 50th percentile of annual peaks. Flood percentiles are determined assuming that peak flows follow a specified distribution. The log Pearson Type III is often used in the United States. Historically, European countries have used the Gumbel (extreme value) distribution, though the GEV distribution is now more common (Ponce (1989)). The most commonly-reported statistic for analyses of low flows is also based on percentiles, the “7-day 10-year low flow” or \\(7Q10\\). The \\(7Q10\\) is the 10th percentile of the distribution of annual values of \\(Y\\), where \\(Y\\) is the lowest average of mean daily flows over any consecutive 7-day period for that year. \\(Y\\) values are commonly fit to Log Pearson III or Gumbel distributions in order to compute the percentile. Often a series of duration periods is used to better define flow characteristics, i.e. the \\(30Q10\\), \\(60Q10\\), and others (Ponce (1989)). Recently, percentiles: water quality of water-quality records appear to be becoming more important in a regulatory framework. Crabtree, Cluckie, and Forster (1987) among others have reported an increasing reliance on percentiles for developing and monitoring compliance with water quality standards2. In these scenarios, the median, 95th, or some other percentile should not exceed (or be below) a standard. As of now, no distribution is usually assumed for water-quality concentrations, so that sample percentiles are commonly computed and compared to the standard. In regulatory frameworks, exceedance of a tolerance interval on concentration is sometimes used as evidence of contamination. A tolerance interval is nothing other than a confidence interval on the percentile. The percentile used is the ‘coverage coefficient’ of the tolerance interval. In light of the ever increasing use of percentiles in water resources applications, understanding of their variability is quite important. In section 3.7.1, interval estimates will be computed without assuming a distribution for the data. Estimates of peak flow percentiles computed in this way will therefore differ somewhat in comparison to those computed using a Log Pearson III or Gumbel assumption. Computation of percentile interval estimates when assuming a specific distributional shape is discussed in section 3.7.3. In sections 3.7.2 and 3.7.4, use of interval estimates for testing hypotheses is illustrated. 3.7.1 Nonparametric Confidence Intervals For Percentiles Confidence intervals can be developed for any percentile analogous to those developed in section 3.3 for the median. First the desired confidence level is stated. For small sample sizes a table of the binomial distribution is entered to find upper and lower limits corresponding to critical values at one-half the desired alpha level (\\(\\alpha / 2\\)). These critical values are transformed into the ranks corresponding to data points at the ends of the confidence interval. The binomial table is entered at the column for \\(p\\), the percentile (actually the quantile) for which a confidence interval is desired. So for a confidence interval on the 75th percentile, the \\(p = 0.75\\) column is used. Go down the column until the appropriate sample size \\(n\\) is found. The tabled probability \\(p^{*}\\) should be found which is as close to \\(\\alpha / 2\\) as possible. The lower critical value \\(x_{l}\\) is the integer corresponding to this probability \\(p^{*}\\). A second critical value \\(x_{u}\\) is similarly obtained by continuing down the column to find a tabled probability \\(p^{\\prime} \\cong (1 − \\alpha / 2)\\). These critical values \\(x_{l}\\) and \\(x_{u}\\) are used to compute the ranks \\(R_{l}\\) and \\(R_{u}\\) corresponding to the data values at the upper and lower ends of the confidence limit (equations (3.14) and (3.15)). The resulting confidence level of the interval will equal (\\(p^{\\prime} − p^{*}\\)). \\[\\begin{equation} R_{l} = x_{l} +1 \\tag{3.14} \\end{equation}\\] \\[\\begin{equation} R_{u} = x_{u} \\tag{3.15} \\end{equation}\\] Example 2,cont. For the arsenic concentrations of Boudette et al. (1985), determine a 95% confidence interval on \\(C_{0.20}\\), the 20th percentile of concentration (\\(p = 0.2\\)). The sample 20th percentile \\(\\hat{C}_{0.20} = 2.9\\) ppb, the \\(0.20 \\bullet (26) = 5.2\\) smallest observation, or two-tenths of the distance between the 5th and 6th smallest observations. To determine a 95% confidence interval for the true 20th percentile \\(C_{0.20}\\), the binomial table from a statistics text such as Bhattacharyya and Johnson (1977) is entered at the \\(p = 0.20\\) column. The integer \\(x_{l}\\) having an entry nearest to \\(\\alpha / 2 = 0.025\\) is found to be \\(1\\) (\\(p^{*} = 0.027\\), the error probability for the lower side of the distribution). From equation (3.14) the rank \\(R_{l} = 2\\). Going further down the column, \\(p^{\\prime} = 0.983\\) for an \\(x_{u} = R_{u} = 9\\). Therefore a 95.6% confidence interval (\\(0.983 − 0.027 = 0.956\\)) for the 20th percentile is the range between the 2nd and 9th observations, or \\[1.5 \\leq C_{0.20} \\leq 8 \\;\\; \\text{at} \\; \\alpha = 0.044\\] The asymmetry around \\(\\hat{C}_{0.20} = 2.9\\) reflects the skewness of the data. When \\(n &gt; 20\\), a large-sample (normal) approximation to the binomial distribution can be used to obtain interval estimates for percentiles. From a table of quantiles of the standard normal distribution, \\(z_{\\alpha / 2}\\) and \\(z_{\\lbrack 1 - \\alpha / 2 \\rbrack}\\) (the \\(\\alpha / 2\\)th and \\(\\lbrack 1 − \\alpha / 2 \\rbrack\\)th normal quantiles) determine the upper and lower ranks of observations corresponding to the ends of the confidence interval. Those ranks are \\[\\begin{equation} R_{l} = np + z_{\\alpha / 2} \\bullet \\sqrt{np(1 - p)} + 0.5 \\tag{3.16} \\end{equation}\\] \\[\\begin{equation} R_{u} = np + z_{\\lbrack 1 - \\alpha / 2 \\rbrack} \\bullet \\sqrt{np(1 - p)} + 0.5 \\tag{3.17} \\end{equation}\\] The 0.5 terms added to each reflect a continuity correction (see Chapter 4) of 0.5 for the lower bound and −0.5 for the upper bound, plus the +1 term for the upper bound analogous to equation (3.5). The computed ranks \\(R_{u}\\) and \\(R_{l}\\) are rounded to the nearest integer. Example 2,cont. Using the large sample approximation of equations (3.16) and (3.17), what is a 95% confidence interval estimate for the true 0.2 quantile? Using \\(z_{\\alpha / 2} = - 1.96\\) the lower and upper ranks of the interval are \\[\\begin{equation} \\begin{aligned} R_{l} &amp;= 25 \\bullet 0.2 + (-1.96) \\bullet \\sqrt{25 \\bullet 0.2(1 - 0.2)} + 0.5 &amp;&amp;= 5 - 1.96 \\bullet 2 + 0.5 &amp;&amp;&amp;= 1.6 \\\\ R_{u} &amp;= 25 \\bullet 0.2 + 1.96 \\bullet \\sqrt{25 \\bullet 0.2(1 - 0.2)} + 0.5 &amp;&amp;= 5 + 1.96 \\bullet 2 + 0.5 &amp;&amp;&amp;= 9.4 \\end{aligned} \\end{equation}\\] After rounding, the 2nd and 9th ranked observations are found to be an approximate \\(\\alpha = 0.05\\) confidence limit on \\(C_{0.2}\\), agreeing with the exact confidence limit computed previously. 3.7.2 Nonparametric Tests For Percentiles Often it is of interest to test whether a percentile is different from, or larger or smaller than, some specified value. For example, a water quality standard \\(X_{0}\\) could be set such that the median of daily concentrations should not exceed \\(X_{0}\\) ppb. Or the 10-year flood (90th percentile of annual peak flows) may be tested to determine if it differs from a regional design value \\(X_{0}\\). Detailed discussions of hypothesis tests do not begin until the next chapter. However, a simple way to view such a test is discussed below. It is directly related to confidence intervals. 3.7.2.1 N-P test for whether a percentile differs from \\(X_{0}\\) (a two-sided test) To test whether the percentile of a data set is significantly different (either larger or smaller) from a pre-specified value X0, compute an interval estimate for the percentile as described in section 3.7.1. If \\(X_{0}\\) falls within this interval, the percentile is not significantly different from \\(X_{0}\\) at a significance level = \\(\\alpha\\) (figure 3.9). If \\(X_{0}\\) is not within the interval, the percentile significantly differs from \\(X_{0}\\) at the significance level of \\(\\alpha\\). Figure 3.9: Interval estimate of \\(p\\)th percentile \\(X_{p}\\) as a test for whether \\(X_{p} = X_{0}\\). A. \\(X_{0}\\) inside interval estimate: \\(X_{p}\\) not significantly different from \\(X_{0}\\). B. \\(X_{0}\\) outside interval estimate: \\(X_{p}\\) significantly different from \\(X_{0}\\). Example 3 In Appendix C1 are annual peak discharges for the Saddle River at Lodi, NJ from 1925 to 1967. Of interest is the 5-year flood, the flood which is likely to be equalled or exceeded once every 5 years (20 times in 100 years), and so is the 80th percentile of annual peaks. Though flood percentiles are usually computed assuming a Log Pearson Type III or Gumbel distribution (Ponce (1989)), here they will be estimated by the sample 80th percentile. Is there evidence that the 20-year flood between 1925-1967 differs from a design value of 1300 cfs at an \\(\\alpha = 0.05\\)? The 80th percentile is estimated from the 43 values between 1925 and 1967 as the \\(0.8 \\bullet (44) = 35.2\\) value when ranked from smallest to largest. Therefore \\(\\hat{Q}_{0.8} = 1672\\) cfs, 0.2 of the distance between the 35th and 36th ranked peak flow. A two-sided confidence interval on this percentile is (following equations (3.16) and (3.17)): \\[\\begin{equation} \\begin{aligned} &amp;R_{l} = np + z_{\\alpha / 2} \\bullet \\sqrt{np(1 - p)} + 0.5 &amp;&amp;R_{u} = np + z_{\\lbrack 1 - \\alpha / 2 \\rbrack} \\bullet \\sqrt{np(1 - p)} + 0.5 \\\\ &amp;R_{l} = 43(0.8) - 1.96 \\bullet \\sqrt{43 \\bullet 0.8 (0.2)} + 0.5 &amp;&amp;R_{u} = 43(0.8) + 1.96 \\bullet \\sqrt{43 \\bullet 0.8 (0.2)} + 0.5 \\\\ &amp;R_{l} = 29.8 &amp;&amp;R_{u} = 40.0\\\\ \\end{aligned} \\end{equation}\\] The \\(\\alpha = 0.05\\) confidence interval lies between the 30th and 40th ranked peak flows, or \\[1370 &lt; Q_{0.8} &lt; 1860\\] which does not include the design value \\(X_{0} = 1300\\) cfs. Therefore the 20-year flood does differ from the design value at a significance level of 0.05. 3.7.2.2 N-P test for whether a percentile exceeds \\(X_{0}\\) (a one-sided test) To test whether a percentile \\(X_{p}\\) significantly exceeds a specified value or standard \\(X_{0}\\), compute the one-sided confidence interval of section 3.7.1. Remember that the entire error level \\(\\alpha\\) is placed on the side below the percentile point estimate \\(\\hat{X}_{p}\\) (figure 3.10). \\(X_{p}\\) will be declared significantly higher than \\(X_{0}\\) if its one-sided confidence interval lies entirely above \\(X_{0}\\). Figure 3.10: One-sided interval estimate as a test for whether \\(X_{p} &gt; X_{0}\\). A. \\(X_{0}\\) inside interval estimate: \\(X_{p}\\) not significantly greater than \\(X_{0}\\). B. \\(X_{0}\\) below interval estimate: \\(X_{p}\\) significantly greater than \\(X_{0}\\). Example 2,cont. Suppose that a water-quality standard stated that the 90th percentile of arsenic concentrations in drinking water shall not exceed 300 ppb. Has this standard been violated at the \\(\\alpha = 0.05\\) confidence level by the New Hampshire data of example 2? The 90th percentile of the example 2 arsenic concentrations is \\[\\begin{equation} \\begin{aligned} \\hat{C}_{0.90} &amp;= (25 + 1) \\bullet 0.9\\text{th} &amp;&amp;= 23.4\\text{th data point} &amp;&amp;&amp;= 300 + 0.4(340 − 300) \\\\ &amp;= 316 \\text{ppb.} \\end{aligned} \\end{equation}\\] Following equation (3.16) but using \\(\\alpha\\) instead of \\(\\alpha / 2\\), the rank of the observation corresponding to a one-sided 95% lower confidence bound on \\(C_{0.90}\\) is \\[\\begin{equation} \\begin{aligned} R_{l} &amp;= np + z_{\\alpha} \\bullet \\sqrt{np(1 - p)} + 0.5 &amp;&amp;= 25 \\bullet 0.9 + z_{0.05} \\bullet \\sqrt{25 \\bullet 0.9(0.1)} + 0.5 \\\\ &amp;= 22.5 + (- 1.64) \\bullet \\sqrt{2.25} + 0.5 \\\\ &amp;= 20.5 \\end{aligned} \\end{equation}\\] and thus the lower confidence limit is the 20.5th lowest observation, or 215 ppb, halfway between the 20th and 21st observations. This confidence limit is less than \\(X_{0} = 300\\), and therefore the standard has not been exceeded at the 95% confidence level. 3.7.2.3 N-P test for whether a percentile is less than \\(X_{0}\\) (a one-sided test) To test whether a percentile \\(X_{p}\\) is significantly less than \\(X_{0}\\), compute the one-sided confidence interval placing all error \\(\\alpha\\) on the side above \\(\\hat{X}_{p}\\) (figure 3.11). \\(X_{p}\\) will be declared as significantly less than \\(X_{0}\\) if its one-sided confidence interval is entirely below \\(X_{0}\\). Figure 3.11: One-sided interval estimate as a test for whether \\(X_{p} &lt; X_{0}\\). A. \\(X_{0}\\) inside interval estimate: \\(X_{p}\\) not significantly less than \\(X_{0}\\). B. \\(X_{0}\\) above interval estimate: \\(X_{p}\\) significantly less than \\(X_{0}\\). Example 4 The following 43 values are annual 7-day minimum flows for 1941−1983 on the Little Mahoning Creek at McCormick, PA. Though percentiles of low flows are often computed using a Log Pearson Type III distribution, here the sample estimate of the percentile will be computed. Is the \\(7Q10\\) low-flow (the 10th percentile of these data) significantly less than 3 cfs at \\(\\alpha = 0.05\\)? 0.69 0.80 1.30 1.40 1.50 1.50 1.80 1.80 2.10 2.50 2.80 2.90 3.00 3.10 3.30 3.70 3.80 3.80 4.00 4.10 4.20 4.30 4.40 4.80 4.90 5.70 5.80 5.90 6.00 6.10 7.90 8.00 8.00 9.70 9.80 10.00 11.00 11.00 12.00 13.00 16.00 20.00 23.00 The sample 10th percentile of the data is 4.4th observation, or \\(\\hat{7Q}_{0.10} = 1.4\\) cfs. The upper 95% confidence interval for \\(Q_{0.10}\\) is located (following equation (3.17) but using \\(\\alpha\\)) at rank \\(R_{u}\\): \\[\\begin{equation} \\begin{aligned} R_{l} &amp;= np + z_{\\lbrack 1 - \\alpha \\rbrack} \\bullet \\sqrt{np(1 - p)} + 0.5\\\\ &amp;= 43 \\bullet 0.1 + 1.64 \\bullet \\sqrt{43 \\bullet 0.1(0.9)} + 0.5 \\\\ &amp;= 8.0 \\end{aligned} \\end{equation}\\] So the upper 95% confidence limit equals 1.8 cfs. This is below the \\(X_{0}\\) of 3 cfs, and therefore the \\(7Q10\\) is significantly less than 3 cfs at an \\(\\alpha = 0.05\\). 3.7.3 Parametric Confidence Intervals For Percentiles Confidence intervals for percentiles can also be computed by assuming that data follow a particular distribution. Distributional assumptions are employed because there are often insufficient data to compute percentiles with the required precision. Adding information contained in the distribution will increase the precision of the estimate as long as the distributional assumption is a reasonable one. However when the distribution which is assumed does not fit the data well, the resulting estimates are less accurate, and more misleading, than if nothing were assumed. Unfortunately, the situation in which an assumption is most needed, that of small sample sizes, is the same situation where it is difficult to determine whether the data follow the assumed distribution. There is little theoretical reason why data should follow one distribution over another. As stated in Chapter 1, most environmental data have a lower bound at zero and may have quite large observations differing from the bulk of the data. Distributions fit to such data must posses skewness, such as the lognormal. But few “first principles” can be drawn on to favor one skewed distribution over another. Empirical studies have found that for specific locations and variables certain distributions seem to fit well, and those have become traditionally used. Thus the lognormal, Pearson Type III and Gumbel distributions are commonly assumed in water resources applications. Computation of point and interval estimates for percentiles assuming a lognormal distribution are straightforward. First the sample mean \\(\\overline{y}\\) and sample standard deviation \\(s_{y}\\) of the logarithms are computed. The point estimate is then \\[\\begin{equation} \\hat{X}_{p} = \\exp{\\left( \\overline{y} + z_{p} \\bullet s_{y} \\right)} \\tag{3.18} \\end{equation}\\] where \\(z_{p}\\) is the pth quantile of the standard normal distribution and \\(y = \\ln{(x)}\\). The interval estimate for the median was previously given by equation (3.7) assuming that data are lognormal. For other percentiles, confidence intervals are computed using the non-central t-distribution (Stedinger (1983)). Tables of that distribution are found in Stedinger’s article, with more complete entries online in commercial computer mathematical libraries. The confidence interval on \\(X_{p}\\) is: \\[\\begin{equation} CI \\left({X}_{p} \\right) = \\exp{\\left( \\overline{y} + \\zeta_{\\alpha / 2} \\bullet s_{y}, \\overline{y} + \\zeta_{\\lbrack 1 - \\alpha / 2 \\rbrack} \\bullet s_{y}\\right)} \\tag{3.19} \\end{equation}\\] where \\(\\zeta_{\\alpha / 2}\\) is the \\(\\alpha / 2\\) quantile of the non-central t distribution for the desired percentile with sample size of \\(n\\). Example 2,cont. Compute a 90% interval estimate for the 90th percentile of the New Hampshire arsenic concentrations, assuming the data are lognormal. The 90th percentile assuming concentrations are lognormal is as given in equation (3.18): \\[\\begin{equation} \\begin{aligned} \\hat{C}_{0.90} &amp;= \\exp{\\left( \\overline{y} + z_{0.90} \\bullet s_{y} \\right)} &amp;&amp;= \\exp{(3.17 + 1.28 \\bullet 1.96)} \\\\ &amp;= 292.6 \\; \\text{ppb.} \\end{aligned} \\end{equation}\\] (which is lower than the sample estimate of 316 ppb obtained without assuming the data are lognormal). The corresponding 90% interval estimate from equation (3.19) is: \\[\\begin{equation} \\begin{aligned} \\exp{\\left( \\overline{y} + \\zeta_{0.05} \\bullet s_{y} \\right)} &lt; &amp; C_{0.90} &amp;&amp; &lt; \\exp{\\left( \\overline{y} + \\zeta_{0.95} \\bullet s_{y} \\right)} \\\\ \\exp{\\left( 3.17 + 0.898 \\bullet 1.96 \\right)} &lt; &amp; C_{0.90} &amp;&amp; &lt; \\exp{\\left( 3.17 + 1.838 \\bullet 1.96 \\right)} \\\\ 138.4 &lt; &amp; C_{0.90} &amp;&amp; &lt; 873.5 \\end{aligned} \\end{equation}\\] This estimate would be preferred over the nonparametric estimate if it was believed that the data were truly lognormal. Otherwise a nonparametric interval would be preferred. When the data are truly lognormal, the two intervals should be quite similar. Interval estimates for percentiles of the Log Pearson III distribution are computed in a similar fashion. See Stedinger (1983) for details on the procedure. 3.7.4 Parametric Tests For Percentiles Analogous to section 3.7.2, parametric interval estimates may be used to conduct a parametric test for whether a percentile is different from (2-sided test), exceeds (1-sided test), or is less than (1-sided test) some specified value \\(X_{0}\\). With the 2-sided test for difference, if \\(X_{0}\\) falls within the interval having \\(\\alpha / 2\\) on either side, the percentile is not proven to be significantly different from \\(X_{0}\\). If \\(X_{0}\\) falls outside this interval, the evidence supports \\(X_{p} \\neq X_{0}\\) with an error level of \\(\\alpha\\). For the one-sided tests, the error level \\(\\alpha\\) is placed entirely on one side before conducting the test, and \\(X_{0}\\) is again compared to the end of the interval to determine difference or similarity. Example 2,cont. Test whether the 90th percentile of arsenic concentrations in drinking water exceeds 300 ppb at the \\(\\alpha = 0.05\\) significance level, assuming the data are lognormal. The one-sided 95% lower confidence limit for the 90th percentile was computed above as 138.4 ppb. (note the nonparametric bound was 215 ppb). This limit is less than the \\(p_{0}\\) value of 300, and therefore the standard has not been exceeded at the 95% confidence level. 3.8 Other Uses For Confidence Intervals Confidence intervals are used for purposes other than as interval estimates. Three common uses are to detect outliers, for quality control charts, and for determining sample sizes necessary to achieve a stated level of precision. Often overlooked are the implications of data non-normality for the three applications. These are discussed in the following three sections. 3.8.1 Implications of Non-Normality For Detection of Outliers An outlier is an observation which appears to differ in its characteristics from the bulk of the data set to which it is assigned. It is a subjective concept – different people may define specific points as either outliers, or not. Outliers are sometimes deleted from a data set in order to use procedures based on the normal distribution. One of the central themes of this book is that this is a dangerous and unwarranted practice. It is dangerous because these data may well be totally valid. There is no law stating that observed data must follow some specific distribution, such as the normal. Outlying observations are often the most important data collected, providing insight into extreme conditions or important causative relationships. Deleting outliers is unwarranted because procedures not requiring an assumption of normality are both available and powerful. Many of these are discussed in the following chapters. In order to delete an outlier, an observation must first be declared to be one. Rules or “tests” for outliers have been used for years, as surveyed by Beckman and Cook (1983). The most common tests are based on a t-interval, and assume that data follow a normal distribution. Usually equation (3.12) for a normal prediction interval is simplified by assuming the (\\(s^{2} / n\\)) terms under the square root sign are negligable compared to \\(s^{2}\\) (true for large \\(n\\)). Points beyond the simplified prediction interval are declared as outliers, and dropped. Real world data may not follow a normal distribution. As opposed to a mean of large data sets, there is no reason to assume that they should. Rejection of points by outlier tests may not indicate that data are in any sense in error, but only that they do not follow a normal distribution (Fisher (1922)). For example, below are 25 observations from a lognormal distribution. When the t-prediction interval is applied with \\(\\alpha = 0.05\\), the largest observation is declared to be an outlier. Yet it is known to be from the same non-normal distribution as generated the remaining observations. Multiple outliers cause other problems for outlier tests that are based on normality (Beckman and Cook (1983)). They may so inflate the estimated standard deviation that no points are declared as outliers. When several points are spaced at increasingly larger distances from the mean, the first may be declared an outlier upon using the test once, but re-testing after deletion causes the second largest to be rejected, and so on. Replication of the test may eventually discard a substantial part of the data set. The choice of how many times to apply the test is entirely arbitrary. 3.8.2 Implications of Non-Normality For Quality Control A visual presentation of confidence intervals used extensively in industrial processes is a control chart (Montgomery (1991)). A small number of products are sampled from the total possible at a given point in time, and their mean calculated. The sampling is repeated at regular or random intervals, depending on the design, resulting in a series of sample means. These are used to construct one type of control chart, the xbar chart. This chart visually detects when the mean of future samples become different from those used to construct the chart. The decision of difference is based on exceeding the parametric confidence interval around the mean given in section 3.4.1. Suppose a chemical laboratory measures the same standard solution at several times during a day to determine whether the equipment and operator are producing consistent results. For a series of \\(n\\) measurements at \\(m\\) time intervals, the total sample size \\(N = n \\bullet m\\). The best estimate of the concentration for that standard is the overall mean \\[\\overline{X} = \\sum_{i=1}^{N} \\frac{X_{i}}{N}\\] \\(\\overline{X}\\) is plotted as the center line of the chart. A confidence interval on that mean is described by equation (3.8), using the sample size \\(n\\) available for computing each individual mean value. Those interval boundaries are also plotted as parallel lines on the quality control chart. Mean values will on average plot outside of these boundaries only \\(\\alpha \\bullet 100\\)% of the time if the means are normally distributed. Points falling outside the boundaries more frequently than this are taken to indicate that something in the process has changed. If \\(n\\) is large (say 30 or more) the Central Limit Theorem states that the means will be normally distributed even though the underlying data may not be. However if \\(n\\) is much smaller, as is often the case, the means may not follow this pattern. In particular, for skewed data (data with outliers on only one side), the distribution around the mean may still be skewed. The result is a large value for the standard deviation, and wide confidence bands. Therefore the chart will have lower power to detect departures or drifts away from the expected mean value than if the data were not skewed. Control charts are also produced to illustrate process variance. These either use the range (R chart) or standard deviation (S chart). Both charts are even more sensitive to departures from normality than is the \\(\\overline{X}\\) chart (Montgomery (1991)). Both will have a difficult time in detecting changes in variance when the underlying data are non-normal, and the sample size \\(n\\) for each mean is small. In water quality studies the most frequent application of control charts is to laboratory chemical analyses. As chemical data tend to be positively skewed, control charts on the logs of the data are usually more applicable than those in the original units. Otherwise large numbers of samples must be used to determine mean values. Use of logarithms results in the center line estimating the median in original units, with multiplicative variation represented by the confidence bands of section 3.3.2. Nonparametric control charts may be utilized if sample sizes are sufficiently large. These could use the confidence intervals for the median rather than the mean, as in section 3.3. Alternatively, limits could be set around the mean or median using the “F-psuedosigma” of Hoaglin, Mosteller, and Tukey (1983). This was done by Schroder, Brooks, and Willoughby (1987). The F-psuedosigma is the interquartile range divided by 1.349. It equals the standard deviation for a normal distribution, but is not as strongly affected by outliers. It is most useful for characterizing symmetric data containing outliers at both ends, providing a more resistant measure of spread than does the standard deviation. 3.8.3 Implications of Non-Normality For Sampling Design The t-interval equations are also used to determine the number of samples necessary to estimate a mean with a specified level of precision. However, such equations require the data to approximately follow a normal distribution. They must consider power as well as the interval width. Finally, one must decide whether the mean is the most appropriate characteristic to measure for skewed data. To estimate the sample size sufficient for determining an interval estimate of the mean with a specified width, equation (3.8) is solved for \\(n\\) to produce \\[\\begin{equation} n = \\left( \\frac{t_{\\alpha / 2, n - 1}s}{\\Delta} \\right)^{2} \\tag{3.20} \\end{equation}\\] where \\(s\\) is the sample standard deviation and \\(\\Delta\\) is one-half the desired interval width. Sanders (1983) and other authors have promoted this equation. As discussed above, for sample sizes less than 30 to 50 and even higher with strongly skewed data, this calculation may have large errors. Estimates of s will be inaccurate, and strongly inflated by any skewness and/or outliers. Resulting estimates of n will therefore be large. For example, Håkanson (1984) estimated the number of samples necessary to provide reasonable interval widths for mean river and lake sediment characteristics, including sediment chemistry. Based on the coefficients of variation reported in the article, the data for river sediments were quite skewed, as might be expected. Necessary sample sizes for rivers were calculated at 200 and higher. Before using such simplistic equations, skewed data should be transformed to something closer to symmetry, if not normality. For example, logarithms will drastically lower estimated sample sizes for skewed data, equivalent to equation (3.13). Samples sizes would result which allow the median (geometric mean) to be estimated within a multiplicative tolerance factor equal to $ 2 $ in log units. A second problem with equations like (3.20) for estimating sample size, even when data follow a normal distribution, is pointed out by Kupper and Hafner (1989). They show that eq. (3.20) underestimates the true sample size needed for a given level of precision, even for estimates of \\(n \\geq 40\\). This is because eq. (3.20) does not recognize that the standard deviation \\(s\\) is only an estimate of the true value \\(\\sigma\\). They suggest adding a tolerance probability to eq. (3.20), akin to a statement of power. Then the estimated interval width will be at least as small as the desired interval width for some stated percentage (say 90 or 95%) of the time. For example, when \\(n\\) would equal 40 based on equation (3.20), the resulting interval width will be less than the desired width \\(2 \\Delta\\) only about 42% of the time! The sample size should instead be 53 in order to insure the interval width is within tolerance range 90% of the time. They conclude that eq. (3.20) and similar equations which do not take power into consideration “behave so poorly in all instances that their future use should be strongly discouraged”. Sample sizes necessary for interval estimates of the median or to perform the nonparametric tests of later chapters may be derived without the assumption of normality required above for t-intervals. Noether (1987) describes these more robust sample size estimates, which do include power considerations and so are more valid than equation (3.20). However, neither the normaltheory or nonparametric estimates consider the important and frequently-observed effects of seasonality or trend, and so may never provide estimates sufficiently accurate to be anything more than a crude guide. Exercises 3.1 Compute both nonparametric and parametric 95% interval estimates for the median of the granodiorite data of exercise 2.3. Which is more appropriate for these data? Why? x &lt;- c(6.0, 0.5, 0.4, 0.7, 0.8, 6.0, 5.0, 0.6, 1.2, 0.3, 0.2, 0.5, 0.5, 10, 0.2, 0.2, 1.7, 3.0) x &lt;- sort(x) median_x &lt;- median(x) N &lt;- length(x) ALPHA &lt;- 1 - 0.95 R_l &lt;- qbinom(ALPHA / 2, N, 0.5) R_u &lt;- N - R_l x_l &lt;- x[R_l] x_u &lt;- x[R_u] nonparametric &lt;- paste0(&quot;nonparametric: &quot;, x_l, &quot; ≤ &quot;, median_x, &quot; ≤ &quot;, x_u) y &lt;- log(x) GM_x &lt;- exp(mean(y)) t &lt;- abs(qt(ALPHA / 2, df = N - 1)) y_l &lt;- exp(mean(y) - (t * sqrt(var(y) / N))) y_u &lt;- exp(mean(y) + (t * sqrt(var(y) / N))) parametric &lt;- paste0(&quot;parametric: &quot;, y_l, &quot; ≤ &quot;, GM_x, &quot; ≤ &quot;, y_u) nonparametric ## [1] &quot;nonparametric: 0.4 ≤ 0.65 ≤ 1.7&quot; parametric ## [1] &quot;parametric: 0.506466777225634 ≤ 0.955914691054861 ≤ 1.80421093280798&quot; Either of intervals is reasonable. The logs of the data still retain some skewness, so the nonparametric interval may be more realistic. The choice would depend on whether the assumption of lognormality was believed. 3.2 Compute the symmetric 95% interval estimate for the mean of the quartz monzonite data of exercise 2.3. Compute the sample mean, and the mean assuming the data are lognormal. Which point estimate is more appropriate for these data? Why? x &lt;- c(1.0, 0.2, 1.2, 1.0, 0.3, 0.1, 0.1, 0.4, 3.2, 0.3, 0.4, 1.8, 0.9, 0.1, 0.2, 0.3, 0.5) x &lt;- sort(x) mean_x &lt;- mean(x) s2_x &lt;- var(x) N &lt;- length(x) ALPHA &lt;- 1 - 0.95 t &lt;- abs(qt(ALPHA / 2, df = N - 1)) x_l &lt;- mean_x - (t * sqrt(s2_x / N)) x_u &lt;- mean_x + (t * sqrt(s2_x / N)) symmetric &lt;- paste0(&quot;Symmetric: &quot;, x_l, &quot; ≤ &quot;, mean_x, &quot; ≤ &quot;, x_u) y &lt;- log(x) mean_y &lt;- mean(y) s2_y &lt;- var(y) mean_lognorm_x &lt;- exp(mean_y + (0.5 * s2_y)) ln_1 &lt;- paste0(&quot;Point estimates: mean = &quot;, mean_x, &quot; (assuming normal distribution)&quot;) ln_2 &lt;- paste0(&quot; mean = &quot;, mean_lognorm_x, &quot; (assuming a lognormal distribution).&quot;) cat(symmetric, ln_1 , ln_2, sep = &quot;\\n&quot;) ## Symmetric: 0.295175103687019 ≤ 0.705882352941177 ≤ 1.11658960219533 ## Point estimates: mean = 0.705882352941177 (assuming normal distribution) ## mean = 0.729414817622854 (assuming a lognormal distribution). As the logs of the data are more symmetric than the data prior to transformation, the lognormal (2nd) estimate is preferred. 3.3 A well yield of 0.85 gallons/min/foot was measured in a well in Virginia. Is this yield likely to belong to the same distribution as the data in exercise 1.1, or does it represent something larger? Answer by computing 95% parametric and nonparametric intervals. Which interval is more appropriate for these data? library(data.table) x &lt;- fread(&#39;data/well_yields_virginia.csv&#39;) x &lt;- x$unit_well_yield x &lt;- sort(x) mean_x &lt;- mean(x) s2_x &lt;- var(x) N &lt;- length(x) ALPHA &lt;- 1 - 0.95 t &lt;- abs(qt(ALPHA / 2, df = N - 1)) PI_l &lt;- mean_x - (t * sqrt(s2_x + (s2_x / N))) PI_u &lt;- mean_x + (t * sqrt(s2_x + (s2_x / N))) sol_0 &lt;- &quot;Parametric 95% prediction interval: &quot; sol_1 &lt;- paste0(PI_l, &quot; to &quot;, PI_u, &quot; gallons/min/foot.&quot;) sol_2 &lt;- &quot;Includes 0.85, so same distribution.&quot; low_idx &lt;- (ALPHA / 2) * (N + 1) up_idx &lt;- (1 - (ALPHA / 2)) * (N + 1) sol_3 &lt;- &quot;Nonparametric 95% prediction interval: &quot; sol_4 &lt;- paste0(&quot;X_&quot;, low_idx, &quot; to &quot;, &quot;X_&quot;, up_idx) cat(sol_0, sol_1, sol_2, sol_3, sol_4, sep = &quot;\\n&quot;) ## Parametric 95% prediction interval: ## -0.525184969204106 to 0.905684969204106 gallons/min/foot. ## Includes 0.85, so same distribution. ## Nonparametric 95% prediction interval: ## X_0.325 to X_12.675 The sample size is too small to produce such a wide (95%) nonparametric prediction interval. Therefore a parametric interval must be used. However, the negative lower end of the parametric prediction interval indicates that a symmetric interval is not appropriate. So an asymmetric interval resulting from taking logarithms should be used instead of the one above. 3.4 Construct the most appropriate 95 percent interval estimates for the mean and median annual streamflows for the Conecuh River at Brantley, Alabama (data in Appendix C2). library(data.table) library(ggplot2) source(&quot;./data/themes.R&quot;) source(&quot;./data/palettes.R&quot;) x &lt;- fread(&#39;data/appendix_c2.csv&#39;) graph_a &lt;- ggplot(x, aes(x = Flow_cfs)) + geom_density(color = &quot;black&quot;, fill = palettes_bright$colset_cheer_brights[2], alpha = 0.4) + theme_generic + labs(x = &quot;Flow in cfs&quot;, y = &quot;DENSITY&quot;) + scale_x_continuous(limits = c(0,1500)) x &lt;- x$Flow_cfs x &lt;- sort(x) mean_x &lt;- mean(x) s2_x &lt;- var(x) N &lt;- length(x) ALPHA &lt;- 1 - 0.95 t &lt;- abs(qt(ALPHA / 2, df = N - 1)) x_l &lt;- mean_x - (t * sqrt(s2_x / N)) x_u &lt;- mean_x + (t * sqrt(s2_x / N)) mean_x_range &lt;- mean_x - x_l sol_0 &lt;- &quot;The data look relatively symmetric, so no logs taken.&quot; sol_1 &lt;- paste0(&quot;mean: &quot;, mean_x, &quot; ± &quot;, mean_x_range, &quot;, or &quot;) sol_2 &lt;- paste0(x_l, &quot; to &quot;, x_u) median_x &lt;- median(x) R_l &lt;- qbinom(ALPHA / 2, N, 0.5) R_u &lt;- N - R_l sol_3 &lt;- paste0(&quot;median: R_l = &quot;, R_l, &quot;, R_u = &quot;, R_u) sol_4 &lt;- paste0(x[R_l], &quot; to &quot;, x[R_u]) graph_a cat(sol_0, sol_1, sol_2, sol_3, sol_4, sep = &quot;\\n&quot;) ## The data look relatively symmetric, so no logs taken. ## mean: 682.75 ± 126.136783051822, or ## 556.613216948178 to 808.886783051822 ## median: R_l = 6, R_u = 14 ## 524 to 859 3.5 Suppose a water intake is to be located on the Potomac River at Chain Bridge in such a way that the intake should not be above the water surface more than 10 percent of the time. Data for the design year (365 daily flows, ranked in order) are given in Appendix C3. Compute a 95% confidence interval for the daily flow guaranteed by this placement during the 90% of the time the intake is below water. library(data.table) x &lt;- fread(&#39;data/appendix_c3.csv&#39;) x &lt;- as.vector(t(x)) x &lt;- sort(x) mean_x &lt;- mean(x) sd_x &lt;- sd(x) N &lt;- length(x) p &lt;- 0.1 np &lt;- N * p ALPHA &lt;- 0.05 idxs &lt;- c(as.integer(np), as.integer(np) + 1) x_10 &lt;- x[idxs[1]] + ((np - idxs[1]) * (x[idxs[2]] - x[idxs[1]])) z_095 &lt;- qnorm((1 - ALPHA), mean = 0, sd = 1) R_u &lt;- np + (z_095 * sqrt(np * (1 - p))) + 0.5 x_u &lt;- as.numeric(x[as.integer(R_u)]) sol_0 &lt;- paste0(&quot;The 10th percentile = &quot;, x_10, &quot; cfs&quot;) sol_1 &lt;- &quot;A one-sided 95% confidence interval for the 90th percentile (an upper confidence limit&quot; sol_2 &lt;- &quot;to insure that the intake does not go dry) is found using the large-sample&quot; sol_3 &lt;- &quot;approximation of equation 3.17: &quot; sol_4 &lt;- paste0(&quot;R_u = &quot;, R_u) sol_5 &lt;- paste0(&quot;The 46th ranked point is the upper CI, or &quot;, x_u ,&quot; cfs&quot;) cat(sol_0, sol_1, sol_2, sol_3, sol_4, sol_5, sep = &quot;\\n&quot;) ## The 10th percentile = 2445 cfs ## A one-sided 95% confidence interval for the 90th percentile (an upper confidence limit ## to insure that the intake does not go dry) is found using the large-sample ## approximation of equation 3.17: ## R_u = 46.4274653256872 ## The 46th ranked point is the upper CI, or 2700 cfs References "],
["ch4.html", "Chapter 4 Hypothesis Tests 4.1 Classification of Hypothesis Tests 4.2 Structure of Hypothesis Tests 4.3 The Rank-Sum Test as an Example of Hypothesis Testing 4.4 Tests for Normality Exercises", " Chapter 4 Hypothesis Tests Scientists collect data in order to learn about the processes and systems those data represent. Often they have prior ideas, called hypotheses, of how the systems behave. One of the primary purposes of collecting data is to test whether those hypotheses can be substantiated, with evidence provided by the data. Statistical tests are the most quantitative ways to determine whether hypotheses can be substantiated, or whether they must be modified or rejected outright. One important use of hypothesis tests is to evaluate and compare groups of data. Water resources scientists have made such comparisons for years, sometimes without formal test procedures. For example, water quality has been compared between two or more aquifers, and some statements made as to which are different. Historic frequencies of exceeding some critical surface-water discharge have been compared with those observed over the most recent 10 years. Rather than using hypothesis tests, the results are sometimes expressed as the author’s educated opinions – “it is clear that development has increased well yield.” Hypothesis tests have at least two advantages over educated opinion: they insure that every analyst of a data set using the same methods will arrive at the same result. Computations can be checked on and agreed to by others. they present a measure of the strength of the evidence (the p-value). The decision to reject an hypothesis is augmented by the risk of that decision being incorrect. In this chapter hypothesis tests are classified based on when each is appropriate for use. The basic structure of hypothesis testing is introduced. The rank-sum test is used to illustrate this structure, as well as to illustrate the origin of tables of test statistic quantiles found in most statistics textbooks. Finally, tests for normality are discussed. Concepts and terminology found here will be used throughout the rest of the book. Figure 4.1: Five types of hypothesis tests 4.1 Classification of Hypothesis Tests The numerous varieties of hypothesis tests often cause unnecessary confusion to scientists. Tests can be classified into the five major types shown in figure 4.1, based on the measurement scales of the data being tested. Within these types, the distributional shape of the data determine which of two major divisions of hypothesis tests, parametric or nonparametric, are appropriate for use. Thus the data, along with the objectives of the study, determine which test procedure should be employed. The terms response variable and explanatory variable are used in the following discussion. A response variable is one whose variation is being studied. In the case of regression, for example, the response variable is sometimes called the “dependent variable” or “y variable”. An explanatory variable is one used to explain why and how the magnitude of the response variable changes. With a t-test, for example, the explanatory variable consists of the two categories of data being tested. 4.1.1 Classification Based on Measurement Scales In figure 4.1, five groupings of test procedures are represented by the five boxes. Each differs only in the measurement scales of the response and explanatory variables under study. The scales of measurement may be either continuous or categorical. Both parametric and nonparametric tests may be found within a given box. Tests represented by the three boxes in the top row of figure 4.1 are all similar in that the response variable is measured on a continuous scale. Examples of variables having a continuous scale are concentration, streamflow, porosity, and many of the other items measured by water resources scientists. Tests represented by the two boxes along the bottom of figure 4.1, in contrast, have response variables measured only on a categorical or grouped measurement scale. These variables can only take on a finite, usually small, number of values. They are often designated as letters or integer values. Categorical variables used primarily as explanatory variables include aquifer type, month, land use group, and station number. Categorical variables used as response variables include above/below a reporting limit (perhaps recorded as 0 or 1), presence or absence of a particular species, and low/medium/high risk of contamination. The top left box represents the two- and multi-sample hypothesis tests such as the rank-sum and t-tests. The subject of Chapters 5 through ??, these tests determine whether a continuous response variable (such as concentration) differs in its central value among two or more grouped explanatory variables (such as aquifer unit). The top right box represents two often-used methods – linear regression and correlation. Both relate a continuous response variable (the dependent or y variable) to a continuous explanatory variable (the independent or x variable). Examples include regression of the 100-year flood magnitude versus basin characteristics, and correlations between concentrations of two chemical constituents. Analysis of trends over time is a special case of this class of methods, where the explanatory variable of primary interest is time. The top center box is a blend of these two approaches, called analysis of covariance. A continuous response variable is related to several explanatory variables, some of which are continuous and some categorical. This is discussed in Chapter ??. The bottom left box represents a situation similar to that for use of t-tests or analysis of variance, except that the response variable is categorical. Examples include determining whether the probability of finding a volatile organic above the reporting limit varies by land-use grouping. Contingency tables appropriately measure the association between two such categorical variables. Further information is found in Chapter ??. The bottom right box shows that a regression-type relationship can be developed for the case of a categorical response variable. Perhaps the proportion of pesticide or other data below the reporting limit exceeds fifty percent, and it makes little sense to try to model mean or median concentrations. Instead, the probability of finding a detectable concentration can be related to continuous variables such as population density, percent of impervious surface, irrigation intensities, etc. This is done through the use of logistic regression, one subject of Chapter ??. Logistic regression can also incorporate categorical explanatory variables in a multiple regression context, making it the equivalent of analysis of covariance for categorical response variables. 4.1.2 Classification Based on the Data Distribution Hypothesis tests which assume that the data have a particular distribution (usually a normal distribution, as in Fig. 1.2) are called parametric tests. This is because the information contained in the data is summarized by parameters, usually the mean and standard deviation, and the test statistic is computed using these parameters. This is an efficient process if the data truly follow the assumed distribution. When they do not, however, the parameters may only poorly represent what is actually occurring in the data. The resulting test can then reach an incorrect conclusion, usually because it lacks sensitivity (power) to detect real effects. Hypothesis tests not requiring the assumption that data follow a particular distribution are called distribution-free or nonparametric tests. Information is extracted from the data by comparing each value with all others (ranking the data) rather than by computing parameters. A common misconception is that nonparametric tests “lose information” in comparison to parametric tests because nonparametric tests “discard” the data values. Bradley (Bradley (1968), p.13) responded to this misconception: “Actually, the utilization of the additional sample information [in the parameters] is made possible by the additional population ‘information’ embodied in the parametric test’s assumptions. Therefore, the distribution-free test is discarding information only if the parametric test’s assumptions are known to be true.” Rather than discarding information, nonparametric tests efficiently extract information on the relative magnitudes (ranks) of data without collapsing the information into only a few simple statistics. Both parametric and nonparametric tests will be presented in the upcoming chapters for each category of hypothesis tests. 4.2 Structure of Hypothesis Tests Hypothesis tests are performed by following the structure discussed in the next six sections: STRUCTURE OF HYPOTHESIS TESTS 1) Choose the appropriate test. 2) Establish the null and alternate hypotheses. 3) Decide on an acceptable error rate α. 4) Compute the test statistic from the data. 5) Compute the p-value. 6) Reject the null hypothesis if p ≤ α. 4.2.1 Choose the Appropriate Test Test procedures are selected based on the data characteristics and study objectives. Figure 4.1 presented the first selection criteria – the measurement scales of the data. The second criteria is the objective of the test. Hypothesis tests are available to detect differences between central values of two groups, three or more groups, between spreads of data groups, and for covariance between two or more variables, among others. For example, to compare central values of two independent groups of data, either the t-test or rank-sum test might be selected (see figure 4.2). Subsequent chapters are organized by test objectives, with several alternate tests discussed in each. The third selection criteria is the choice between parametric or nonparametric tests. This should be based on the expected distribution of the data involved. If similar data in the past were normally distributed, a parametric procedure would usually be selected. If data were expected to be non-normal, or not enough is known to assume any specific distribution, nonparametric tests would be preferred. The power of parametric tests to reject H0 when H0 is false can be quite low when applied to non-normal data, and type II errors commonly result Bradley (1968). This loss of power is the primary concern when using parametric tests. Sometimes the choice of test is based on a prior test of normality for that particular data set. If normality is rejected a nonparametric test is chosen. Otherwise, a parametric test is used. This can lead to two problems. First, with small data sets it is difficult to reject the null hypothesis of normality because there is so little evidence on which to base a decision. Tests based on little data have little power. Thus a parametric test might easily be used when the underlying data are actually non-normal. Nonparametric tests are particularly appropriate for small data sets unless experience supports the assumption of normality. Second, small departures from normality not large enough to detect with a test may be sufficiently large to weaken the power of parametric tests. An example is given in Chapter ??. For nearly-normal data, such as produced by power transformations to near-symmetry, the two classes of methods will often give the same result. Test procedures should be selected that have greater power for the types of data expected to be encountered. Comparisons of the power of two test procedures, one parametric and one nonparametric, can be based on the tests’ asymptotic relative efficiencies (ARE), a property of their behavior with large sample sizes (Bradley (1968), p.58). A test with larger ARE will have generally greater power. For non-normal data the ARE of nonparametric tests can be many times those of parametric tests Hollander and Wolfe (1973). Thus their power to reject H0 when it is truly false is generally much higher in this case. When data are produced by a normal distribution, nonparametric tests have generally lower (5-15%) ARE than parametric tests Hollander and Wolfe (1973). Thus nonparametric tests are, in general, never much worse than their parametric counterparts in their ability to detect departures from the null hypothesis, and may be far, far better. As an example, the rank-sum test has a larger ARE (more power) than the t-test for distributions containing outliers (William Jay Conover and Conover (1980), p.225). Kendall and Stuart (Kendall and others (1979), p.540) show that for the gamma distribution (a skewed distribution commonly used in water resources) a moderate skew of 1.15 produces an ARE of greater than 1.25 for the rank-sum versus the t test. As skewness increases, so does the ARE. Therefore in the presence of skewness and outliers, precisely the characteristics commonly shown by water resources data, nonparametric tests exhibit greater power than do parametric tests. One question which always arises is how non-normal must a distribution be in order for nonparametric tests to be preferred? Blair and Higgins Blair and Higgins (1980) gave insight into this question. They mixed data from two normal distributions, 95 percent from one normal distribution and 5 percent from a second normal distribution with quite different mean and standard deviation. Such a situation could easily be envisioned when data result from low to moderate discharges with occasional storm events, or from a series of wells where 5 percent are affected by a contaminant plume, etc. A difference of 5 percent from truly normal may not be detectable by a graph or test for normality. Yet when comparing two groups of this type, they found that the rank-sum test exhibited large advantages in power over the t-test. As a result, data groups correctly discerned as different by the rank-sum test could be found “not significantly different” by the t-test. Their paper is recommended for further detail and study. The greatest strengths of parametric procedures are in modeling and estimation, such as performed with regression. Relationships among multiple variables can be described and tested which are difficult, if not nearly impossible, with nonparametric methods. Statistical practice has historically been dominated by parametric procedures, due largely to their computational elegance. Transformations are sometimes used to make data more normally distributed, prior to performing a parametric test. There is no guarantee that a given transformation, such as taking logarithms, will produce data sufficiently close to a normal distribution. Often several attempts to find a suitable transformation are required before the data appear approximately normal. The primary pitfall in using transformations is that when two or more groups are to be compared, no single transformation may provide nearly-normal data simultaneously for all groups. Groups whose right-skewness was solved by transformation may be offset by relatively symmetric groups which are now left-skewed. When several tests are performed, such as trend tests at numerous locations, parametric tests might be appropriate in some cases but not in others. Comparisons of results across sites are more difficult when test procedures and/or transformations vary for each case. Nonparametric tests allow the freedom to use the identical test procedure in all cases, without the requirement that the many individual data sets follow the same distribution. Finally, transformations may produce nearly-symmetric data, but cannot compensate for a heavy-tailed distribution – the presence of more data near the extremes than found in a normal distribution. It should be noted that there are actually three versions of most nonparametric tests: 1. Exact test. Exact versions of nonparametric tests provide results (in the form of p-values, defined soon) which are exactly correct . They are computed by comparing the test statistic to a table of quantiles that is specific for the sample sizes present. Therefore an extensive set of tables is required, one for every possible combination of sample sizes. When sample sizes are small, only the exact version will provide accurate results. 2. Large sample approximation. To avoid the necessity for large books filled with tables of test statistic quantiles, approximate p-values are obtained by assuming that the distribution of the test statistic can be approximated by some common distribution, such as the normal. This does not mean the data themselves follow that distribution, but only that the test statistic does. For large sample sizes (30 or more observations per group, but sometimes less) this approximation is very accurate. The test statistic is modified if necessary (often standardized by subtracting its mean, and dividing by its standard deviation), and then compared to a table of the common distribution to determine the p-value. WARNING: Computer software predominantly uses large sample approximations when reporting p-values, whether or not the sample sizes are sufficient to warrant using them. For small sample sizes, p-values should be taken from exact tables rather than from the computer printout. 3. Rank transformation test. In this approximation, parametric procedures are computed not on the data themselves, but on the ranks of the data (smallest observation has rank=1, largest has rank=N). Conover and Iman Conover and Iman (1981) have shown this to adequately approximate many exact nonparametric tests for large samples sizes. The rank-sum test would be approximated in this fashion by computing a t-test on joint ranks of the data. In fact, Iman and Conover Iman and Conover (1983) use the name “rank-sum test” for just this procedure. We would call this version a “t-test on ranks”, reserving the traditional name for the first or second versions of the test and more accurately describing what was done. Rank approximations are most useful when performing nonparametric tests using statistics packages which contain only parametric procedures. They are also very useful for situations where there is no equivalent nonparametric analog, such as for multiple-factor analysis of variance. In figure 4.2, exact and rank transform tests are aligned with their parametric counterparts, as a guide to the use of hypothesis tests. 4.2.2 Establish the Null and Alternate Hypotheses The null and alternate hypotheses should be established prior to collecting data. These hypotheses are a concise summary of the study objectives, and will keep those objectives in focus during data collection. The null hypothesis (H0) is what is assumed to be true about the system under study prior to data collection, until indicated otherwise. It usually states the “null” situation – no difference between groups, no relation between variables. One may “suspect”, “hope”, or “root for” either the null or alternate hypothesis, depending on one’s vantage point. But the null hypothesis is what is assumed true until the data indicate that it is likely to be false. For example, an engineer may test the hypothesis that wells upgradient and downgradient of a hazardous waste site have the same concentrations of some contaminant. They may “hope” that downgradient concentrations are higher (the company gets a new remediation project), or that they are the same (the company did the original site design!). In either case, the null hypothesis assumed to be true is the same: concentrations are similar in both groups of wells. The alternate hypothesis (H1) is the situation anticipated to be true if the evidence (the data) show that the null hypothesis is unlikely. It is in some cases just the negation of H0, such as “the 100-year flood is not equal to the design value.” H1 may also be more specific than just the negation of H0 – “the 100-year flood is greater than the design value.” Alternate hypotheses come in two general types: one-sided, and two-sided. Their associated hypothesis tests are called one-sided and two-sided tests. These are often confused and misapplied. Two-sided tests occur when evidence in either direction from the null hypothesis (larger or smaller, positive or negative) would cause the null hypothesis to be rejected in favor of the alternate hypothesis. For example, if evidence that “the 100-year flood is smaller than the design value” or “the 100-year flood is greater than the design value” would both cause doubt about the null hypothesis, the test is two-sided. Most tests in water resources are of this kind. Figure 4.2: Guide to the classification of some hypothesis tests One-sided tests occur when departures in only one direction from the null hypothesis would cause the null hypothesis to be rejected in favor of the alternate hypothesis. With one-sided tests, it is considered supporting evidence for H0 should the data indicate differences opposite in direction to the alternate hypothesis. For example, suppose only evidence that the 100-year flood is greater than the previous design value is of interest, as only then must the culvert be replaced. The null hypothesis would be stated as “the 100-year flood is less-than or equal to the design flood”, while the alternate hypothesis is that “the 100-year flood exceeds the design value.” Any evidence that the 100-year flood is smaller than the design value is considered evidence for H0. If it cannot be stated prior to looking at any data that departures from H0 in only one direction are of interest, a two-sided test should be performed. If one simply wants to look for differences between two streams or two aquifers or two time periods, then a two-sided test is appropriate. It is not appropriate to look at the data, find that group A is considerably larger in value than group B, and perform a one-sided test that group A is larger. This would be ignoring the real possibility that had group B been larger there would have been interest in that situation as well. Examples in water resources where one-sided tests would be appropriate are: 1. testing for decreased annual floods or downstream sediment loads after completion of a flood-control dam, 2. testing for decreased nutrient loads or concentrations due to a new sewage treatment plant or best management practice, 3. testing for an increase in concentration when comparing a suspected contaminated site to an upstream or upgradient control site. 4.2.3 Decide on an Acceptable Error Rate α The α-value, or significance level, is the probability of incorrectly rejecting the null hypothesis (rejecting H0 when it is in fact true, called a “Type I error”). Figure 4.3 shows that this is one of four possible outcomes of an hypothesis test. The significance level is the risk of a Type I error deemed acceptable by the decision maker. It is a “management tool” dependent not on the data, but on the objectives of the study. Statistical tradition uses a default of 5% (0.05) for α, but there is no reason why other values should not be used. Suppose that an expensive cleanup process will be mandated if the null hypothesis of “no contamination” is rejected, for example. The α-level for this test might be set very small (such as 1%) in order to minimize the chance of needless cleanup costs. On the other hand, suppose the test was simply a first cut at classifying sites into “high” and “low” values prior to further analysis of the “high” sites. In this case the α - level might be set to 0.10 or 0.20, so that all sites with high values would likely be retained for further study. Figure 4.3: Four possible results of hypothesis testing. Since α represents one type of error, why not keep it as small as possible? One way to do this would be to never reject H0 – α would then equal zero. Unfortunately this would lead to large errors of a second type – failing to reject H0 when it was in fact false. This second type of error is called a Type II error, or lack of power (Fig. 4.3). Both errors are of concern to practitioners, and both will have some finite probability of occurrence unless decisions to “always reject” or “never reject” are made. Once a decision is made as to an acceptable Type I risk α, two steps can be taken to concurrently reduce the risk of Type II error β: 1. Increase the sample size n. 2. Use the test procedure with the greatest power for the type of data being analyzed. For water quality applications, null hypotheses are usually of “no contamination”. Situations with low power mean that actual contamination may not be detected. This happens with simplistic formulas for determining sample sizes Kupper and Hafner (1989). Instead, probabilities of Type II errors should be considered when setting sample size. Power is also sacrificed when data having the characteristics outlined in Chapter 1 are analyzed with tests requiring a normal distribution. Power loss increases as skewness and the number of outliers increase. 4.2.4 Compute the Test Statistic from the Data Test statistics summarize the information contained in the data. If the test statistic is not unusually different from what is expected to occur if the null hypothesis is true, the null hypothesis is not rejected. However, if the test statistic is a value unlikely to occur when H0 is true, the null hypothesis is rejected. The p-value measures how unlikely the test statistic is when H0 is true. 4.2.5 Compute the p-Value The p-value is the probability of obtaining the computed test statistic, or one even less likely, when the null hypothesis is true. It is derived from the data, concisely expressing the evidence against the null hypothesis contained in the data. It measures the “believability” of the null hypothesis. The smaller the p-value, the less likely is the observed test statistic when H0 is true, and the stronger the evidence for rejection of the null hypothesis. The p-value is also called the “attained significance level”, the significance level attained by the data. How do p-values differ from α levels? The α-level does not depend on the data, but states the risk of making a Type I error that is acceptable a priori to the scientist or manager. The α-level is the critical value which allows a “yes/no” decision to be made – the treatment plant has improved water quality, nitrate concentrations in the well exceed standards, etc.. The p-value provides more information – the strength of the scientific evidence. Reporting the p-value allows someone with a different risk tolerance (different α) to make their own yes/no decision. For example, consider a test of whether upgradient and downgradient wells have the same expected contaminant concentrations. If downgradient wells show evidence of higher concentrations, some form of remediation will be required. Data are collected, and a test statistic calculated. A decision to reject at α = 0.01 is a statement that “remediation is warranted as long as there is less than a 1 percent chance that the observed data would occur when upgradient and downgradient wells actually had the same concentration.” This level of risk was settled on as acceptable, so that 1 percent of the time remediation would be performed when in fact it is not required. Reporting only “reject” or “not reject” would prevent the audience from distinguishing a case that is barely able to reject (p = 0.009) from one in which H0 is virtually certain to be untrue (p = 0.0001). Reporting a p-value of 0.02, for example, would allow a later decision by someone with a greater tolerance of unnecessary cleanup (α = 5 percent, perhaps) to decide for remediation. 4.2.6 Make the Decision to Reject H0 or Not Reject H0 when: p-value &lt; α-level. When the p-value is less than the decision criteria (the α-level), H0 is rejected. When the pvalue is greater than α, H0 is not rejected. The null hypothesis is never “accepted”, or proven to be true. It is assumed to be true until proven otherwise, and is “not rejected” when there is insufficient evidence to do so. 4.3 The Rank-Sum Test as an Example of Hypothesis Testing Suppose that aquifers X and Y are sampled to determine whether the concentrations of a contaminant in the aquifers are similar or different. This is a test for differences in location or central value, and will be covered in detail in Chapter 5. Two samples xi are taken from aquifer X (n = 2), and 5 samples yi from aquifer Y (m = 5) for a total of 7 samples (N = n+m = 7). Also suppose that there is a prior reason to believe that X values tend to be lower than Y values: aquifer X is deeper, and is likely to be uncontaminated. The null hypothesis (H0) and alternative hypothesis (H1) of this one-sided test are as follows: H0: xi and yi are samples from the same distribution, or H0: Prob (xi ≥ yi ) = 0.5. H1: xi is from a distribution which is generally lower that of yi, or H1: Prob (xi ≥ yi ) &lt; 0.5. Remember that with one-sided tests such as this one, data indicating differences opposite in direction to H1 (xi frequently larger than yi) are considered supporting evidence for H0. With one-sided tests we can only be interested in departures from H0 in one direction. Having established the null and alternate hypotheses, an acceptable error rate α must be set. As in a court of law, innocence is assumed (i.e. concentrations are identical) unless evidence is collected to show “beyond a reasonable doubt” that aquifer Y has higher concentrations (i.e. that differences observed are not likely to have occurred by chance alone). The “reasonable doubt” is set by α, the significance level. If the t-test were to be considered as the test procedure, each data group should be tested for normality. However, sample sizes of 2 and 5 are too small for a reliable test of normality. Thus the nonparametric rank-sum test is appropriate. This test procedure entails ranking all 7 values (lowest concentration has rank = 1, highest has rank = 7) and summing the ranks of the 2 values from the population with the smaller sample size (X). This rank-sum is the statistic W used in the exact test. Next, W would be computed and compared to a table of test statistic quantiles to determine the p-value. Where do these tables come from? We will derive the table for sample sizes 2 and 5 as an example. What are the possible values W may take, given that the null hypothesis is true? The collection of all of the possible outcomes of W defines its distribution, and therefore composes the table of rank-sum test statistic quantiles. Shown below are all the possible combinations of ranks of the two x values. 1,2 1,3 1,4 1,5 1,6 1,7 2,3 2,4 2,5 2,6 2,7 3,4 3,5 3,6 3,7 4,5 4,6 4,7 5,6 5,7 6,7 If H0 is true, each of the 21 possible outcomes must be equally likely. That is, it is just as likely for the two x’s to be ranks 1 and 2, or 3 and 5, or 1 and 7, etc. Each one of the outcomes results in a value of W, the sum of the two ranks. The 21 W values corresponding to the above outcomes are 3 4 5 6 7 8 5 6 7 8 9 7 8 9 10 9 10 11 11 12 13 The expected value of W is the mean (and median) of the above values, or 8. Given that each outcome is equally likely when H0 is true, the probability of each possible W value is: W 3 4 5 6 7 8 9 10 11 12 13 Prob(W) 1/21 1/21 2/21 2/21 3/21 3/21 3/21 2/21 2/21 1/21 1/21 What if the data collected produced 2 x values having ranks 1 and 4? Then W would be 5, lower than the expected value E [W] = 8. If H1 were true rather than H0, W would tend toward low values. What is the probability that W would be as low as 5 or lower if H0 were true? It is the sum of the probabilities for W = 3, 4, and 5, or 4/21 = 0.190 (see figure 4.4). This number is the p-value for the test statistic of 5. It says that the chance of a departure from E [W] of at least this magnitude occurring when H0 is true is 0.190, which is not very uncommon (about 1 chance in 5). Thus the evidence against H0 is not too convincing. If the ranks of the 2 x values had been 1 and 2, then W = 3 and the p-value would be 1/21 = 0.048. This result is much less likely than the previous case but is still not extremely rare. In fact, due to such a small sample size the test can never result in a highly compelling case for rejecting H0. Adding more data would make it possible to attain lower p-values, providing a stronger case against H0. Figure 4.4: Probabilities of occurrence for a rank-sum test with sample sizes of 2 and 5.The p-value for a one-sided test equals the area shaded. This example has considered only the one-sided p-value, which is appropriate when there is some prior notion that x should be smaller than y (or the reverse). Quite often the situation is that there is no prior notion of which should be lower. In this case a two-sided test must be done. The two-sided test has the same null hypothesis as was stated above, but H1 is now that xi and yi are from different distributions, or \\(H1: Prob (x_i \\geq y_i ) \\neq 0.5.\\) Suppose that W for the two-sided test were found to be 5. The p-value equals the probability that W will differ from E [W] by this much or more, in either direction. It is \\(Prob (W \\leq 5) + Prob (W \\geq 11).\\) (see figure 4.5) Where did the 11 come from? It is just as far from E [W] = 8 as is 5. The two-sided p-value therefore equals 8/21 = 0.381, twice the one-sided p-value. Symbolically we could state: \\(Prob (|W− E [W] |\\geq 3) = 8/21.\\) To summarize the subject of p-values: they describe how “far” the observed test statistic is from that expected to occur if the null hypothesis were true. They are the probability of being that far or farther given that the null hypothesis is true. The lower the p-value the stronger is the case against the null hypothesis. Now, lets look at an α-level approach. Return to the original problem, the case of a one-sided test. Assume α is set equal to 0.1. This corresponds to a critical value for W, call it \\(W^*\\), such that \\(Prob (W ≤ W^*) = α. Whenever W≤W^*\\), H0 is rejected with no more than a 0.1 frequency of error if H0 were always true. However, because W can only take on discrete, in fact integer, values as seen above, a W* which exactly satisfies the equation is not usually available. Instead the largest possible W* such that \\(Prob (W ≤ W^*) ≤ α\\) is used. Figure 4.5: Probabilities of occurrence for a rank-sum test with sample sizes of 2 and 5.The p-value for a one-sided test equals the area shaded. Searching the above table of possible W values and their probabilities, \\(W^*\\) = 4 because Prob (W ≤ 4) = 0.095 ≤ 0.1. Note the “lumpiness” of the relationship between α and \\(W^*\\). If α =0.09 had been selected then \\(W^*\\) would be 3. This lumpiness can be avoided by reporting p-values rather than only “reject” or “not reject”. For a two-sided test a pair of critical values \\(W_U*\\) and \\(W_L^*\\) is needed, where \\(Prob (W ≤ W_L^*) + Prob (W ≥ W_U*) ≤ α \\ \\ \\ and \\ \\ \\ \\ W_U*− E [W] = E [W] − W_L*.\\) These upper and lower critical values of W are symmetrical around E [W] such that the probability of W falling on or outside of these critical levels is as close as possible to α, without exceeding it, under the assumption that H0 is true. In the case at hand, if α = 0.1, then \\(WL^*= 3 \\ \\ \\ and \\ \\ \\ \\ WU^*= 13\\) because Prob (W ≤ 3) + Prob (W ≥ 13) = 0.048 + 0.048 = 0.095 ≤ 0.1. Note that for a two-sided test, the critical values are farther from the expected value than in a one-sided test at the same α level. It should be recognized that p-values are also influenced by sample size. For a given magnitude of difference between the x and y data, and a given amount of variability in the data, p values will tend to be smaller when the sample size is large. In the extreme case where vast amounts of data are available, it is a virtual certainty that p values will be small even if the differences between x and y are what might be called “of no practical significance.” Most statistical tables are set up for one-sided tests. That is, the rejection region α or the pvalue is given in only one direction. When a two-sided test at significance level α is performed, the tables must be entered using α/2. In this way rejection can occur with a probability of α/2 on either side, and an overall probability of α. Similarly, tabled p-values must be doubled to get p-values for a two-sided test. Modern statistical software often reports p-values with its output, eliminating the need for tables. Be sure to know whether it is one-sided or two-sided p-values being reported. 4.4 Tests for Normality The primary reason to test whether data follow a normal distribution is to determine if parametric test procedures may be employed. The null hypothesis for all tests of normality is that the data are normally distributed. Rejection of H0 says that this is doubtful. Failure to reject H0, however, does not prove that the data do follow a normal distribution, especially for small sample sizes. It simply says normality cannot be rejected with the evidence at hand. Use of a larger α-level (say 0.1) will increase the power to detect non-normality, especially for small sample sizes, and is recommended when testing for normality. The test for normality used in this book is the probability plot correlation coefficient (PPCC) test discussed by Looney and Gulledge Looney and Gulledge Jr (1985b). Remember from Chapter 2 that the more normal a data set is, the closer it plots to a straight line on a normal probability plot. To test for normality, this linearity is tested by computing the linear correlation coefficient between data and their normal quantiles (or “normal scores”, the linear scale on a probability plot). Samples from a normal distribution will have a correlation coefficient very close to 1.0. As data depart from normality, their correlation coefficient will decrease below 1. To perform a test of H0: the data are normal versus H1: they are not, the correlation coefficient (r) between the data and their normal quantiles is tested to see if it is significantly less than 1. For a sample size of n, if r is smaller than the critical value r* of table B3 for the desired α-level, reject H0. Looney and Gulledge Looney and Gulledge Jr (1985a) have shown this table, developed using the Blom plotting position, is also valid for other plotting positions except the Weibull position i/(n+1). In order to use one plotting position for all functions in this book, the Cunnane plotting position was adopted as explained in Chapter 2. To illustrate this test, probability plots of the unit well yield data from Chapter 2 are shown in figures 4.6 and 4.7. For the valleys without fracturing, r = 0.805, the correlation coefficient between yi and Zp in the left-hand side of Table 4.1. From table B3 with n=12, if r is below the α = 0.05 critical value of r* = .928, normality is rejected. Therefore normality is rejected for the yields without fractures at α = 0.05. A p-value for this test would be &lt;0.005, as r=0.805 is less than the tabled r* of 0.876 for α=0.005. Note the nonlinearity of the data on the probability plot (figure 4.6). For the yields with fracturing, n=13, r* is 0.932 at α = 0.05, and the PPCC r = 0.943; therefore fail to reject normality at α=0.05. The p-value for the yields with fracturing is just under 0.10 (normality would barely be rejected at α=0.10). The probability plot, figure 4.7, shows a closer adherence to a straight line than for figure 4.6. Table 4.1. Unit well yields (in gal/min/ft) in Virginia Wright (1985) Computer packages use several methods for testing normality. Several are based on probability plots. The most common is perhaps the Shapiro-Wilk test, as its power to detect non-normality is as good or better than other tests Shapiro, Wilk, and Chen (1968). A table of quantiles for this test statistic is available for n &lt; 50 William Jay Conover and Conover (1980). Shapiro and Francia Shapiro and Francia (1972) developed a modification of the Shapiro-Wilk test useful for all sample sizes. It is essentially identical to the PPCC test, as it is the \\(r^2\\) for a regression between the data and their normal scores. Therefore pvalues and power characteristics for the two tests should be essentially the same. Tests for normality not related to probability plots include the Kolmogorov and chi-square tests, described in more detail by Conover William Jay Conover and Conover (1980). Both are general tests that may be used for data which are ordinal (data recorded only as low/medium/high, etc) but do not possess a continuous scale. This makes them less powerful than the probability plot tests, however, for the specific purpose of testing continuous data for normality Shapiro, Wilk, and Chen (1968). The important advantage of the PPCC test is its graphical analog, the probability plot, which visually illustrates its results. The probability plot itself provides information on how the data depart from normality, something not provided by any test statistic. To make the PPCC test easy to perform by hand, normal quantiles for the Cunnane plotting positions of table B1 are listed in table B2 of the Appendix. For the n=12 yields without fracturing, for example, the upper six quantiles are easily found in the table. Lower quantiles are mirror images around zero of the upper quantiles, and so equal the upper values multiplied by −1. Table B2 quantiles were computed by first calculating the Cunnane plotting position to more significant digits than found in table B1, and then looking up the corresponding normal quantiles in a table of the normal distribution. Figure 4.6: Probability plot for the yields without fracturing, with PPCC r Figure 4.7: Probability plot for the yields with fracturing, with PPCC r Exercises 4.1 The following are annual streamflows for the Green R. at Munfordville, KY. Beginning in 1969 the stream was regulated by a reservoir. before after 1950 4910 1960 2340 1969 1350 1951 3660 1961 2600 1970 2350 1952 3910 1962 3410 1971 3140 1953 1750 1963 1870 1972 3060 1954 1050 1964 1730 1973 3630 1955 2670 1965 2730 1974 3890 1956 2880 1966 1550 1975 3780 1957 2600 1967 4060 1976 3180 1958 3520 1968 2870 1977 2260 1959 1730 1978 3430 1979 5290 1980 2870 Test both before and after data sets for normality using the PPCC test. If either are nonnormal, transform the data and re-test in order to find a scale which appears to be close to a normal distribution. library(&quot;ggplot2&quot;) library(&quot;ppcc&quot;) number_ticks &lt;- function(n) {function(limits) pretty(limits, n)} ## Before before = data.frame(year = 1950:1968, flows = c(4910, 3660, 3910, 1750, 1050, 2670, 2880, 2600, 3520, 1730, 2340, 2600, 3410, 1870, 1730, 2730, 1550, 4060, 2870)) ggplot(before, aes(x = flows)) + geom_density(color=&quot;blue&quot;, alpha=0.8)+ labs(x =&quot;Annual streamflows (1950-1968)&quot;)+ scale_x_continuous(breaks=number_ticks(10)) + scale_y_continuous(breaks=number_ticks(10))+ theme_bw() ## After after = data.frame(year = 1969:1980, flows = c(1350, 2350, 3140, 3060, 3630, 3890, 3780, 3180, 2260, 3430, 5290, 2870)) ggplot(after, aes(x = flows)) + geom_density(color=&quot;Red&quot;, alpha=0.8)+ labs(x =&quot;Annual streamflows (1968-1980)&quot;)+ scale_x_continuous(breaks=number_ticks(10)) + scale_y_continuous(breaks=number_ticks(10))+ theme_bw() ## PPCC test ppccTest(before$flows) ## ## Probability Plot Correlation Coefficient Test ## ## data: before$flows ## ppcc = 0.98556, n = 19, p-value = 0.7182 ## alternative hypothesis: before$flows differs from a Normal distribution ppccTest(after$flows) ## ## Probability Plot Correlation Coefficient Test ## ## data: after$flows ## ppcc = 0.97046, n = 12, p-value = 0.44 ## alternative hypothesis: after$flows differs from a Normal distribution PPCC test analysis- The PPCC test for both before and after datasets shows normality because p value for both are found significant and datasets are showing normal distribution. 4.2 Test the arsenic data and transformed data of Exercise 2.2 for normality. ars_data = c(1.3, 1.5, 1.8, 2.6, 2.8, 3.5, 4.0, 4.8, 8, 9.5, 12, 14, 19, 23, 41, 80, 100, 110, 120, 190, 240, 250, 300, 340, 580) ggplot() + geom_density(aes(x = ars_data), color = &#39;blue&#39;)+ labs(x =&quot;Arsenic data&quot;)+ theme_bw() ##Normality test for normal data shapiro.test(ars_data) ## ## Shapiro-Wilk normality test ## ## data: ars_data ## W = 0.71947, p-value = 1.348e-05 ##Transformation of data trans_data = (mean(ars_data) - ars_data)/sd(ars_data) trans_data ## [1] 0.67078106 0.66939875 0.66732528 0.66179603 0.66041371 0.65557562 ## [7] 0.65211984 0.64659059 0.62447358 0.61410624 0.59682733 0.58300420 ## [13] 0.54844638 0.52080013 0.39639199 0.12684101 -0.01139026 -0.08050589 ## [19] -0.14962153 -0.63343097 -0.97900914 -1.04812478 -1.39370295 -1.67016549 ## [25] -3.32894072 ggplot() + geom_density(aes(x = trans_data), color = &#39;Red&#39;)+ labs(x =&quot;Transform arsenic data&quot;)+ scale_x_continuous(limits = c(-3.8, 3.8))+ theme_bw() ##Normality test for transformed data shapiro.test(trans_data) ## ## Shapiro-Wilk normality test ## ## data: trans_data ## W = 0.71947, p-value = 1.348e-05 For the both the datasets, p value is not significant, that representing a strong confirmations for non-normality in data. References "],
["ch5.html", "Chapter 5 Differences between Two Independent Groups 5.1 The Rank-Sum Test 5.2 The t-Test 5.3 Graphical Presentation of Results 5.4 Estimating the Magnitude of Differences Between Two Groups Exercises", " Chapter 5 Differences between Two Independent Groups Wells upgradient and downgradient of a hazardous waste site are sampled to determine whether the concentrations of some toxic organic compound known to reside in drums at the site are greater in the downgradient wells. Are they greater at the α = 0.01 significance level? If so, the ground water is declared to be contaminated, and the site will need to be cleaned up. Measurements of a biological diversity index are made on sixteen streams. Eight of the streams represent “natural” conditions, while the other eight have received urban runoff. Is the biological quality of the urban streams worse than that of the “natural” streams? Unit well yields are determined for a series of bedrock wells in the Piedmont region. Some wells tap areas where fracturing is prevalent, while other wells are drilled in largely unfractured rock. Does fracturing affect well yields, and if so how? These are examples of comparisons of two independent groups of data, to determine if one group tends to contain larger values than the other. The data are independent in the sense that there is no natural structure in the order of observations across groups – there are no pairings of data between observation 1 of group 1 and observation 1 of group 2, etc. Where such a pairing does exist, methods like those of Chapter 6 should be used. In some cases it is known ahead of time which group is expected to be larger (a one-sided test), and in other cases it is not (a twosided test). This chapter will present and discuss the rank-sum test, a nonparametric procedure for determining whether two independent groups differ. In the special case where the data within each group are known to be normally distributed, and the differences between the groups are additive, the t-test may also be used. Graphical presentations of the test results will be quickly surveyed. Finally, methods for estimating the magnitude of the difference between the two groups are presented, including the Hodges-Lehmann estimator, one of a class of efficient and resistant nonparametric estimators unfamiliar to many water resources scientists. 5.1 The Rank-Sum Test The rank-sum test goes by many names. It was developed by Wilcoxon Wilcoxon (1945), and so is sometimes called the Wilcoxon rank-sum test. It is equivalent to a test developed by Mann and Whitney near the same time period, and the test statistics can be derived one from the other. Thus the Mann-Whitney test is another name for the same test. The combined name of Wilcoxon-Mann-Whitney rank-sum test has also been used. 5.1.1 Null and Alternate Hypotheses In its most general form, the rank-sum test is a test for whether one group tends to produce larger observations than the second group. It has as its null hypothesis: H0: Prob [x &gt; y] = 0.5 where the x are data from one group, and the y are from a second group. In words, this states that the probability of an x value being higher than any given y value is one-half. The alternative hypothesis is one of three statements: H1: Prob [x &gt; y] ≠ 0.5 (2-sided test -- x might be larger or smaller than y). H2: Prob [x &gt; y] &gt; 0.5 (1-sided test -- x is expected to be larger than y) H3: Prob [x &gt; y] &lt; 0.5 (1-sided test-- x is expected to be smaller than y). Note that no assumptions are made about how the data are distributed in either group. They may be normal, lognormal, exponential, or any other distribution, They may be uni-, bi- or multi-modal. In fact, if the only interest in the data is to determine whether one group tends to produce higher observations, the two groups do not even need to have the same distribution! Usually however, the test is used for a more specific purpose – to determine whether the two groups come from the same population (same median and other percentiles), or alternatively whether they differ only in location (central value or median). If both groups of data are from the same population, about half of the time an observation from either group could be expected to be higher than that from the other, so the above null hypothesis applies. However, now it must be assumed that if the alternative hypothesis is true, the two groups differ only in their central value, though not necessarily in the units being used. For example, suppose the data are shaped like the two lognormal distributions of figure 5.1. In the original units, the data have different sample medians and interquartile ranges, as shown by the two boxplots. A rank-sum test performed on these data has a p-value of &lt;0.001, leading to the conclusion that they do indeed differ. But is this test invalid because the variability, and therefore the shape, of the two distributions differs? Changing units by taking logs, the boxplots of figure 5.2 result. The logs of the data appear to have different medians, but similar IQR’s, and thus the logs of the data appear to differ only in central location. The test statistic and p-value for a rank-sum test computed on these transformed data is identical to that for the original units! Nonparametric tests possess the very useful property of being invariant to power transformations such as those of the ladder of powers. Since only the data or any power transformation of the data need be similar except for their central location in order to use the rank-sum test, it is applicable in many situations. Figure 5.1: Boxplots of two lognormal distributions with different medians and IQRs. Figure 5.2: Boxplots of the logarithms of the figure 5.1 data. Medians still differ, while IQRs are the same. 5.1.2 Computation of the Exact Test The exact form of the rank-sum test is given below. It is the only form appropriate for comparing groups of sample size 10 or smaller per group. When both groups have samples sizes greater than 10 (n, m &gt; 10), the large-sample approximation may be used. Remember that computer packages report p-values from the large sample approximation regardless of sample size. Exact Version of the Rank-Sum test! Situation Two independent groups of data are to be compared. The sample size for the smaller of the two groups xi, i=1,…n is designated n, while the larger sample size yj, j=1,…m is designated m. Test Statistic Compute the joint ranks Rk. Rk = 1 to (N = n + m), using average ranks in case of ties. The exact test statistic \\(W_{rs}\\) = sum of ranks for the group having the smaller sample size, = \\(ΣR_i\\) i=1,n (use either group when sample sizes are equal: n = m) Decision Rule. To reject H0 : Prob [x &gt; y] = 0.5 H1 : Prob [x &gt; y] ≠ 0.5 (the smaller data set tends to have either higher or lower values than the larger data set) Reject H0 if \\(W_{rs} ≤ x^*_{α/2,n,m}\\) or \\(W_{rs} ≥ x_{α/2,n,m}\\) from Table B4 of the Appendix; otherwise do not reject H0. H2 : Prob [x &gt; y] &gt; 0.5 (the smaller data set tends to have higher values than the larger dataset) Reject H0 if \\(W_{rs} ≥ x_{α,n,m}\\) from Table B4; otherwise do not reject H0. H3 : Prob [x &gt; y] &lt; 0.5 (the smaller data set tends to have lower values than the larger data set) Reject H0 if \\(W_{rs} ≤ x^*_{α,n,m}\\) from Table B4; otherwise do not reject H0. Example 1. Precipitation quality was compared at sites with different land uses by Oltmann and Shulters Oltmann and Shulters (1989). A rank-sum test is used to determine if one of the constituents, ammonia plus organic nitrogen, significantly differs (α = 0.05) between the industrial and residential sites. H0 : median concentration (industrial) = median concentration (residential) H3 : median concentration (industrial) ≠ median concentration (residential). The 10 observations at each site are assigned ranks from 1 to 20 as follows. Note that three pairs of concentrations (at 0.7, 1.1, and 1.3 mg/L) are tied, and so are assigned tied ranks equal to the average of their two individual ranks: Ammonia plus organic nitrogen concentration (in mg/L) in precipitation Wrs = sum of the 10 ranks for the residential site (n=m=10, so either could be used) = 78.5 For this two-sided test, reject H0 if \\(W_{rs} ≤ x^*_{α/2,n,m}\\) or \\(W_rs ≥ x_{α/2,n,m}\\). From Table B4, \\(x^*_{.026,10,10} = 79\\) and \\(x^*_{.022,10,10} = 78\\). Interpolating halfway between these for Wrs = 78.5, the p-value for the two-sided test is 0.024•2 = 0.048, and the decision would be to reject H0 at α = 0.05. Reporting the p-value shows how very close the risk of Type I error is to 0.05. The conclusion is therefore that ammonia plus organic nitrogen concentrations from industrial precipitation are significantly different than those in residential precipitation at a p-value of 0.048. 5.1.3 The Large Sample Approximation For the rank sum test, the distribution of the test statistic Wrs closely approximates a normal distribution when the sample size for each group is 10 or above figure 5.3. With n=m=10, there are 184,756 possible arrangements of the data ranks. The collection of test statistics for each of these comprises the exact distribution of Wrs, shown as bars in figure 5.3, with a mean of 105. Superimposed on the exact distribution is the normal distribution which closely approximates the exact values. This demonstrates how well the exact distribution of this test can be approximated, even for relatively small sample sizes. The inset shows a magnified view of the peak of the distribution, with the normal approximation crossing the center of the exact distribution bars. This approximation does not imply that the data are or must be normally distributed. Rather, it is based on the near normality of the test statistic at large sample sizes. If there are no ties, Wrs has a mean μW and standard deviation σW when H0 is true of: \\(\\mu_W = n•(N+1)/2\\) 5.1 \\(\\sigma_W = \\sqrt{n•m•(N+1)/12}\\) 5.2 where N = n + m. Figure 5.3: Illustration of the distribution of Wrs and its fitted normal distribution. The test statistic for the large sample approximation is computed by standardizing Wrs and making a continuity correction. The continuity correction occurs because the normal distribution fits halfway through the top of the bars of the exact test statistic distribution figure 5.3. The correction moves the probability of occurrence from the outer edge of each bar to its center prior to using the normal curve. It therefore equals d/2, where d is the minimum difference between possible values of the test statistic (the bar width). For the rank-sum test d=1, as the test statistic values change by units of one. Zrs, the standardized form of the test statistic, is therefore computed as \\[ \\begin{equation} Z_{rs} = \\begin{cases} \\frac{W_{rs}-d/2-m_{W}}{s_{W}} &amp;\\text{if } W_{rs} &gt; 0 \\\\\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ {0} &amp;\\text{if } W_{rs}=m_W \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ [5.3]\\\\\\\\ \\frac{W_{rs}+d/2-m_W}{s_W} &amp; \\text{if } W_{rs} &lt; m_W \\end{cases} \\end{equation} \\] Zrs is compared to a table of the standard normal distribution for evaluation of the test results. Example 1, cont. The large-sample approximation is applied to the precipitation nitrogen data. Note that this is inappropriate because there are three pairs of tied values. How close is the approximate to the exact p-value? For the exact test above, Wrs = 78.5. \\(\\mu_W = 10(21)/2 = 105 \\ \\ \\ \\ \\ \\ \\ \\ \\sigma_W = \\sqrt{10\\ \\ 10 (21)/12} = 13.23\\) Therefore $ Zrs = = 1.965 $ and p ≅ 2•0.025 = 0.05 from a table of the normal distribution such as Table A2 of Iman and Conover Iman and Conover (1983). This is very close to the exact test results, and errors decrease with increasing sample sizes. 5.1.4 Correction for ties Conover William Jay Conover and Conover (1980) presents a further correction to σW when ties occur, and tied ranks are assigned. The formula below for σWt should be used for computing the large sample approximation rather than σW when more than a few ties occur. \\[ \\sigma_{Wt} = \\sqrt{ \\frac{nm}{N(N-1)} \\sum_{k=1}^{N} R_{k^2} \\ \\frac{nm(N+1)^2}{4(N-1)}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{where N = n+m [5.4]}\\] Example 1, cont. The tie correction is applied to the large sample approximation for the precipitation nitrogen data. \\[ \\sigma_{Wt} = \\sqrt{ \\frac{100}{2019} 2868.5 \\ \\frac{100(21)^2}{419}} = \\sqrt{174.61} = 13.21.\\] 5.1.5 The Rank Transform Approximation Another approximation to the exact rank-sum test is to compute the equivalent parametric test, in this case the t-test, on the ranks Rk rather than on the original data themselves. Computations will be illustrated in detail following the presentation of the t-test in the next section. The rank-transform p-value calculated in that section for the precipitation nitrogen data is 0.042, close to but lower than the exact value, and not as close as the large sample approximation. Rank transform approximations are not as widely accepted as are the large sample approximations. This is due to the fact that the rank transform approximations can result in a lower p-value than the exact test, while the large sample approximation will not. In addition, the rank approximation is often not as close as the large-sample approximation for the same sample size. Statisticians prefer that an approximation never result in a lower p-value than the exact test, as this means that H0 will be rejected more frequently than it should. However, this problem only occurs for small sample sizes. For the sample sizes (conservatively, n and m both larger than 25) at which the rank approximation should be used, it should perform well. 5.2 The t-Test The t-test is perhaps the most widely used method for comparing two independent groups of data. It is familiar to most water resources scientists. However, there are five often overlooked problems with the t-test that make it less applicable for general use than the nonparametric ranksum test. These are 1) lack of power when applied to non-normal data, 2) dependence on an additive model, 3) lack of applicability for censored data, 4) assumption that the mean is a good measure of central tendency for skewed data, and 5) difficulty in detecting non-normality and inequality of variance for the small sample sizes common to water resources data. These problems were discussed in detail by Helsel and Hirsch Helsel and Cohn (1988), and will be evaluated here in regard to the precipitation nitrogen data. 5.2.1 Assumptions of the Test The t-test assumes that both groups of data are normally distributed around their respective means, and that they have the same variance. The two groups therefore are assumed to have identical distributions which differ only in their central location (mean). Therefore the t-test is a test for differences in central location only, and assumes that there is an additive difference between the two means, if any difference exists. These are strong assumptions rarely satisfied with water resources data. The null hypothesis is stated as \\[ H0 \\colon \\mu_x = \\mu_y \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{the means for groups x and y are identical.}\\] 5.2.2 Computation of the t-Test Two Sample t-test Situation Two independent groups of data are to be compared. Each group is normally distributed around its respective mean value, and the two groups have the same variance. The sole difference between the groups is that their means may not be the same. Test Statistic Compute the t-statistic: \\[ t = \\frac{\\bar{x} - \\bar{y}}{\\sqrt[s]{1/n+1/m}}\\] where \\(\\bar{x}\\) is the sample mean of data in the first group xi i=1,n \\(\\bar{y}\\) is the sample mean of data in the second group yj j=1, m and s is the pooled sample standard deviation, estimating the standard deviation assumed identical in both groups: \\[ s = \\sqrt{ \\frac{(n −1)s^2_x + (m −1)s^2_y}{n+m-2}}\\] The sample variances of both groups \\(s^2_x\\) and \\(s^2_y\\) are used to estimate s. Decision Rule. To reject \\(H0 \\colon \\mu_x = \\mu_y\\) \\(H1 : \\mu_x \\neq \\mu_y\\) (the two groups have different mean values, but there is no prior knowledge which of x or y might be higher) Reject H0 if \\(t \\lt −t_{α/2,(n+m−2)}\\) or \\(t \\gt t_{α/2,(n+m−2)}\\) from a table of the t distribution; otherwise do not reject H0. \\(H2 : \\mu_x \\gt \\mu_y\\) (prior to seeing any data, x is expected to be greater than y) Reject H0 if \\(t \\gt t _{α,(n+m−2)}\\) from a table of the t distribution; otherwise do not reject H0. \\(H3 : \\mu_x \\lt \\mu_y\\) (prior to seeing any data, y is expected to be greater than x) Reject H0 if \\(t \\lt −t _{α,(n+m−2)}\\) from a table of the t distribution; otherwise do not reject H0. 5.2.3 Modification for Unequal Variances When the two groups have unequal variances the degrees of freedom and test statistic t should be modified using Satterthwaite’s approximation: Two Sample t-test with Unequal Variances Situation The mean values of two independent groups of data are to be tested for similarity. Each group is normally distributed around its respective mean value, and the two groups do not have the same variance. Test Statistic Compute the t-statistic: \\[ t = \\frac{\\bar{x} - \\bar{y}}{\\sqrt[]{s^2_x/n+s^2_y/m}}\\] where \\(s^2_x\\) is the sample variance of the first group, and \\(s^2_y\\) is the sample variance of the second group. Also compute the approximate degrees of freedom df, where \\[ df = \\frac{{(s^2_x/n + s^2_y/m)}}{{\\frac{(s^2_x/n)^2}{(n-1)}+\\frac{(s^2_y/m)^2}{(m-1)}}}\\] Decision Rule. To reject \\(H0 : \\mu_x = \\mu_y\\) \\(H1 : \\mu_x \\neq \\mu_y\\) (the two groups have different mean values, but there is no prior knowledge which of x or y might be higher) Reject H0 if \\(t \\lt −t_{α/2,(df)}\\) or \\(t \\gt t_{α/2,(df)}\\) from a table of the t distribution; otherwise do not reject H0. \\(H2 : \\mu_x \\gt \\mu_y\\) (prior to seeing any data, x is expected to be greater than y) Reject H0 if \\(t \\gt t_{α,(df)}\\) from a table of the t distribution; otherwise do not reject H0. \\(H3 : \\mu_x \\lt \\mu_y\\) (prior to seeing any data, y is expected to be greater than x) Reject H0 if \\(t \\lt −t_{α,(df)}\\) from a table of the t distribution; otherwise do not reject H0. Example 1, cont. The t-test is applied to the precipitation nitrogen data. Are the means of the two groups of data equal? As the variance for the industrial data is 1.2 while for the residential data it is 8.1, Satterthwaite’s approximation is used rather than computing an overall variance: \\[ t = \\frac{{1.67} - {1.64}}{\\sqrt[]{1.17/10+8.12/10}}=0.03, \\ \\ \\ \\ and \\ \\ \\ df = \\frac{{(1.17/10} + {8.12/10)}^2}{{\\frac{(1.17/10)^2}{9}+\\frac{(8.12/10)}{9}}}= 11.5\\] Therefore from a table of the t-distribution, the p-value is 0.98. The conclusion: fail to reject H0. There is essentially no evidence that the means differ using the t-test. The “t-test on ranks” approximation to the rank-sum test is also computed. This t-test is computed using the joint ranks Rk rather than the original data themselves: \\[ t_{rank} = \\frac{{13.15} - {7.85}}{5.4\\sqrt{1/10+1/10}} = 2.19\\] where 13.15 is the mean rank of the x data, etc. Comparing this to \\(t_{.025,18} = 2.10\\), H0 is rejected with a p-value of 0.042. The medians are declared different. 5.2.4 Consequences of Violating the t-Test’s Assumptions Computing the probability plot correlation coefficient to test for normality of the two groups of precipitation nitrogen data, the industrial group had a PPCC of 0.895, while the residential group had a PPCC of 0.66. From Table B3 of the Appendix, both correlation coefficients are below the critical value of 0.918 for an α of 0.05, and so both groups must be considered non-normal (see Chapter 4 for details on the PPCC test). A t-test should not have been used on these data. However, if the normality test results are ignored, the t-test declares the group means to be similar, which is commonly interpreted to mean that the two groups are similar. The rank-sum test finds the two groups to be significantly different. This has the following consequences: This example demonstrates the lack of power encountered when a t-test is applied to non-normal data. When parametric tests are applied to non-normal data, their power to detect differences which are truly present is much lower than that for the equivalent nonparametric test Bradley (1968). Thus the t-test is not capable of discerning the difference between the two groups of precipitation nitrogen. The skewness and outliers in the data inflate the sample standard deviation used in the t-test. The t-test assumes it is operating on normal distributions having this standard deviation, rather than on non-normal data with smaller overall spread. It then fails to detect the differences present. As shown by the Q-Q plot of figure 5.5, these data do not exhibit an additive difference between the data sets. A multiplicative model of the differences is more likely, and logs of the data should be used rather than the original units in a t-test. Of course, this is not of concern to the rank-sum test, as the test results will in either units be identical. A t-test cannot be easily applied to censored data, such as data below the detection limit. That is because the mean and standard deviation of such data cannot be computed without either substituting some arbitrary values, or making a further distributional assumption about the data. This topic is discussed further in Chapter ??. It will only be noted here that all data below a single detection limit can easily be assigned a tied rank, and a rank-sum test computed, without making any distributional assumptions or assigning arbitrary values to the data. The t-test assumes that the mean is a good measure of central tendency for the data being tested. This is certainly not true for skewed data such as the precipitation nitrogen data. The mean of the residential data is greatly inflated by the one large outlier figure 5.4, making it similar to the mean at the industrial site. The mean is neither resistant to outliers, nor near the center (50th percentile) of skewed data. Therefore tests on the mean often make little sense. When prior tests for normality are used to decide whether a nonparametric test is warranted, departures from normality must be large before they are detected for the small sample sizes (n&lt;25 or 30) commonly investigated. In this example, departures were sufficiently drastic that normality was rejected. For lesser departures from normality, computing both the rank sum and t-test would protect against the potential loss of power of the t-test for nonnormal data. Alternatively, just the rank sum test could be used for analysis of small data sets. 5.3 Graphical Presentation of Results In Chapter 2 a detailed discussion of graphical methods for comparisons of two or more groups of data was presented. Overlapping and side-by-side histograms, and dot and line plots of means and standard deviations, inadequately portray the complexities commonly found in water resources data. Probability plots and quantile plots allow complexity to be shown, plotting a point for every observation, but often provide too much detail for a visual summarization of hypothesis test results. Two methods, side-by-side boxplots and Q-Q plots, are very well suited to describing both the results of hypothesis tests, and visually allowing a judgement of whether data fit the assumptions of the test being employed. This is illustrated by the precipitation nitrogen data below. 5.3.1 Side-by-Side Boxplots The best method for illustrating results of the rank-sum test is side-by-side boxplots. With boxplots only a few quantiles are compared, but the loss of detail is compensated for by greater clarity. In figure 5.4 are boxplots of the precipitation nitrogen data. Note the difference in medians is clearly displayed, as well as the similarity in spread (IQR). The rejection of normality by PPCC tests is seen in the presence of skewness (industrial) and an outlier (residential). Sideby-side boxplots are an effective and concise method for illustrating the basic characteristics of data groups, and of differences between those groups. Figure 5.4: Boxplots of the precipitation nitrogen data. Note the skewness and outliers. 5.3.2 Q-Q Plots Another method for illustration of rank-sum results is the quantile-quantile (Q-Q) plot described in Chapter 2. Quantiles from one group are plotted against quantiles of the second data group. Chapter 2 has shown that when sample sizes of the two groups are identical, the x’s and y’s can be ranked separately, and the Q-Q plot is simply a scatterplot of the ordered data pairs (x1 , y1)…..(xn, yn). When sample sizes are not equal (n&lt;m), the quantiles from the smaller data set are used as is, and the n corresponding quantiles for the larger data set are interpolated. It is always helpful in a Q-Q plot comparing two groups to plot the y = x line. Figure 5.5 is a QQ plot of the precipitation nitrogen data. Two important data characteristics are apparent. First, the data are not parallel to the y = x line, and therefore quantiles do not differ by an additive constant. Instead, they increasingly depart from the line of equality indicating a multiplicative relationship. Note that the Q-Q plot shows that a t-test would not be applicable without a transformation, because it assumes an additive difference between the two groups. The rank-sum test does not make this assumption, and is directly applicable to groups differing by a multiplicative constant (rank procedures will not be affected by a power transformation). The magnitude of this relationship between two sets of quantiles on a Q-Q plot can be estimated using the median of all possible ratios \\((y_j/x_i)\\), i=1,n and j=1,n. This is a type of Hodges-Lehmann estimator, as discussed in the next section. The median ratio equals 0.58, and the line residential = 0.58•industrial is drawn in figure 5.5. Note the resistance of the median ratio to the one large outlier. Figure 5.5: Q-Q plot of the precipitation nitrogen data. Second, the data are crowded together at low concentrations while spread further apart at higher concentrations – a pattern indicating right-skewness. To remedy both skewness and nonadditivity, a power transformation with θ &lt; 1 was chosen, the base 10 log transform (θ = 0). A Q-Q plot of data logarithms is shown in figure 5.6. Note that the data are now spread more evenly from low to high concentrations, indicating skewness has decreased. The slope of the quantiles is now parallel to the y = x line. Thus a multiplicative relationship in original units has become an additive relationship in log units, with the Hodges-Lehmann estimate (see next section) of the difference between log(x) and log(y) \\(\\hat{\\Delta}\\) equal to −0.237. Note that \\(\\hat{\\Delta}\\) is the log of the Hodges-Lehmann estimate of the ratios in the original units, \\(log_{10}(0.58) = −0.237\\). The line parallel to y=x, log(residential) = −0.237•log(industrial), is plotted on figure 5.6. A t-test would now be appropriate for the logarithms, assuming each group’s transformed data were approximately normal. In summary, Q-Q plots of the quantiles of two data groups illustrate the reasonableness of hypothesis tests (t-test or rank-sum), while providing additional insight that the test procedures do not provide. Q-Q plots can demonstrate skewness, the presence of outliers, and inequality of variance to the data analyst. Perhaps most importantly, the presence of either an additive or multiplicative relationship between the two groups can easily be discerned. Since the t-test requires an additive difference between two groups, Q-Q plots can signal when transformations to produce additivity are necessary prior to using the t-test. Figure 5.6: Q-Q plot of the logs of the precipitation nitrogen data. 5.4 Estimating the Magnitude of Differences Between Two Groups After completion of an hypothesis test comparing two groups of data, the logical next step is to determine by how much the two groups differ. The most well-known approach, related to the two-sample t-test, is to compute the difference between the two group means (\\(\\bar{x}−\\bar{y}\\)). A more robust alternative, related to the rank-sum test, is one of a class of nonparametric estimators known as Hodges-Lehmann estimators. These two estimators are compared in the following sections. 5.4.1 The Hodges-Lehmann Estimator One nonparametric estimate of the difference between two independent groups is a Hodges-Lehmann estimator \\(\\hat{\\Delta}\\) (Hodges Jr and Lehmann (1963) ; Hollander and Wolfe (1973), p. 75-77). This estimator is the median of all possible pairwise differences between the x values and y values \\(\\hat{\\Delta} = median [x_i - y_j] \\ \\ \\text{for xi, i=1,...n and yj, j=1,..m. } \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ [5.5]\\) There will be n•m pairwise differences. Example 2 For the following x’s and y’s , compute 15 − 8 = 7, 15 − 27 = −12, etc: xi yj All Possible Differences(xi − yj) 15 8 7 9 17 17 27 -12 -10 -2 25 3 12 14 22 5 10 12 20 Ranked in order from smallest to largest, the 3•4 = 12 pairwise differences are −12, −10, −2, 7, 9, 10, 12, 12, 14, 17, 20, 22. The median of these is the average of the 6th and 7th smallest values, or \\(\\hat{\\Delta}\\) = 11. Note that the unusual y value of 27 could have been any number greater than 14 and the estimator \\(\\hat{\\Delta}\\) would be unchanged. Thus \\(\\hat{\\Delta}\\) is resistant. The \\(\\hat{\\Delta}\\) estimator is related to the rank-sum test, in that if \\(\\hat{\\Delta}\\) were subtracted from each of the x observations, the rank-sum statistic Wrs would provide no evidence for rejection of the null hypothesis. In other words, a shift of size \\(\\hat{\\Delta}\\) makes the data appear devoid of any evidence of difference between x and y when viewed by the rank-sum test. \\(\\hat{\\Delta}\\) is a median unbiased estimator of the difference in the medians of populations x and y. That is, the probability of underestimating or overestimating the difference between the median of x and the median of y is exactly one-half. If the populations were both normal, it would be a slightly less efficient estimator of differences in medians (or means) than would the parametric estimator x − y . However, when one or both populations is substantially non-normal, it is a more efficient (lower variance) estimator of this difference. There is another logical nonparametric estimator of the difference in population medians – the difference between the sample medians (xmed − ymed). For example 2, (xmed − ymed) = 10.5. Note that the difference in sample medians is not necessarily equal to the median of the differences \\(\\hat{\\Delta}\\). In addition, (xmed − ymed) is always somewhat more variable (less efficient) than is \\(\\hat{\\Delta}\\) and so is less desirable. A modified version of the \\(\\hat{\\Delta}\\) statistic is used as the estimate of the magnitude of the step trend in the seasonal rank-sum test procedure described by Crawford, Slack, and Hirsch (1983) (p. 74). 5.4.2 Confidence Interval for \\(\\hat{\\Delta}\\) A nonparametric interval estimate for \\(\\hat{\\Delta}\\) illustrates how variable the difference between the medians might be. No distribution is assumed for the pairwise differences. The interval is computed by a process similar to that for the confidence interval on the median described earlier. The tabled distribution of the test statistic is entered to find upper and lower critical values at one-half the desired alpha level. These critical values are transformed into ranks. After ordering the n•m pairwise differences from smallest to largest, the differences corresponding to those ranks are the ends of the confidence interval. For small sample sizes, table B4 for the rank-sum test is entered to find the critical value x* having a p-value nearest to α/2. This critical value is then used to compute the ranks Ru and Rl corresponding to the pairwise differences at the upper and lower confidence limits for \\(\\hat{\\Delta}\\). These limits are the Rlth ranked data points going in from either end of the sorted list of N=n•m pairwise differences. \\(R_l = X^* -\\frac{n•(n + 1)}{2} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{5.6}\\) \\(R_u = N-R_l +1 \\ \\ \\ \\ \\ \\ \\ \\ for N=n•m \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{5.7}\\) Example 2, cont. The N=12 possible pairwise differences between x and y were: −12, −10, −2, 7, 9, 10, 12, 12, 14, 17, 20, 22. The median of these (\\(\\hat{\\Delta}\\)) was 11. To determine an α ≅ 0.10 confidence interval for \\(\\hat{\\Delta}\\), the tabled critical value x* nearest to α/2 = 0.05 is 7 (p=0.057). The rank Rl of the pairwise difference at the lower end of the confidence interval is therefore \\(R_l = 7 -\\frac{3•4}{2} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\text{1 for n=3 and m=4.}\\) Ru, the rank of the pairwise difference at the upper end of the confidence interval is \\(Ru = 12.\\) With such a small data set, the α = 2•0.057 = 0.014 confidence limit for \\(\\hat{\\Delta}\\) is the range of the entire data set (the 1st difference in from either end), or \\(−12≤ \\hat{\\Delta} ≤ 22\\). When the large-sample approximation to the rank-sum test is used, a critical value \\(z_{α/2}\\) from the table of standard normal quantiles determines the upper and lower ranks of the pairwise differences corresponding to the ends of the confidence interval. Those ranks are \\[R_l = \\frac{N-z_a/2 • \\sqrt \\frac{N(n+m+1)}{3}}{2} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{5.8}\\] \\[R_u = \\frac{N+z_a/2 • \\sqrt \\frac{(N(n+m+1)}{3}}{2} +1\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{5.9}\\] \\[=N-R_l+1\\] Example 1 cont. For the precipitation nitrogen data there were N = (10)(10) = 100 possible pairwise differences. \\(\\hat{\\Delta}\\) would be the average of the 50th and 51st ranked differences. For a 95 percent confidence interval on \\(\\hat{\\Delta}\\) , zα/2 = 1.96 and \\[R_l = \\frac{100-1.96 • \\sqrt \\frac{100(10+10+1)}{3}}{2} =24.1\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{5.9}\\] \\[R_u = 100 - 24.1+1 = 76.9\\] the 24.1st ranked slope from either end. Rounding to the nearest integer, the 24th and 77th ranked slopes are used as the ends of the α ≅ 0.05 confidence limit on \\(\\hat{\\Delta}\\). Note that using the exact formula, from Table B4 the exact α level is determined to be 2•0.026 = 0.052. 5.4.3 Difference Between Mean Values As noted above, in the situation where the t-test is appropriate, the difference between the means of both groups \\(\\bar{x} − \\bar{y}\\) is the most efficient estimator of the difference between the two groups of data. Perhaps obvious is that when x and y are transformed prior to performing the ttest, ( \\(\\bar{x} − \\bar{y}\\) ) does not estimate the difference between group means in their original units. Less obvious is that a re-transformation of ( \\(\\bar{x} − \\bar{y}\\)) back to original units also does not estimate the difference between group means, but is closer to a function of group medians. For the log transformation as an example, \\(\\bar{x} − \\bar{y}\\) retransformed would equal the ratio of geometric means of the two groups. How close such a re-transformation comes to estimating the ratio of group medians depends on how close the data are to being symmetric in their transformed units. 5.4.4 Confidence Interval for \\(\\bar{x} − \\bar{y}\\) An interval estimate for the difference in means \\(\\bar{x} − \\bar{y}\\) is also available. It is appropriate in situations where the t-test may be used – when both data groups closely follow a normal distribution. When the variances of the two groups are similar and the pooled standard deviation s is used in the test, the confidence interval is \\[ CI = \\bar{x} - \\bar{y} ± t_{α/2,(n+m−2)} • s\\sqrt{1/n + 1/m}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{5.10}\\] When the standard deviations of the two groups are dissimilar and cannot be pooled, the confidence interval becomes \\[ CI = \\bar{x} - \\bar{y} ± t_{α/2,(df)} • s\\sqrt{s^2_x/n + s^2_y/m}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{5.11}\\] where df is the approximate degrees of freedom used in the t-test. Exercises 5.1 For the precipitation nitrogen data of Example 1, what would \\(W_{rs}\\) have been had the industrial site been used rather than the arbitrary choice of the residential site. What is the effect on the p-value? library(&quot;ggplot2&quot;) library(&quot;data.table&quot;) indus_site = c(0.59, 0.87, 1.1, 1.1, 1.2, 1.3, 1.6, 1.7, 3.2, 4.0) resid_site = c(0.3, 0.36, 0.5, 0.7, 0.7, 0.9, 0.92, 1.0, 1.3, 9.7) data &lt;- data.frame(site = rep(c(&quot;indus_site&quot;, &quot;resid_site&quot;), each = 10), conc = c(indus_site, resid_site)) print(data) ## site conc ## 1 indus_site 0.59 ## 2 indus_site 0.87 ## 3 indus_site 1.10 ## 4 indus_site 1.10 ## 5 indus_site 1.20 ## 6 indus_site 1.30 ## 7 indus_site 1.60 ## 8 indus_site 1.70 ## 9 indus_site 3.20 ## 10 indus_site 4.00 ## 11 resid_site 0.30 ## 12 resid_site 0.36 ## 13 resid_site 0.50 ## 14 resid_site 0.70 ## 15 resid_site 0.70 ## 16 resid_site 0.90 ## 17 resid_site 0.92 ## 18 resid_site 1.00 ## 19 resid_site 1.30 ## 20 resid_site 9.70 ggplot(data, aes(x = site, y = conc, fill = site)) + geom_boxplot()+ theme_bw() test &lt;- wilcox.test(x = indus_site, y = resid_site) ## Warning in wilcox.test.default(x = indus_site, y = resid_site): cannot compute ## exact p-value with ties test ## ## Wilcoxon rank sum test with continuity correction ## ## data: indus_site and resid_site ## W = 76.5, p-value = 0.04911 ## alternative hypothesis: true location shift is not equal to 0 The p-value is less than the significance level (0.05) that indicates a difference in the sum of ranks on two sample sites. 5.2 Historical ground-water quality data for a shallow aquifer underlying agricultural land shows the following nitrate concentrations (mg/L): Pre-1970 1 2 4 1 3 5 1 3 5 2 4 10 Post-1970 1 5 14 2 8 15 2 10 18 4 11 23 Given that we wish to test for a change in concentration between the two periods, should this be a one-sided or two-sided test? This should be a two-sided test because the test will be performed to estimate whether the amount of change has been increased or decreased. 5.3 Annual streamflows for the Green R. at Munfordville, KY were listed in Exercise 4.1. Beginning in 1969 the stream was regulated by a reservoir. a. Construct a Q-Q plot, and indicate whether the flows exhibit an additive or multiplicative relationship, or neither. Does there appear to be a relationship between (after−before) or (after/before) and the magnitude of annual flow itself? If so, explain why this might occur. Test whether flows after the reservoir came onstream are different. library(&quot;ggplot2&quot;) before = data.frame(year = 1950:1968, flows = c(4910, 3660, 3910, 1750, 1050, 2670, 2880, 2600, 3520, 1730, 2340, 2600, 3410, 1870, 1730, 2730, 1550, 4060, 2870)) before ## year flows ## 1 1950 4910 ## 2 1951 3660 ## 3 1952 3910 ## 4 1953 1750 ## 5 1954 1050 ## 6 1955 2670 ## 7 1956 2880 ## 8 1957 2600 ## 9 1958 3520 ## 10 1959 1730 ## 11 1960 2340 ## 12 1961 2600 ## 13 1962 3410 ## 14 1963 1870 ## 15 1964 1730 ## 16 1965 2730 ## 17 1966 1550 ## 18 1967 4060 ## 19 1968 2870 after = data.frame(year = 1969:1980, flows = c(1350, 2350, 3140, 3060, 3630, 3890, 3780, 3180, 2260, 3430, 5290, 2870)) after ## year flows ## 1 1969 1350 ## 2 1970 2350 ## 3 1971 3140 ## 4 1972 3060 ## 5 1973 3630 ## 6 1974 3890 ## 7 1975 3780 ## 8 1976 3180 ## 9 1977 2260 ## 10 1978 3430 ## 11 1979 5290 ## 12 1980 2870 #a- Q-Q plot qqplot(before$flows, after$flows, xlab=&quot;Before&quot;, ylab=&quot;After&quot;)+ theme_classic() ## NULL #b. Relationship between the magnitude of annual flow (after−before) or (after/before) rel_bef &lt;- mean(before$flows) - mean(after$flows) rel_bef ## [1] -457.4123 rel_aft &lt;- mean(after$flows) - mean(before$flows) rel_aft ## [1] 457.4123 library(&quot;ggplot2&quot;) before = 1950:1968 after = 1969:1980 before_flow = c(4910, 3660, 3910, 1750, 1050, 2670, 2880, 2600, 3520, 1730, 2340, 2600, 3410, 1870, 1730, 2730, 1550, 4060, 2870) after_flow = c(1350, 2350, 3140, 3060, 3630, 3890, 3780, 3180, 2260, 3430, 5290, 2870) data &lt;- data.frame(year = c(before, after), flows = c(before_flow, after_flow), group = c(rep(&quot;Before&quot;, 19), rep(&quot;After&quot;, 12))) ggplot(data, aes(x = year, y = flows, color = group))+ geom_line(colour=&quot;blue&quot;, linetype=&quot;19&quot;, size=0.2) + geom_point()+ xlab(&quot;Year&quot;)+ ylab(&quot;Annual Peak Discharge (cfs)&quot;)+ theme_bw() #The magnitude of annual streamflow after 1969, shows an increase in flow rate and this increase is a result of the increase in the discharge volume of the reservoir. #c. Test whether flows after the reservoir came onstream are different. before = data.frame(year = 1950:1968, flows = c(4910, 3660, 3910, 1750, 1050, 2670, 2880, 2600, 3520, 1730, 2340, 2600, 3410, 1870, 1730, 2730, 1550, 4060, 2870)) before ## year flows ## 1 1950 4910 ## 2 1951 3660 ## 3 1952 3910 ## 4 1953 1750 ## 5 1954 1050 ## 6 1955 2670 ## 7 1956 2880 ## 8 1957 2600 ## 9 1958 3520 ## 10 1959 1730 ## 11 1960 2340 ## 12 1961 2600 ## 13 1962 3410 ## 14 1963 1870 ## 15 1964 1730 ## 16 1965 2730 ## 17 1966 1550 ## 18 1967 4060 ## 19 1968 2870 after = data.frame(year = 1969:1980, flows = c(1350, 2350, 3140, 3060, 3630, 3890, 3780, 3180, 2260, 3430, 5290, 2870)) after ## year flows ## 1 1969 1350 ## 2 1970 2350 ## 3 1971 3140 ## 4 1972 3060 ## 5 1973 3630 ## 6 1974 3890 ## 7 1975 3780 ## 8 1976 3180 ## 9 1977 2260 ## 10 1978 3430 ## 11 1979 5290 ## 12 1980 2870 test &lt;- wilcox.test(before$flows, after$flows) ## Warning in wilcox.test.default(before$flows, after$flows): cannot compute exact ## p-value with ties test ## ## Wilcoxon rank sum test with continuity correction ## ## data: before$flows and after$flows ## W = 83.5, p-value = 0.2236 ## alternative hypothesis: true location shift is not equal to 0 #The p-value is higher than the significance level (0.05) that represents no difference. 5.4 Consider the following small data set X: 1.0, 2.0, 3.0, 4.0 Y: 1.5, 2.5, 3.5, 4.5, 5.5, 7.0, 10.0, 20.0, 40.0, 100.0 Using the Table B4, determine the two-sided p value for an additive difference between the X and Y data using the exact rank-sum test. Then compute it using the large-sample approximation. Then compute it using the t-test on ranks. Compute the expected difference \\(\\hat{\\Delta}\\) between X and Y. x = c(1.0, 2.0, 3.0, 4.0) y = c(1.5, 2.5, 3.5, 4.5, 5.5, 7.0, 10.0, 20.0, 40.0, 100.0) wilcox.test(x, y, exact = FALSE, correct = T) ## ## Wilcoxon rank sum test with continuity correction ## ## data: x and y ## W = 6, p-value = 0.05624 ## alternative hypothesis: true location shift is not equal to 0 #Large-sample approximation nom_x = length(x) nom_y = length(y) total = nom_x + nom_y mean_approx = nom_x*(total + 1)/2 mean_approx ## [1] 30 var_approx = sqrt(nom_x*nom_y*(total + 1)/12) var_approx ## [1] 7.071068 rank_data = rank(c(x, y)) rank_x = rank_data[1:4] rank_y = rank_data[5:14] wil_rank_sum = sum(rank(c(x,y))[1:4]) man_whit = sum(rank(c(x,y))[5:14]) stand_value = (wil_rank_sum + 1/2 - man_whit) / var_approx stand_value ## [1] -10.25305 prob &lt;- pnorm(q = stand_value) prob ## [1] 5.73345e-25 t.test(rank_x, rank_y, var.equal=F) ## ## Welch Two Sample t-test ## ## data: rank_x and rank_y ## t = -2.7349, df = 8.6547, p-value = 0.02386 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -8.9777718 -0.8222282 ## sample estimates: ## mean of x mean of y ## 4.0 8.9 5.5 Unit well yields, in gallons per minute per foot of water-bearing material, were contrasted for wells within valleys containing fracturing versus valleys with no fracturing (Wright, 1985). For the PPCC test for normality, r(with)=0.943 and r(without)=0.805. Perform the appropriate α = 0.05 test to discern whether fracturing is associated with higher mean unit well yield with &lt;- c(0.95, 0.72, 0.51, 0.44, 0.40, 0.30, 0.18, 0.16, 0.16, 0.13, 0.086, 0.031, 0.020) without &lt;- c(1.02, 0.49, 0.454, 0.10, 0.077, 0.041, 0.040, 0.030, 0.020, 0.007, 0.003, 0.001) test &lt;- data.frame(fracturing = c(rep(&quot;without&quot;, 12), rep(&quot;with&quot;, 13)), yield = c(without, with)) test ## fracturing yield ## 1 without 1.020 ## 2 without 0.490 ## 3 without 0.454 ## 4 without 0.100 ## 5 without 0.077 ## 6 without 0.041 ## 7 without 0.040 ## 8 without 0.030 ## 9 without 0.020 ## 10 without 0.007 ## 11 without 0.003 ## 12 without 0.001 ## 13 with 0.950 ## 14 with 0.720 ## 15 with 0.510 ## 16 with 0.440 ## 17 with 0.400 ## 18 with 0.300 ## 19 with 0.180 ## 20 with 0.160 ## 21 with 0.160 ## 22 with 0.130 ## 23 with 0.086 ## 24 with 0.031 ## 25 with 0.020 t.test(without, with, alternative = &quot;greater&quot;) ## ## Welch Two Sample t-test ## ## data: without and with ## t = -1.0413, df = 22.219, p-value = 0.8455 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -0.3287448 Inf ## sample estimates: ## mean of x mean of y ## 0.1902500 0.3143846 As the p-value is higher than α, therefore a significant difference exists. Higher p-value means non significance of t-statistic, that means very less support to reject the null hypothesis. 5.6 Assume that the unit well yield data are now trace organic analyses from two sampling sites and that all values below 0.050 were reported as “&lt; 0.05.” Retest the hypothesis that H0 : μx = μy versus H1 : μx &gt; μy using the rank-sum test. By how much does the test statistic change? Are the results altered by presence of a detection limit? Could a t-test be used in this situation? library(&quot;ggplot2&quot;) with &lt;- c(0.95, 0.72, 0.51, 0.44, 0.40, 0.30, 0.18, 0.16, 0.16, 0.13, 0.086, 0.031, 0.020) without &lt;- c(1.02, 0.49, 0.454, 0.10, 0.077, 0.041, 0.040, 0.030, 0.020, 0.007, 0.003, 0.001) test &lt;- data.frame(fracturing = c(rep(&quot;without&quot;, 12), rep(&quot;with&quot;, 13)), yield = c(without, with) ) test ## fracturing yield ## 1 without 1.020 ## 2 without 0.490 ## 3 without 0.454 ## 4 without 0.100 ## 5 without 0.077 ## 6 without 0.041 ## 7 without 0.040 ## 8 without 0.030 ## 9 without 0.020 ## 10 without 0.007 ## 11 without 0.003 ## 12 without 0.001 ## 13 with 0.950 ## 14 with 0.720 ## 15 with 0.510 ## 16 with 0.440 ## 17 with 0.400 ## 18 with 0.300 ## 19 with 0.180 ## 20 with 0.160 ## 21 with 0.160 ## 22 with 0.130 ## 23 with 0.086 ## 24 with 0.031 ## 25 with 0.020 ggplot(test, aes(x = fracturing, y = yield)) + geom_boxplot()+ theme_bw() wilcox.test(without, with, exact = FALSE, correct = T) ## ## Wilcoxon rank sum test with continuity correction ## ## data: without and with ## W = 43.5, p-value = 0.0643 ## alternative hypothesis: true location shift is not equal to 0 nom_x = length(without) nom_y = length(with) total = nom_x + nom_y mean_approx = nom_x*(total + 1)/2 mean_approx ## [1] 156 var_approx = sqrt(nom_x*nom_y*(total + 1)/12) var_approx ## [1] 18.38478 rank_data = rank(c(without, with)) rank_x = rank_data[1:12] rank_y = rank_data[12:25] wil_rank_sum = sum(rank(c(without,with))[1:12]) man_whit = sum(rank(c(without,with))[13:25]) stand_value = (wil_rank_sum + 1/2 - man_whit) / var_approx stand_value ## [1] -4.433016 prob &lt;- pnorm(q = stand_value) prob ## [1] 4.646207e-06 t.test(rank_x, rank_y, var.equal=F) ## ## Welch Two Sample t-test ## ## data: rank_x and rank_y ## t = -1.5256, df = 21.964, p-value = 0.1414 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.575645 1.611359 ## sample estimates: ## mean of x mean of y ## 10.12500 14.60714 contained in the data below detection limit is extracted using ranks. Results are the same (one-sided p-value = 0.039. Reject equality). A t-test could not be used without improperly substituting References "],
["ch6.html", "Chapter 6 Matched-Pair Tests 6.1 The Sign Test 6.2 The Signed-Rank Test 6.3 The Paired t-Test 6.4 Consequences of Violating Test Assumptions 6.5 Graphical Presentation of Results 6.6 Estimating the Magnitude of Differences Between Two Groups Exercises", " Chapter 6 Matched-Pair Tests To determine the effectiveness of an acid solution in developing wells in carbonate rock, yields of twenty wells were measured both before and after treatment of the wells with acid. Factoring out the differences in yield between wells, have the yields changed as a result of using the acid? What is the magnitude of this change? Annual sediment loads are measured at two sites over a period of twenty-four years. Both drainage basins are of essentially the same size, and have the same basin characteristics. However, logging has occurred in one basin during the period, but not in the other. Can the year to year variation in load (due to differences in precipitation) be compensated for, to determine whether the site containing logging produced generally higher loads than the other? Two laboratories are compared in a quality assurance program. Each lab is sent one of a pair of 30 samples split into duplicates in the field, to determine if one lab consistently over- or underestimates the concentrations of the other. If no difference between the labs is seen, their data may be combined prior to interpretation. The differences between labs must be discerned above the sample to sample differences. As with the tests of Chapter 5, we wish to determine if one group tends to contain larger values than the other. However, now there is a logical pairing of the observations within each group. Further, there may be a great deal of variability between these pairs, as with the year-to-year pairs of sediment data in the second example above. Both basins exhibit low yields in dry years, and higher yields in wet years. This variability between pairs of observations is noise which would obscure the differences between the two groups being compared if the methods of Chapter 5 were used. Instead, pairing is used to block out this noise by performing tests on the differences between data pairs. Two nonparametric tests are presented for determining whether paired observations differ, the sign test and the signed-rank test. Also presented is the paired ttest, the parametric equivalent which may be used when the differences between pairs are known to be normally distributed. After surveying graphical methods to illustrate the test results, estimators for the difference between the two groups are discussed. For paired observations (xi,yi), i=1,2,…n, their differences Di = xi − yi are computed. The tests in this chapter determine whether xi and yi are from the same population – the null hypothesis – by analyzing the Di. If there are differences, the null hypothesis is rejected. When the Di’s have a normal distribution, a paired t-test can be employed. The paired t-test determines whether the mean of the Di’s equals 0. This is equivalent to stating that the mean of the xi and the yi are the same. If the Di’s are symmetric, but not necessarily normal, a signedrank test can be used. The signed-rank test determines whether the median of the the Di’s is equal to 0. The assumption of symmetry made by the signed-rank test is much less restrictive than that of normality, as there are many non-normal distributions which are symmetric. As a result, the signed-rank test is a more generally applicable method than the t-test. If the differences are asymmetric, the sign test may be used. The sign test does not require an assumption of symmetry or normality. It tests a more general hypothesis than comparisons of means or medians – does x tend to be higher (or lower, or different) than y? The sign test is the most generally applicable of the three methods. It is also appropriate when the magnitude of the paired differences cannot be computed but one observation can be determined to be higher than the other, as when comparing a &lt;1 to a 3. (Analysis of data below the detection limit is discussed in detail in Chapter ??. See also exercises 6.4 and 6.5 at the end of this chapter.) 6.1 The Sign Test For data pairs (xi,yi) i=1,…n, the sign test determines whether x is generally larger (or smaller, or different) than y, without regard to whether that difference is additive. The sign test may be used regardless of the distribution of the differences, and thus is fully nonparametric. 6.1.1 Null and Alternate Hypotheses The null and alternative hypotheses may be stated as follows: H0: Prob [x &gt; y] = 0.5, versus one of the three possible alternative hypotheses: H1: Prob [x &gt; y] ≠ 0.5 (2-sided test -- x might be larger or smaller than y). H2: Prob [x &gt; y] &gt; 0.5 (1-sided test -- x is expected to be larger than y) H3: Prob [x &gt; y] &lt; 0.5 (1-sided test-- x is expected to be smaller than y). 6.1.2 Computation of the Exact Test If the null hypothesis is true, about half of the differences Di will be positive (xi &gt; yi) and about half negative (xi &lt; yi). If one of the alternate hypotheses is true instead, more than half of the differences will tend to be either positive or negative. The exact form of the sign test is given below. It is the form appropriate when comparing 20 or fewer pairs of samples. With larger sample sizes the large-sample approximation may be used. Remember that computer packages usually report p-values from the large sample approximation regardless of sample size. Exact form of the sign test Situation Two paired groups of data are to be compared, to determine if one group tends to produce larger (or different) values than the other group. No assumptions about the distribution of the differences Di = xi − yi, i = 1,…N are required. This means that no assumption is made that all pairs are expected to differ by about the same amount. Numerical values for the data are also not necessary, as long as their relative magnitudes may be determined. Tied data Ignore all tied data pairs (all Di =0). Reduce the sample size of the test to the number of nonzero differences n. Computation Delete all Di= 0 (xi = yi). The test uses the n nonzero differences n= N−[number of Di=0]. Assign a + for all Di &gt; 0, and a − for all Di &lt; 0. Test Statistic S+ = the number of +’s, the number of times xi &gt; yi, i = 1,…n. Decision Rule To reject H0: Prob [x &gt; y] = 0.5, H1: Prob [x &gt; y] ≠ 0.5 (the x measurement tends to be either larger or smaller than the y measurement). Reject H0 if \\(S^+ \\geq x_{α/2,n}\\) or \\(S^+ \\leq x&#39;_{α/2,n}\\) from Table B5; otherwise do not reject H0. H2: Prob [x &gt; y] &gt; 0.5 (the x measurement tends to be larger than the y measurement). Reject H0 if \\(S^+ \\geq x_{α,n}\\) from Table B5; otherwise do not reject H0. H3: Prob [x &gt; y] &lt; 0.5 (the x measurement tends to be smaller than the y measurement). Reject H0 if \\(S^+ \\leq x&#39;_{α,n}\\) from Table B5; otherwise do not reject H0 . Example 1. Counts of mayfly nymphs were recorded in 12 small streams at low flow above and below industrial outfalls. The mayfly nymph is an indicator of good water quality. The question to be considered is whether effluents from the outfalls decreased the number of nymphs found on the streambeds of that region. A Type I risk level α of 0.01 is set as acceptable. Figure 6.1a presents a separate boxplot of the counts for the above and below groups. Both groups are positively skewed. There is a great deal of variability within these groups due to the differences from one stream to another, though in general the counts below the outfalls appear to be smaller. A rank-sum test as in Chapter 5 between the the two groups would be inefficient, as it would not block out the stream to stream variation (no matching of the pair of above and below counts in each stream). Variation in counts among the streams could obscure the difference being tested for. The natural pairing of observations at the same stream can be used to block out the stream to stream variability by computing the above−below differences in counts for each stream (figure 6.1 b). A test is then performed on these differences. Note the asymmetry of the paired differences. They do not appear to all be of about the same magnitude. Figure 6.1: a) above and below counts. b) above − below differences. The null hypothesis H0 is that the counts above the outfalls are equally likely to be higher or lower than counts below the outfalls. The one-sided alternate hypothesis H2 is that the counts below the outfalls are expected to be lower, in which case S+ would be large. Of the 12 pairs, 11 are increases, so S+ = 11. Note that this statistic is very resistant to outliers, as the magnitudes of the differences are not used in computing the test statistic. From Table B5 of the Appendix, the one-sided p-value for S+ = 11 is 0.003. Therefore reject that counts above and below the outfall are the same at α = 0.01. 6.1.3 The Large Sample Approximation For sample sizes n&gt;20 the exact sign test statistic can be modified so that its distribution closely follows a standard normal distribution. Again, this does not mean that the data or their differences require normality. It is only the modified test statistic which follows a normal distribution. The large sample approximation for the sign test takes the standardized form \\[ \\begin{equation} Z_{rs} = \\begin{cases} \\frac{S^{+}-1/2-\\mu_S{+}}{\\sigma_S^{+}} &amp;\\text{if } S^{+} &gt; \\mu_S+ \\\\\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ {0} &amp;\\text{if } S^{+}=\\mu_S+ \\\\\\\\ \\frac{S^{+} +1/2-\\mu_S+}{\\sigma_S+} &amp; \\text{if } S^{+} &lt; \\mu_S+ \\end{cases} \\end{equation} \\] \\[where \\ \\ \\ \\mu_S+ = \\frac{n}{2}, \\ \\ and \\ \\ \\sigma_{S^+}=\\frac{1}{2}{\\sqrt{n} \\ \\ .} \\] The 1/2 in the numerator of Z+ is again a continuity correction (see Chapter 5). Z+ is compared to a table of the standard normal distribution to obtain the approximate p-value. Using the mayfly data of Example 1, the approximate p-value of p = 0.005 is obtained below. This is very close to the true (exact) p=0.003, and both are sufficiently small that the decision to reject H0 would not be altered by their difference. Therefore, if accurate p-values are of primary concern, such as when p is close to the agreedupon risk α, and the sample size is 20 or smaller, perform the exact test to get accurate p-values. Regardless of sample size, if p-values are not the primary interest and one must simply decide to reject H0 or not, when p-values are much smaller (such as 0.001) or much larger (such as 0.50) than α the decision whether to reject H0 will be sufficiently clear from the approximate procedure. Example 1, cont. \\[For \\ \\ \\ S^+=11, \\ \\ \\ \\ \\mu_S+=\\frac{12}{2}=6\\sigma_S+=\\frac{1}{2}{\\sqrt{12}=1.73} \\\\Z^+=\\frac{11-\\frac{1}{2}-6}{1.73}=2.60\\] And from a table of the normal distribution, the approximate one-sided p-value = 0.005. 6.2 The Signed-Rank Test The signed-rank test was developed by Wilcoxon (1945), and is sometimes called the Wilcoxon signed-rank test. It is used to determine whether the median difference between paired observations equals zero. It may also be used to test whether the median of a single data set is significantly different from zero. 6.2.1 Null and Alternate Hypotheses For Di = xi − yi, the null hypothesis for the signed-rank test is stated as: H0: median[D] = 0. The alternative hypothesis is one of three statements: H1: median[D] ≠ 0 (2-sided test -- x might be larger or smaller than y). H2: median[D] &gt; 0 (1-sided test -- x is expected to be larger than y) H3: median[D] &lt; 0 (1-sided test-- x is expected to be smaller than y). The signed-rank test is usually stated as a determination of whether the x’s and y’s come from the same population (same median and other percentiles), or alternatively that they differ only in location (central value or median). If both groups are from the same population, regardless of the shape, about half of the time their difference will be above 0, and half below 0. In addition, the distribution of data above 0 will on average mirror that below 0, so that given a sufficient sample size the differences will be symmetric. They may not be anything like a normal distribution, however. If the alternative hypothesis is true, the differences will be symmetric when x and y come from the same shaped distribution (whatever the shape), differing only in central value (median). This is called an additive difference between the two groups, meaning that the variability and skewness within each group is the same for both. Boxplots for the two groups would look very similar, with the only difference being an offset of one from the other. The signed-rank test determines whether this “offset”, the magnitude of difference between paired observations, is significantly different from zero. For additive differences (the assumption of symmetric differences is valid), the signed-rank test has more power to detect differences than does the sign test. In addition, the signed-rank test is also appropriate when the differences are not symmetric in the units being used, but a logarithmic transformation of both data sets will produce differences which are symmetric. In such a situation a multiplicative relationship is made into an additive relationship in the logarithms. For example, figure 6.2 displays the differences between two positively skewed distributions. A multiplicative relationship between x and y is suspected, ie. x = c•y, where c is some constant. This is a common occurrence with water resources data; data sets having higher median values also often have higher variances than “background” sites with low median values. In the original units the Di from such data are asymmetric. Changing units by taking the logarithms of the data prior to calculating differences, the boxplot of figure 6.3 results. The log transformation (θ = 0) changes a multiplicative relationship to an additive one: log x = log c + log y. The variances of the logs are often made similar by the transformation, so that the logs differ only in central value. The Dli, the differences in log units, are therefore much more symmetric than the differences in the original units. The median difference in the logs can then be re-transformed to estimate the median ratio of the original units, \\(\\hat{c} = median [y/x] = exp (median [Dl]\\) ). Figure 6.2: Boxplot of asymmetric Di = xi − yi Figure 6.3: Boxplot of symmetric \\(Dl_i = log(x_i) − log(y_i)\\) 6.2.2 Computation of the Exact Test If the null hypothesis is true, the median [D] will be close to zero, and the differences will be symmetric around zero. If one of the alternate hypotheses is true instead, the differences will not have a median near zero, but show a symmetric distribution around a nonzero median. Therefore more than half will be either positive or negative. The signed-rank test uses both the signs of the differences as in the sign test, along with the ranks of the absolute values of those differences. This latter information makes sense to use only when the differences are symmetric. The exact form of the signed-rank test is given below. It is the only form appropriate for comparing 15 or less pairs of samples. With larger sample sizes either large-sample or rank transform approximations may be used. Example 1, cont. The differences Di result in the signed-ranks Ri of table 6.2. From these \\(W^+\\) = the sum of the positive \\(R_{i}&#39;s\\) = 72 From Table B6, the one-sided p-value for n=12 and W+ = 72 is 0.003. This is strong evidence against the null hypothesis being true. However, the Di are asymmetric, violating one of the test’s assumptions, and indicating that the differences between the two groups may not be an additive one. Asymmetry can be expected to occur when large values tend to produce large differences, and smaller values smaller differences. This indicates that a multiplicative relationship between the data pairs is more realistic. So projecting that a multiplicative relationship may have produced the skewed distribution of Di’s, the base 10 logs of the data were calculated, and a new set of differences \\(Dl_i = log(x_i) − log(y_i)\\) are computed and presented in table 6.2 and figure 6.4. Comparing figures 6.4 and figure 6.1b, note that these Dli are much more symmetric than those in the original units. Using the Dli, \\(W^+\\) = the sum of the positive \\(Rl_i&#39;s = 69\\) and the exact p-value from Table B6 is 0.008. This should be considered more correct than the results for the untransformed data, as the differences are more symmetric, meeting the requirements of the test procedure. Note that the p-values are not drastically changed, however, and the conclusion to reject H0 was not affected by the lack of a transformation. 6.2.3 The Large Sample Approximation To avoid requiring a large table of exact signed-rank test statistics for all possible sample sizes, the exact test statistic is standardized by subtracting its mean and dividing by its standard deviation so that its distribution closely follows a standard normal distribution. This approximation is valid for sample sizes of n&gt;15. Figure 6.4: Boxplot for the differences of the base 10 logarithms of the mayfly data. The large sample approximation for the signed-ranks test takes the standardized form \\[ \\begin{equation} Z_{sr^+} = \\begin{cases} \\frac{W^{+}-\\frac{1}{2}-\\mu_W{+}}{\\sigma_W^{+}} &amp;\\text{if } W^{+} &gt; \\mu_W+ \\\\\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ {0} &amp;\\text{if } W^{+}=\\mu_W+ \\\\\\\\ \\frac{W^{+} +\\frac{1}{2}-\\mu_W+}{\\sigma_W+} &amp; \\text{if } W^{+} &lt; \\mu_W+ \\end{cases} \\end{equation} \\] \\[where \\ \\ \\ \\mu_W+ = \\frac{n•{(n+1)}}{4}, \\ \\ and \\ \\ \\sigma_{W^+}={\\sqrt \\frac{n•(n+1)•(2n+1)}{24} \\ \\ .} \\] The 1/2 in the numerator of Zsr+ is the continuity correction. Zsr+ is compared to a table of the standard normal distribution to obtain the approximate p-value for the signed-rank test. For the logarithms of the mayfly data of Example 1, the approximate p-value of p = 0.01 is obtained below. This is close to the exact value of 0.008, considering that the sample size of 12 is too small for use of the approximation. When the sample size is 15 or smaller, perform the exact test to get accurate p-values. Example 1, cont. For \\(W^+=69\\), \\[\\mu_W+=\\frac{12•(13)}{4}=39 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma_W+=\\sqrt\\frac{12•(13)•(25)}{24}=12.75\\\\ Z_{sr+}=\\frac{69-\\frac{1}{2}-39}{12.75}=1.31\\] And from a table of the normal distribution, the approximate one-sided p-value = 0.010. 6.2.4 The Rank Transform Approximation The rank transform approximation for the signed-rank test is computed by performing a paired t-test on the signed ranks Ri (or Rli, if the differences of the logs are more symmetric) rather than on the original data. For this approximation the zero differences Di = 0 are retained prior to computing the test so that there are N, not n, signed ranks. This approximation should be called a “t-test on signed ranks” rather than a signed-ranks test for the sake of clarity. Computations will be given in detail following the presentation of the paired t-test in the next section. The rank-transform p-value calculated in that section for the logs of the mayfly data is 0.005, close to the exact p-value of 0.008. The rank transform approximation should be acceptable for sample sizes greater than 15. 6.3 The Paired t-Test The paired t-test is the most commonly used test for evaluating matched pairs of data. However, it should not be used without expecting the paired differences Di to follow a normal distribution. Only if the Di are normal should the t-test be used. As with the signed-ranks test, logarithms may be taken prior to testing for normality if a multiplicative relationship is suspected. In contrast, all symmetric data, or data which would be symmetric after taking logarithms, may be tested using the signed-ranks test regardless of whether they follow a normal distribution. 6.3.1 Assumptions of the Test The paired t-test assumes that the paired differences Di are normally distributed around their mean. The two groups of data are assumed to have the same variance and shape. Thus if they differ, it is only in their mean (central value). The null hypothesis can be stated as \\(H0 : \\mu_x = μ_y\\) the means for groups x and y are identical, or \\(H0 : \\mu_{[D]} = 0\\) the mean difference between groups x and y equals 0. When the Di are not normal, and especially when they are not symmetric, the p-values obtained from the t-test will not be accurate. When the Di are asymmetric, the mean will not provide a good estimate of the center, as discussed in Chapter 1. Therefore \\(\\mu_{[D]}\\) will not be a good estimate of the additive difference between x and y. 6.3.2 Computation of the Paired t-Test Paired t-test Situation Two paired groups of data are to be compared, to determine if their differences Di = xi − yi are significantly different from zero. These differences must be normally distributed. Both x and y follow the same distribution (same variance), except that μx and μy might not be equal. Test Statistic Compute the paired t-statistic: \\[t_p =\\frac{\\bar{D}\\sqrt{n}}{s}\\] where \\(\\bar{D}\\) is the sample mean of the differences Di \\(\\bar{D} = \\frac{\\sum_{i=1}^n D_i}{n}, \\ \\ \\ \\ \\ and \\ \\ \\ s=\\frac{\\sqrt{\\sum_{i=1}^n (D_i-\\bar{D})^2}}{n-1}\\) the sample standard deviation of the Di’s. Decision Rule. To reject H0 : μx = μy H1 : μx ≠ μy (the two groups have different mean values, but there is no prior knowledge which of x or y might be higher) Reject H0 if \\(tp &lt; −t_{(1−α/2),(n−1)}\\) or \\(tp &gt; t_{(1−α/2),(n−1)}\\) from a table of the t distribution; otherwise do not reject H0. H2 : μx &gt; μy (prior to seeing any data, x is expected to be greater than y) Reject H0 if \\(tp &gt; t_{(1−α),(n−1)}\\) from a table of the t distribution; otherwise do not reject H0 . H3 : μx &lt; μy (prior to seeing any data, y is expected to be greater than x) Reject H0 if \\(tp &lt; −t_{(1−α),(n−1)}\\) from a table of the t distribution; otherwise do not reject H0 . Example 1, cont. Paired t-test on the mayfly data: The PPCC test for normality on the paired differences Di has r = 0.82, with an associated p-value of &lt;0.005. Therefore it is highly unlikely that these data come from a normal distribution, and the t-test cannot validly be run. In an attempt to obtain a distribution closer to normal, the logs of the data are computed. Again as with the signed-rank test, this implies that a multiplicative rather than an additive relationship exists between x and y. The PPCC test for normality of the differences between the logarithms Dli has r = 0.92, and a p-value of 0.036. Therefore normality of the logarithms would still be rejected at α = 0.05, and the t-test should still not be performed. One could try a series of power transformations, selecting the one whose PPCC test coefficient is closest to 1.0. Howver, it may be difficult to translate the results back into original units – “the negative square root of differences is statistically different”. If the t-test performed on the logs, the following would result: \\(\\bar{D}=0.333, \\ \\ \\ s=\\sqrt{\\frac {1.59^2}{11}=0.479, \\ \\ \\ \\ \\ so \\ \\ \\ t_p=2.41.}\\) Reject H0 in favor of H2 if \\(t_p &gt; t_{0.95, 11} = 1.80\\). Therefore reject that μx = μy. The onesided p-value for tp is about 0.02. Note that this is higher than the signed-rank test’s p-value of 0.008, reflecting a probable slight loss in power for the t-test as computed on the (non-normal) logarithms of the data. Rank approximation to the signed-rank test (t-test on signed-ranks): The t-test is performed on the signed-ranks of Dli, (see Table 6.2). \\(\\bar{Rl}=5, \\ \\ \\ \\ \\ \\ \\ s=\\sqrt{\\frac{18.71^2}{11}}=5.64, \\ \\ \\ and \\ \\ t_r=3.07.\\) Reject H0 in favor of H2 if \\(t_r &gt; t_{0.95, 11} = 1.80\\). Therefore reject H0. The one-sided p-value equals 0.005, close to the exact p-value of 0.008. Note that the t-test on signed-ranks, as a nonparametric test, ably overlooks the non-normality of the data. The paired t-test does not, and is less able to distinguish the differences between the data logarithms (as shown by its higher p-value) because those differences are non-normal. 6.4 Consequences of Violating Test Assumptions 6.4.1 Assumption of Normality (t-Test) The primary consequence of overlooking the normality assumption underlying the t-test is a loss of power to detect differences which may truly be present. The second consequence is an unfounded assumption that the mean difference is a meaningful description of the differences between the two groups. For example, suppose a t-test was blindly conducted on the mayfly data without checking for normality of the differences. The test statistic of t=2.08 has a one-sided p-value of 0.03. This is one order of magnitude above the exact p-value for the (nonparametric) sign test of 0.003. Had an α of 0.01 been chosen, the t-test would be unable to reject H0 while the sign test would easily reject. The non-normality of the differences “confuses” the t-test by inflating the estimate of standard deviation s, and making deviations from a zero difference difficult to discern. The mean difference \\(\\bar{D}\\) of 74.4 counts for the mayfly data is larger than 10 of the 12 paired differences listed in table ??. It has little usefulness as a measure of how many more mayfly nymphs are found above outfalls than below. The lack of resistance of the mean to skewness and outliers heavily favors the general use of the median or Hodges-Lehmann estimator. Another drawback to the mean is that when transformations are used prior to computing a t-test, re-transforming the estimate of the mean difference back into the original units does not provide an estimate of the mean difference in the original units. 6.4.2 Assumption of Symmetry (Signed-Rank Test) When the signed-rank test is performed on asymmetric differences, it rejects H0 slightly more often than it should. The null hypothesis is essentially that symmetric differences have a median of zero, and asymmetry favors rejection as does a nonzero median. Some authors have in fact stated that it is a test for asymmetry. However, asymmetry must be severe before a substantial influence is felt on the p-value. While only one outlier can disrupt the t-test’s ability to detect differences between two groups of matched pairs, most of the negative differences must be smaller in absolute value than are the positive differences before a signed-rank test rejects H0 due solely to asymmetry. One or two outliers will have little effect on the signed-rank test, as it uses their rank and not their value itself for the computation. Therefore violation of the symmetry assumption of the signed-rank test produces p-values only slightly lower than they should be, while violating the t-test’s assumption of normality can produce p-values much larger than what is correct. Add to this the fact that the assumption of symmetry is less restrictive than that of normality, and the signed-rank test is seen to be relatively insensitive to violation of its assumptions as compared to the t-test. Inaccurate p-values for the signed-rank test is therefore not the primary problem caused by asymmetry. The p-values for the mayfly data, for example, are not that different (p = 0.003 for the original units and 0.008 for the logs) before and after a transformation to achieve symmetry. Both are similar to the p-value for the sign test, which does not require symmetry. However, inappropriate estimates of the magnitude of the difference between data pairs will result from estimating an additive difference when the evidence points towards a multiplicative relationship. Therefore symmetry is especially important to check if the magnitude of the difference between data pairs is to be estimated. Equally as important to check is the form of the relationship between x and y, using the scatterplots of the next section. 6.5 Graphical Presentation of Results Methods for illustrating matched-pair test results are those already given in Chapter 2 for illustrating a single data set, as the differences between matched pairs constitute a single data set. A probability plot of the paired differences, for example, will show whether or not the data follow a normal distribution. Of the methods in Chapter 2, the boxplot is the single graphic which best illustrates both the test results and the degree of conformity to the test’s assumptions. The equivalent graphic to a Q-Q plot for paired data is a scatterplot of the data pairs. The addition of the x=y line and a smooth of the paired data will help illustrate the test results. 6.5.1 Boxplots {#6-5-1} The best method for directly illustrating the results of the sign, signed-rank or paired t-tests is a boxplot of the differences, as in figure 6.1 b. The number of data above and below zero and the nearness of the median difference to zero are clearly displayed, as is the degree of symmetry of the Di. Though a boxplot is an effective and concise way to illustrate the characteristics of the differences, it will not show the characteristics of the original data. This can be better done with a scatterplot. 6.5.2 Scatterplots With X=Y Line Scatterplots illustrate the relationships between the paired data (figure 6.5. Each (x,y) pair is plotted as a point. Similarity between the two groups of data is illustrated by the closeness of the data to the x=y line. If x is generally greater than y, most of the data will fall below the line. When y exceeds x, the data will lie largely above the x=y line. This relationship can be made clearer for large data sets by superimposing a lowess smooth (see Chapter ?? ) of the paired data onto the plot. Data points (or their smooth) generally parallel to the x=y line on the scatterplot would indicate an additive difference between the (x,y) data pairs. Therefore the line x = y + d could be plotted on the figure to illustrate the magnitude of the difference between x and y, where d is the appropriate estimate of the difference between x and y as described in the next section. In figure 6.6 the line x = y + 31.5 is plotted, where 31.5 is the median difference. For an additive relationship the data points would scatter around this line. Obviously the differences do not appear to be additive. Figure 6.5: Scatterplot of the example 1 mayfly data. Figure 6.6: Mayfly data with ill-fitting additive relationship x = y+31.5. Figure 6.7: Mayfly data with multiplicative relationship x = y•1.76. Alternatively, an increasing difference between the data and the x=y reference line indicates that there is a multiplicative difference between x and y, requiring a logarithmic transformation prior to the signed-rank or t-test. For a multiplicative relation the line \\(x = y•f^{−1}(d)\\) can be plotted as an aid in visualizing the relation between x and y. For base 10 logs, \\(f^{−1}(d) = 10^d\\) while for natural logs it is exp(d). The mayfly data of example 1 exhibit such a multiplicative relationship, as shown in figure 6.7. There \\(d = \\bar{D}\\), the Hodges-Lehmann estimate in log units, resulting in the line x = y•1.76. 6.6 Estimating the Magnitude of Differences Between Two Groups After testing for differences between matched pairs, a measure of the magnitude of that difference is usually desirable. If outliers are not present, and the differences can be considered normal, an efficient estimator is the mean difference\\(\\bar{D}\\). This estimator is appropriate whenever the paired t-test is valid. When outliers or non-normality are suspected, a more robust estimator is necessary. The estimator associated with the signed-rank test is a Hodges-Lehmann estimator \\(\\bar{\\Delta}\\) Hollander and Wolfe (1973) . \\(\\bar{\\Delta}\\) is the median of all possible pairwise averages of the differences. When the Di are not symmetric and the sign test is used, the associated estimate of difference is simply the median of the differences Dmed. 6.6.1 The Median Difference (Sign Test) For the mayfly data of example 1, the median difference in counts is 31.5. As these data are asymmetric, there is no statement that the two groups are related in an additive fashion. But subtracting this median value from the x data (the sites above the outfalls) would produce data having no evidence for rejection of H0 as measured by the sign test. Therefore the median is the most appropriate measure of how far from “equality” the two groups are in their original units. Half of the differences are larger, and half smaller, than the median. A confidence inteval on this difference is simply the confidence interval on the median previously presented in Chapter 4. 6.6.2 The Hodges-Lehmann Estimator (Signed-Rank Test) Hodges-Lehmann estimators are computed as the median of all possible appropriate combinations of the data. They are associated with many nonparametric test procedures. For the matched-pairs situation, \\(\\bar{\\Delta}\\) is the median of the n•(n+1)/2 possible pairwise averages: \\(\\bar{\\Delta}=median[A_{ij}] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where A_{ij}=[(D_i+D_j)/2)] \\ \\ \\ \\ \\ \\ \\ for\\ \\ \\ all\\ \\ \\ \\ i≤j \\ \\ \\ \\ \\ \\ \\ \\ 6.1\\) \\(\\bar{\\Delta}\\) is related to the signed-rank test in that subtracting \\(\\bar{\\Delta}\\) from all paired differences (or equivalently, from the x’s or y’s, whichever is larger) would cause the signed-rank test to have W+ close to 0, and find no evidence of difference between x and y. For the cases of symmetric differences where the signed-rank test is appropriate, the Hodges-Lehmann estimator more efficiently measures the additive difference between two data groups than does the sample median of the differences Dmed. For the mayfly data, \\(\\bar{\\Delta}\\) of the logs = 0.245. The log of upstream counts minus 0.245 estimates the log of the counts below the outfalls. Thus the counts above the outfalls divided by 10 0.245 = 1.76 best estimates the counts below the outfalls (the line X = 1.76 Y in figure 6.7. 6.6.3 Confidence interval on \\(\\bar{\\Delta}\\) A nonparametric interval estimate of the difference between paired observations is computed by a process similar to that for the confidence interval for other Hodges-Lehmann estimators. The tabled distribution of the test statistic is entered to find upper and lower critical values at onehalf the desired alpha level. These critical values are transformed into ranks. The pairwise differences \\(A_{ij}\\) are ordered from smallest to largest, and those corresponding to the computed ranks are the ends of the confidence interval. For small sample sizes, table B6 for the signed-rank test is entered to find the critical value x’ having a p-value nearest to α/2. This critical value is then used to compute the ranks Ru and Rl corresponding to the pairwise averages \\(A_{ij}\\) at the upper and lower confidence limits for \\(\\bar{\\Delta}\\). These limits are the \\(R_lth\\) ranked \\(A_{ij}\\) going in from either end of the sorted list of n(n+1)/2 differences. Rl = x’ for x’ = (α/2)th quantile of signed-rank test statistic 6.2 Ru = x + 1 for x = (1−α/2)th quantile of signed-rank test statistic 6.3 Example 1, cont.d For the n=12 logarithms of the mayfly data, there are N=78 pairwise averages. For an α ≅ 0.05 confidence interval, x’=14 and x=64 from table B6 (α =2•0.026 = 0.052). The confidence interval is composed of the 14th and 65th ranked averages (the 14th average in from either end. For larger sample sizes where the large-sample approximation is used, a critical value zα/2 from the table of standard normal quantiles determines the upper and lower ranks of the pairwise averages \\(A_{ij}\\) corresponding to the ends of the confidence interval. Those ranks are \\(R_l=\\frac{N-z_\\alpha/2 •\\sqrt{\\frac{n(n+1)(2n+1)}{6}}}{2}\\) \\(R_l=\\frac{N+z_\\alpha/2•\\sqrt{\\frac{n(n+1)(2n+1)}{6}}}{2}+1\\\\=N-R_l+1 \\ \\ \\ \\ \\ \\text{where N=n(n+1)/2}\\) Example 1 cont. For the mayfly data with N=78 and n=12, an approximate α=0.05 confidence interval is between the 14th and 65th ranked averages, as computed below: \\(R_l=\\frac{78-1.96•\\sqrt{\\frac{12(13)(25)}{6}}}{2}=14.0 \\\\R_u=78-14+1=65\\) 6.6.4 Mean Difference (t-Test) For the situation where the differences are not only symmetric but normally distributed and the t-test is used, the most efficient estimator of the difference between the two groups is the mean difference \\(\\bar{D}\\). However, \\(\\bar{D}\\) is only slightly more efficient than is \\(\\hat{\\Delta}\\), so that when the data depart from normality even slightly the Hodges-Lehmann estimator is just as efficient as \\(\\bar{D}\\). This mirrors the power characteristics of their associated tests, as the signed-rank test is as efficient as the t-test for only slight departures from normality Lehmann and D’Abrera (1975). Therefore when using “real data” which is never “exactly normal” the mean difference has little advantage over \\(\\hat{\\Delta}\\), while \\(\\hat{\\Delta}\\) is more appropriate in a wider number of situations – for data which are symmetric but not normal. 6.6.5 Confidence interval on the mean difference A confidence interval on the mean difference \\(\\hat{\\Delta}\\) is computed exactly like any confidence interval for a mean: \\(CI = \\bar{D}± t_{\\alpha/2,(n-1)}\\frac{s}{\\sqrt{n}} \\ \\ \\ \\ \\ \\ \\ [6.6]\\) where s is the standard deviation of the differences Di. Exercises 6.1 Test the null hypothesis that the median of annual flows for the Conecuh R. at Brantley, Ala. (data in Appendix C2) is 683 cfs for 1941 - 1960. The alternate hypothesis is that it is less than 683 cfs, and alpha = 0.05. data &lt;- read.csv(&#39;data/appendix_c2.csv&#39;) print(data) ## Year Flow_cfs ## 1 1941 369 ## 2 1942 683 ## 3 1943 923 ## 4 1944 1193 ## 5 1945 413 ## 6 1946 1025 ## 7 1947 894 ## 8 1948 859 ## 9 1949 1157 ## 10 1950 524 ## 11 195l 327 ## 12 1952 574 ## 13 1953 762 ## 14 1954 578 ## 15 1955 379 ## 16 1956 374 ## 17 1957 581 ## 18 1958 581 ## 19 1959 530 ## 20 1960 929 summary(data[,2]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 327.0 496.2 581.0 682.8 901.2 1193.0 x &lt;- data[,2] y &lt;- 682.75 hypo_test &lt;- wilcox.test(x, y, correct = FALSE, alternative = &quot;less&quot;) ## Warning in wilcox.test.default(x, y, correct = FALSE, alternative = &quot;less&quot;): ## cannot compute exact p-value with ties hypo_test ## ## Wilcoxon rank sum test ## ## data: x and y ## W = 9, p-value = 0.4344 ## alternative hypothesis: true location shift is less than 0 6.2 Which of the following are not matched pairs? analyses of same standard solutions sent to two different laboratories daily sediment discharges above and below a reservoir nitrate analyses from randomly selected wells in each of two aquifers all of the above are matched pairs. Answer - c. Nitrate analyses from randomly selected wells in each of two aquifers 6.3 The following values of specific conductance were measured on the two forks of the Shenandoah River in Virginia (D. Lynch, personal communication). State the appropriate null and alternate hypotheses to see if conductance values are the same in the two forks. Null hypothesis: the specific conductance values are similar for two groups Alternate hypotheses: the specific conductance values are different for two groups Determine whether a parametric or nonparametric test should be used. Parametric test, as the boxplot shows low median and no outliers. Compute an α = .05 test and report the results. south_fork =c(194, 348, 383, 225, 266, 194, 212, 320, 340, 310) north_fork =c(255, 353, 470, 353, 353, 295, 199, 410, 346, 405) data &lt;- data.frame(site = rep(c(&quot;south_fork&quot;, &quot;north_fork&quot;), each = 10), conc = c(south_fork, north_fork)) print(data) ## site conc ## 1 south_fork 194 ## 2 south_fork 348 ## 3 south_fork 383 ## 4 south_fork 225 ## 5 south_fork 266 ## 6 south_fork 194 ## 7 south_fork 212 ## 8 south_fork 320 ## 9 south_fork 340 ## 10 south_fork 310 ## 11 north_fork 255 ## 12 north_fork 353 ## 13 north_fork 470 ## 14 north_fork 353 ## 15 north_fork 353 ## 16 north_fork 295 ## 17 north_fork 199 ## 18 north_fork 410 ## 19 north_fork 346 ## 20 north_fork 405 test &lt;- wilcox.test(x = south_fork, y = north_fork, paired = TRUE, correct = FALSE) ## Warning in wilcox.test.default(x = south_fork, y = north_fork, paired = TRUE, : ## cannot compute exact p-value with ties test ## ## Wilcoxon signed rank test ## ## data: south_fork and north_fork ## V = 3, p-value = 0.01246 ## alternative hypothesis: true location shift is not equal to 0 Alternative hypothesis(H1) accept. Illustrate and check the results with a plot. library(&quot;ggplot2&quot;) ggplot(data, aes(x = site, y = conc, fill = site)) + geom_boxplot()+ theme_bw() Estimate the amount by which the forks differ in conductance, regardless of the test outcome. Date South Fork North Fork Date South Fork North Fork 5-23-83 194 255 2-22-84 194 295 8-16-83 348 353 4-24-84 212 199 10-05-83 383 470 6-04-84 320 410 11-15-83 225 353 7-19-84 340 346 1-10-84 266 353 8-28-84 310 405 forks_diff_cond &lt;- mean(south_fork) - mean(north_fork) forks_diff_cond ## [1] -64.7 6.4 Atrazine concentrations in shallow groundwaters were measured by Junk et al. (1980) before (June) and after (September) the application season. The data are given in Appendix C4. Determine if concentrations of atrazine are higher in groundwaters following surface application than before. data &lt;- read.csv(&#39;data/appendix_c4.csv&#39;) print(data) ## June.atrazine Sept.atrazine ## 1 0.38 2.66 ## 2 0.04 0.63 ## 3 -0.01 0.59 ## 4 0.03 0.05 ## 5 0.03 0.84 ## 6 0.05 0.58 ## 7 0.02 0.02 ## 8 -0.01 0.01 ## 9 -0.01 -0.01 ## 10 -0.01 -0.01 ## 11 0.11 0.09 ## 12 0.09 0.31 ## 13 -0.01 0.02 ## 14 -0.01 -0.01 ## 15 -0.01 0.50 ## 16 -0.01 0.03 ## 17 0.02 0.09 ## 18 0.03 0.06 ## 19 0.02 0.03 ## 20 0.02 0.01 ## 21 0.05 0.10 ## 22 0.03 0.25 ## 23 0.05 0.03 ## 24 -0.01 88.36 summary(data) ## June.atrazine Sept.atrazine ## Min. :-0.01000 Min. :-0.010 ## 1st Qu.:-0.01000 1st Qu.: 0.020 ## Median : 0.02000 Median : 0.075 ## Mean : 0.03667 Mean : 3.968 ## 3rd Qu.: 0.04250 3rd Qu.: 0.520 ## Max. : 0.38000 Max. :88.360 x = data[,1] y = data[,2] conc_atrazine &lt;- wilcox.test(x, y, correct = FALSE, paired = TRUE, alternative = &quot;greater&quot;) ## Warning in wilcox.test.default(x, y, correct = FALSE, paired = TRUE, alternative ## = &quot;greater&quot;): cannot compute exact p-value with ties ## Warning in wilcox.test.default(x, y, correct = FALSE, paired = TRUE, alternative ## = &quot;greater&quot;): cannot compute exact p-value with zeroes conc_atrazine ## ## Wilcoxon signed rank test ## ## data: x and y ## V = 12, p-value = 0.9997 ## alternative hypothesis: true location shift is greater than 0 6.5 Try performing the comparison of atrazine concentrations in 6.4 using a t-test, setting all values below the detection limit to zero. Compare the results with those of 6.4. Discuss why the results are similar or different. comp_atrazine_conc &lt;- t.test(x, y, paired = TRUE, alternative = &quot;greater&quot;) comp_atrazine_conc ## ## Paired t-test ## ## data: x and y ## t = -1.0704, df = 23, p-value = 0.8522 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -10.22569 Inf ## sample estimates: ## mean of the differences ## -3.93125 References "],
["references.html", "References", " References "]
]
